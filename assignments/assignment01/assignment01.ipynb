{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7630800",
   "metadata": {},
   "source": [
    "# Assignment 01\n",
    "\n",
    "This assignment consists of two tasks with subtasks. Every subtask has a point value and lists expectations for answers. Please read both task and expectations carefully before answering.\n",
    "\n",
    "### Hand-in Instructions\n",
    "\n",
    "Submit a single `.ipynb` file with all outputs saved. The notebook must be fully self-contained and ready to read without running any cells.\n",
    "\n",
    "### Overview\n",
    "\n",
    "| Task  | Topic                                     | Points  |\n",
    "| ----- | ----------------------------------------- | ------- |\n",
    "| **1** | **PCA**                                   |         |\n",
    "| 1.1   | 3D scatter plot                           | 5       |\n",
    "| 1.2   | 2D scatter plot                           | 5       |\n",
    "| 1.3   | Interpreting variance                     | 10      |\n",
    "| 1.4   | Variance and geometry                     | 10      |\n",
    "| **2** | **Breast Cancer Classification Pipeline** |         |\n",
    "| 2.1   | Exploratory Data Analysis                 | 30      |\n",
    "| 2.2   | Train/Test Split                          | 5       |\n",
    "| 2.3   | Baseline Model                            | 5       |\n",
    "| 2.4   | Kitchen Sink Model                        | 10      |\n",
    "| 2.5   | Build Your Own Pipeline                   | 20      |\n",
    "|       | **Total**                                 | **100** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a799d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155834c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "\n",
    "from assignment_utils import generate_annulus_4d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3848b85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: PCA\n",
    "\n",
    "You've been given a mysterious dataset with **4 dimensions** (F1, F2, F3, F4). We can't directly visualize 4D data, but we can look at 3 dimensions at a time and use color for the 4th.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4d, radius = generate_annulus_4d()\n",
    "df = pd.DataFrame(data_4d, columns=[\"F1\", \"F2\", \"F3\", \"F4\"])\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Let's visualize the first 3 dimensions (F1, F2, F3) in 3D\n",
    "# The 4th dimension (F4) is represented as color\n",
    "fig = px.scatter_3d(\n",
    "    df,\n",
    "    x=\"F1\",\n",
    "    y=\"F2\",\n",
    "    z=\"F3\",\n",
    "    color=df[\"F4\"],\n",
    "    color_continuous_scale=\"viridis\",\n",
    "    title=\"3D View of the 4D Dataset (F1, F2, F3, color=F4)\",\n",
    "    labels={\"color\": \"F4\"},\n",
    ")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.update_layout(width=800, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472977bf",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "**PCA** is a technique that finds new axes (called _principal components_) that capture the most variance in the data.\n",
    "\n",
    "**Mathematical formulation:**\n",
    "\n",
    "1. **Standardize** the data: $\\mathbf{Z} = \\frac{\\mathbf{X} - \\boldsymbol{\\mu}}{\\boldsymbol{\\sigma}}$\n",
    "\n",
    "2. Compute the **covariance matrix**: $\\mathbf{C} = \\frac{1}{n-1} \\mathbf{Z}^T \\mathbf{Z}$\n",
    "\n",
    "3. Find the **eigenvectors** and **eigenvalues** of $\\mathbf{C}$:\n",
    "   $$\\mathbf{C} \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i$$\n",
    "4. **Project** the data onto the principal components: $\\mathbf{Z}_{PC} = \\mathbf{Z} \\mathbf{V}$\n",
    "\n",
    "where $\\mathbf{V} = [\\mathbf{v}_1, \\mathbf{v}_2, \\ldots]$ are the eigenvectors sorted by decreasing eigenvalue $\\lambda_i$.\n",
    "\n",
    "**Key ideas:**\n",
    "\n",
    "- **PC1** points in the direction of maximum variance (largest $\\lambda$)\n",
    "- **PC2** is perpendicular to PC1 and captures the next most variance\n",
    "- The **explained variance ratio** for each PC is: $\\frac{\\lambda_i}{\\sum_j \\lambda_j}$\n",
    "- The eigenvectors $\\mathbf{V} = [\\mathbf{v}_1, \\mathbf{v}_2, \\ldots]$ are basis vectors for the principal component space. Any data vector can be completely reconstructed by a linear combination of these principal component basis vectors.\n",
    "\n",
    "If the data lies on a lower-dimensional subspace, PCA can reveal it by finding the directions that matter most.\n",
    "\n",
    "Let's standardize the data first (so all features have equal scale), then apply PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4a7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize and apply PCA\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df)\n",
    "\n",
    "pca = PCA()\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Create a DataFrame with principal components\n",
    "df_pca = pd.DataFrame(data_pca, columns=[\"PC1\", \"PC2\", \"PC3\", \"PC4\"])\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Cumulative explained variance ratio:\", np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "# Get the PC directions (loadings) - each row is a PC, each column is a feature\n",
    "components = pca.components_  # Shape: (4, 4) - 4 PCs x 4 features\n",
    "\n",
    "# Create the scatter plot of scaled data (first 3 features)\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the data points\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=data_scaled[:, 0],\n",
    "        y=data_scaled[:, 1],\n",
    "        z=data_scaled[:, 2],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=2, opacity=0.5),\n",
    "        name=\"Data\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add arrows for PC1, PC2, PC3 directions\n",
    "# Arrow length proportional to explained variance ratio (with minimum for visibility)\n",
    "colors = [\"red\", \"green\", \"blue\"]\n",
    "base_scale = 5  # Base scale factor\n",
    "min_scale = 0.5  # Minimum scale so small PCs are still visible\n",
    "\n",
    "for i in range(3):\n",
    "    pc_direction = components[i, :3]  # First 3 components of each PC\n",
    "    # Scale by explained variance ratio - longer arrow = more variance\n",
    "    # Use minimum scale so all arrows are visible\n",
    "    variance_ratio = pca.explained_variance_ratio_[i] / max(\n",
    "        pca.explained_variance_ratio_\n",
    "    )\n",
    "    scale = max(base_scale * variance_ratio, min_scale)\n",
    "\n",
    "    # Arrow line\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=[0, pc_direction[0] * scale],\n",
    "            y=[0, pc_direction[1] * scale],\n",
    "            z=[0, pc_direction[2] * scale],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=colors[i], width=8),\n",
    "            name=f\"PC{i + 1} ({pca.explained_variance_ratio_[i]:.1%} var)\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Arrow head (cone)\n",
    "    fig.add_trace(\n",
    "        go.Cone(\n",
    "            x=[pc_direction[0] * scale],\n",
    "            y=[pc_direction[1] * scale],\n",
    "            z=[pc_direction[2] * scale],\n",
    "            u=[pc_direction[0]],\n",
    "            v=[pc_direction[1]],\n",
    "            w=[pc_direction[2]],\n",
    "            colorscale=[[0, colors[i]], [1, colors[i]]],\n",
    "            showscale=False,\n",
    "            sizemode=\"absolute\",\n",
    "            sizeref=0.3,\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Principal Component Directions (arrow length ∝ variance explained)\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"F1 (scaled)\",\n",
    "        yaxis_title=\"F2 (scaled)\",\n",
    "        zaxis_title=\"F3 (scaled)\",\n",
    "    ),\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8317af",
   "metadata": {},
   "source": [
    "### Task 1.1 3D scatter plot\n",
    "\n",
    "- Task: Create a 3D scatter plot using PC1, PC2, PC3 as axes\n",
    "- Points: 5\n",
    "- Expectations: A working 3D scatter plot of the PCA-transformed data (similar in style to the first 3D plot). No further analysis or comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f9718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fce4b7",
   "metadata": {},
   "source": [
    "### Task 1.2 2D scatter plot\n",
    "\n",
    "- Task: Create a 2D scatter plot using PC1, PC2 as axes\n",
    "- Points: 5\n",
    "- Expectations: A working 2D scatter plot of the PCA-transformed data. No further analysis or comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689673ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5cd7d",
   "metadata": {},
   "source": [
    "### Task 1.3 Interpreting variance\n",
    "\n",
    "- Task: How much variance do PC1 and PC2 capture together? Based on this, what can you conclude about the original 4D dataset?\n",
    "- Points: 10\n",
    "- Expectations: A written response (1 paragraph).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e308ce",
   "metadata": {},
   "source": [
    "#### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872f0819",
   "metadata": {},
   "source": [
    "### Task 1.4 Variance and geometry\n",
    "\n",
    "- Task: If PC1 explained 90% of the variance and PC2 only 10%, what shape would you expect the data to form? Now compare this to your actual ~50/50 split — what does this tell you about the geometry of your data?\n",
    "- Points: 10\n",
    "- Expectations: A written response (1 paragraph).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968012e8",
   "metadata": {},
   "source": [
    "#### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af0e1ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Breast Cancer Classification Pipeline\n",
    "\n",
    "Now let's apply what you've learned to a real-world dataset: the **Wisconsin Breast Cancer** dataset. This dataset contains measurements from cell nuclei in breast tissue samples, and the goal is to classify tumors as **malignant** or **benign**.\n",
    "\n",
    "**Sources:**\n",
    "\n",
    "- [sklearn.datasets.load_breast_cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html)\n",
    "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)\n",
    "\n",
    "We'll work through a complete machine learning workflow:\n",
    "\n",
    "1. Exploratory Data Analysis (EDA)\n",
    "2. Train/test split\n",
    "3. Baseline model\n",
    "4. \"Kitchen sink\" model (all features, no preprocessing)\n",
    "5. Build your own pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18989855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "df_cancer = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df_cancer[\"target\"] = cancer.target\n",
    "\n",
    "print(f\"Dataset shape: {df_cancer.shape}\")\n",
    "print(f\"Target classes: {cancer.target_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bc8c8c",
   "metadata": {},
   "source": [
    "### Task 2.1 Exploratory Data Analysis (EDA)\n",
    "\n",
    "- Task: Conduct an EDA of the breast cancer dataset. For each analysis you perform, explain _why_ you chose to look at it and what it tells you.\n",
    "- Points: 30\n",
    "- Expectations: A mix of code, plots, and written commentary. Quality of reasoning and plots matters more than quantity of plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16716954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your EDA here. Add as many code and markdown cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd0ec0",
   "metadata": {},
   "source": [
    "### Task 2.2 Train/Test Split\n",
    "\n",
    "- Task: Split the data into training and test sets (80/20) before any modeling.\n",
    "- Points: 5\n",
    "- Expectations: Complete the TODO line to create an 80/20 split with `random_state=42` for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c1773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_cancer.drop(\"target\", axis=1)\n",
    "y = df_cancer[\"target\"]\n",
    "\n",
    "# TODO: Split into train/test sets\n",
    "# X_train, X_test, y_train, y_test ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e8002",
   "metadata": {},
   "source": [
    "### Task 2.3 Baseline Model\n",
    "\n",
    "Before building a real model, it's wise to establish a **baseline** — a classifier that any real model should beat.\n",
    "\n",
    "A **confusion matrix** shows how predictions compare to actual labels:\n",
    "\n",
    "|                     | Predicted Negative  | Predicted Positive  |\n",
    "| ------------------- | ------------------- | ------------------- |\n",
    "| **Actual Negative** | TN (True Negative)  | FP (False Positive) |\n",
    "| **Actual Positive** | FN (False Negative) | TP (True Positive)  |\n",
    "\n",
    "For cancer diagnosis: FN means missing a malignant tumor (bad!), FP means a false alarm (less bad, but still costly).\n",
    "\n",
    "- Task: Run the code below, note the accuracy and examine the confusion matrix. Describe what this classifier does. Would you trust it for diagnosis? Why or why not?\n",
    "- Points: 5\n",
    "- Expectations: A written response (1 paragraph).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1308362",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "baseline_accuracy = dummy.score(X_test, y_test)\n",
    "print(f\"Baseline accuracy: {baseline_accuracy:.3f}\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    dummy, X_test, y_test, display_labels=cancer.target_names, cmap=\"Blues\"\n",
    ")\n",
    "plt.title(\"Confusion Matrix: Baseline Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556423c",
   "metadata": {},
   "source": [
    "#### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b26210",
   "metadata": {},
   "source": [
    "### Task 2.4 Kitchen Sink Model\n",
    "\n",
    "The \"kitchen sink\" approach: throw all features into the model without any preprocessing. Let's see what happens.\n",
    "\n",
    "**Logistic Regression** is a linear classifier that predicts the probability of a binary outcome. It models:\n",
    "\n",
    "$$P(y=1 | \\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b) = \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + b)}}$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function, $\\mathbf{w}$ are the feature weights, and $b$ is the bias. The model is trained by minimizing the logistic loss using an iterative optimizer. Here we use **SAGA** (`solver=\"saga\"`), a stochastic gradient method whose fast convergence is only guaranteed on features with approximately the same scale ([sklearn docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)).\n",
    "\n",
    "- Task: Run the code below. Did the model converge? Why or why not? Explain based on your EDA findings and how gradient-based optimization works.\n",
    "- Points: 10\n",
    "- Expectations: A written response (1 paragraph).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5757c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_kitchen = LogisticRegression(solver=\"saga\", max_iter=100, random_state=42)\n",
    "lr_kitchen.fit(X_train, y_train)\n",
    "\n",
    "kitchen_train_accuracy = lr_kitchen.score(X_train, y_train)\n",
    "kitchen_accuracy = lr_kitchen.score(X_test, y_test)\n",
    "print(f\"Kitchen sink train accuracy: {kitchen_train_accuracy:.3f}\")\n",
    "print(f\"Kitchen sink test accuracy:  {kitchen_accuracy:.3f}\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    lr_kitchen, X_test, y_test, display_labels=cancer.target_names, cmap=\"Blues\"\n",
    ")\n",
    "plt.title(\"Confusion Matrix: Kitchen Sink Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f415a",
   "metadata": {},
   "source": [
    "#### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f999713",
   "metadata": {},
   "source": [
    "### Task 2.5 Build Your Own Pipeline\n",
    "\n",
    "Now it's your turn. Based on your EDA findings, build a classification pipeline.\n",
    "\n",
    "A **Pipeline** chains multiple preprocessing steps and a final estimator into a single object. This ensures:\n",
    "\n",
    "- No data leakage (preprocessing is fit only on training data)\n",
    "- Clean, reproducible code\n",
    "- Easy experimentation with different configurations\n",
    "\n",
    "Example pipeline structure:\n",
    "\n",
    "```python\n",
    "Pipeline([\n",
    "    (\"step1_name\", SomeTransformer()),\n",
    "    (\"step2_name\", AnotherTransformer()),\n",
    "    (\"classifier\", SomeClassifier()),\n",
    "])\n",
    "```\n",
    "\n",
    "- Task: Build a pipeline that preprocesses the data and fits a classifier. Evaluate your model, compare it to the kitchen sink model, and justify your preprocessing choices based on your EDA insights.\n",
    "- Points: 20\n",
    "- Notes:\n",
    "  - You are free to use any preprocessing technique (e.g., StandardScaler, PCA, column selection via ColumnTransformer, or others)\n",
    "  - There is no single \"correct\" answer — the goal is thoughtful justification\n",
    "- Expectations:\n",
    "  - A working pipeline with at least one preprocessing step\n",
    "  - A confusion matrix plot for your model\n",
    "  - A comparison with the kitchen sink model's confusion matrix\n",
    "  - A reflection on your model's errors — consider which types of mistakes matter most in a medical diagnosis context (1 paragraph)\n",
    "  - A brief explanation of why you chose your preprocessing steps (1 paragraph)\n",
    "  - **NOTE:** Your understanding and evaluation of the model performance is the objective here. The model's performance (how well it accurately classifies the data) will not detract from your grade. So if your model doesn't perform well, but you can explain why it doesn't perform well, then you can still receive the full 20 points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6405ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build your pipeline\n",
    "# Consider: What preprocessing steps would help based on your EDA?\n",
    "# Available transformers: StandardScaler, PCA, ColumnTransformer, etc.\n",
    "\n",
    "# Example pipeline\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        # Example: ColumnTransformer to select/transform specific columns\n",
    "        # (\"preprocessor\", ColumnTransformer([\n",
    "        #     (\"selected_features\", StandardScaler(), [\"mean radius\", \"mean texture\", ...]),\n",
    "        # ])),\n",
    "        # (\"scaler\", StandardScaler()),\n",
    "        # (\"pca\", PCA(n_components=10)),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            LogisticRegression(solver=\"saga\", max_iter=1000, random_state=42),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# TODO: Fit the pipeline on training data\n",
    "\n",
    "\n",
    "# TODO: Evaluate and print accuracy\n",
    "\n",
    "\n",
    "# TODO: Plot confusion matrix\n",
    "\n",
    "# Add as many code and markdown cells as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e36ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ing3513",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
