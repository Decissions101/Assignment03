{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f710b9",
   "metadata": {},
   "source": [
    "# Lab 11: Clustering Algorithms\n",
    "\n",
    "In this lab, you'll explore **unsupervised learning** through clustering — the task of grouping data points based on similarity without using labels. You'll compare K-Means, DBSCAN, and Hierarchical clustering, understand their parameters, and learn when to use each algorithm.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Understand K-Means clustering and choose K using the elbow method\n",
    "- Understand DBSCAN and its sensitivity to ε (eps) and MinPts\n",
    "- Interpret dendrograms from hierarchical clustering\n",
    "- Compare clustering algorithms on datasets that highlight their strengths and weaknesses\n",
    "\n",
    "### Overview\n",
    "\n",
    "| Part | Topic                                 |\n",
    "| ---- | ------------------------------------- |\n",
    "| 1    | K-Means Clustering                    |\n",
    "| 2    | DBSCAN Clustering                     |\n",
    "| 3    | Hierarchical Clustering & Dendrograms |\n",
    "| 4    | Algorithm Comparison                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add3e1c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c6446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ea1ae",
   "metadata": {},
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01888abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(\n",
    "    X, labels, ax=None, title=\"Clusters\", cmap=\"viridis\", show_noise=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot clustered data points with different colors for each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape (n_samples, 2)\n",
    "        Data points\n",
    "    labels : array-like\n",
    "        Cluster labels (-1 indicates noise for DBSCAN)\n",
    "    ax : matplotlib axis (optional)\n",
    "    title : plot title\n",
    "    cmap : colormap\n",
    "    show_noise : whether to highlight noise points\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    unique_labels = set(labels)\n",
    "\n",
    "    # Plot each cluster\n",
    "    for label in unique_labels:\n",
    "        mask = labels == label\n",
    "        if label == -1 and show_noise:\n",
    "            # Noise points (DBSCAN)\n",
    "            ax.scatter(\n",
    "                X[mask, 0],\n",
    "                X[mask, 1],\n",
    "                c=\"gray\",\n",
    "                marker=\"x\",\n",
    "                s=50,\n",
    "                alpha=0.6,\n",
    "                label=\"Noise\",\n",
    "            )\n",
    "        else:\n",
    "            ax.scatter(\n",
    "                X[mask, 0],\n",
    "                X[mask, 1],\n",
    "                s=50,\n",
    "                edgecolors=\"k\",\n",
    "                linewidths=0.5,\n",
    "                label=f\"Cluster {label}\",\n",
    "            )\n",
    "\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_kmeans_centroids(X, kmeans, ax=None, title=\"K-Means Clusters\"):\n",
    "    \"\"\"\n",
    "    Plot K-Means clusters with centroids marked.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot data points colored by cluster\n",
    "    scatter = ax.scatter(\n",
    "        X[:, 0],\n",
    "        X[:, 1],\n",
    "        c=kmeans.labels_,\n",
    "        cmap=\"viridis\",\n",
    "        s=50,\n",
    "        edgecolors=\"k\",\n",
    "        linewidths=0.5,\n",
    "    )\n",
    "\n",
    "    # Plot centroids\n",
    "    ax.scatter(\n",
    "        kmeans.cluster_centers_[:, 0],\n",
    "        kmeans.cluster_centers_[:, 1],\n",
    "        c=\"red\",\n",
    "        marker=\"X\",\n",
    "        s=200,\n",
    "        edgecolors=\"k\",\n",
    "        linewidths=2,\n",
    "        label=\"Centroids\",\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba63df1",
   "metadata": {},
   "source": [
    "### Generate Datasets\n",
    "\n",
    "We'll create several synthetic datasets that highlight different clustering scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca35acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1: Well-separated spherical blobs (ideal for K-Means)\n",
    "X_blobs, y_blobs = make_blobs(\n",
    "    n_samples=300, centers=3, cluster_std=0.6, random_state=42\n",
    ")\n",
    "\n",
    "# Dataset 2: Two moons (non-convex shapes, ideal for DBSCAN)\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.08, random_state=42)\n",
    "\n",
    "# Dataset 3: Concentric circles (non-convex, ideal for DBSCAN)\n",
    "X_circles, y_circles = make_circles(\n",
    "    n_samples=300, noise=0.05, factor=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Dataset 4: Blobs with varying density (challenging for DBSCAN)\n",
    "X_varied, y_varied = make_blobs(\n",
    "    n_samples=[100, 300, 50],\n",
    "    centers=[[0, 0], [3, 3], [6, 0]],\n",
    "    cluster_std=[0.3, 1.0, 0.5],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Dataset 5: Data with outliers\n",
    "X_outliers, y_outliers = make_blobs(\n",
    "    n_samples=250, centers=2, cluster_std=0.5, random_state=42\n",
    ")\n",
    "# Add outliers\n",
    "outliers = np.array([[-4, 4], [5, -3], [-3, -4], [6, 5], [0, 5]])\n",
    "X_outliers = np.vstack([X_outliers, outliers])\n",
    "\n",
    "# Visualize all datasets\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "datasets = [\n",
    "    (X_blobs, \"Spherical Blobs\\n(Ideal for K-Means)\"),\n",
    "    (X_moons, \"Two Moons\\n(Non-convex shapes)\"),\n",
    "    (X_circles, \"Concentric Circles\\n(Non-convex shapes)\"),\n",
    "    (X_varied, \"Varying Density\\n(Different cluster sizes)\"),\n",
    "    (X_outliers, \"Data with Outliers\\n(5 outlier points added)\"),\n",
    "]\n",
    "\n",
    "for ax, (X, title) in zip(axes, datasets):\n",
    "    ax.scatter(X[:, 0], X[:, 1], s=30, edgecolors=\"k\", linewidths=0.5)\n",
    "    ax.set_title(title, fontsize=11)\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "\n",
    "# Hide the empty subplot\n",
    "axes[5].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Synthetic Datasets for Clustering\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    \"These datasets will help us understand when each clustering algorithm excels or struggles.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0b39c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: K-Means Clustering\n",
    "\n",
    "### How K-Means Works\n",
    "\n",
    "K-Means is an iterative algorithm that partitions data into **K clusters** by:\n",
    "\n",
    "1. **Initialize:** Randomly place K centroids\n",
    "2. **Assign:** Assign each point to the nearest centroid\n",
    "3. **Update:** Move each centroid to the mean of its assigned points\n",
    "4. **Repeat:** Steps 2-3 until centroids stop moving (convergence)\n",
    "\n",
    "**Objective:** Minimize the **within-cluster sum of squares (inertia)**:\n",
    "\n",
    "$$\\text{Inertia} = \\sum_{i=1}^{K} \\sum_{x \\in C_i} ||x - \\mu_i||^2$$\n",
    "\n",
    "where $C_i$ is cluster $i$ and $\\mu_i$ is its centroid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeff4cc",
   "metadata": {},
   "source": [
    "### 1.1 K-Means Step by Step\n",
    "\n",
    "Let's visualize how K-Means converges over iterations. We'll manually implement the algorithm to see exactly what happens at each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual K-Means implementation to visualize each step clearly\n",
    "def kmeans_step_by_step(X, initial_centroids, n_steps=6):\n",
    "    \"\"\"\n",
    "    Run K-Means manually, returning centroids and labels at each step.\n",
    "    \"\"\"\n",
    "    centroids = initial_centroids.copy()\n",
    "    history = []\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        # ASSIGN: Each point to nearest centroid\n",
    "        distances = np.sqrt(((X[:, np.newaxis] - centroids) ** 2).sum(axis=2))\n",
    "        labels = distances.argmin(axis=1)\n",
    "\n",
    "        # Calculate inertia\n",
    "        inertia = sum(\n",
    "            ((X[labels == k] - centroids[k]) ** 2).sum() for k in range(len(centroids))\n",
    "        )\n",
    "\n",
    "        history.append(\n",
    "            {\"centroids\": centroids.copy(), \"labels\": labels.copy(), \"inertia\": inertia}\n",
    "        )\n",
    "\n",
    "        # UPDATE: Move centroids to mean of assigned points\n",
    "        # Handle empty clusters by keeping centroid in place\n",
    "        new_centroids = []\n",
    "        for k in range(len(centroids)):\n",
    "            if (labels == k).sum() > 0:\n",
    "                new_centroids.append(X[labels == k].mean(axis=0))\n",
    "            else:\n",
    "                new_centroids.append(centroids[k])  # Keep old position\n",
    "        new_centroids = np.array(new_centroids)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.allclose(centroids, new_centroids):\n",
    "            # Fill remaining history with converged state\n",
    "            for _ in range(n_steps - step - 1):\n",
    "                history.append(history[-1].copy())\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# Use blobs dataset\n",
    "X = X_blobs.copy()\n",
    "\n",
    "# Start with initial centroids positioned near the center\n",
    "# This causes each centroid to initially \"steal\" points from multiple clusters,\n",
    "# resulting in more gradual convergence over several steps\n",
    "initial_centroids = np.array(\n",
    "    [\n",
    "        [-3, 3],  # Between top and bottom-left clusters\n",
    "        [0, 0],  # Central position\n",
    "        [1, 5],  # Between top and right clusters\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Run K-Means step by step\n",
    "history = kmeans_step_by_step(X, initial_centroids, n_steps=4)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (ax, state) in enumerate(zip(axes, history)):\n",
    "    # Plot clusters\n",
    "    ax.scatter(\n",
    "        X[:, 0],\n",
    "        X[:, 1],\n",
    "        c=state[\"labels\"],\n",
    "        cmap=\"viridis\",\n",
    "        s=50,\n",
    "        edgecolors=\"k\",\n",
    "        linewidths=0.5,\n",
    "    )\n",
    "\n",
    "    # Plot centroids\n",
    "    ax.scatter(\n",
    "        state[\"centroids\"][:, 0],\n",
    "        state[\"centroids\"][:, 1],\n",
    "        c=\"red\",\n",
    "        marker=\"X\",\n",
    "        s=200,\n",
    "        edgecolors=\"k\",\n",
    "        linewidths=2,\n",
    "    )\n",
    "\n",
    "    # Draw arrows showing centroid movement (except first frame)\n",
    "    if idx > 0:\n",
    "        prev_centroids = history[idx - 1][\"centroids\"]\n",
    "        for old, new in zip(prev_centroids, state[\"centroids\"]):\n",
    "            # Only draw arrow if centroid moved a meaningful distance\n",
    "            dist = np.sqrt(((old - new) ** 2).sum())\n",
    "            if dist > 0.1:  # Minimum distance threshold\n",
    "                ax.annotate(\n",
    "                    \"\",\n",
    "                    xy=new,\n",
    "                    xytext=old,\n",
    "                    arrowprops=dict(arrowstyle=\"->\", color=\"red\", lw=2),\n",
    "                )\n",
    "\n",
    "    ax.set_title(f\"Step {idx + 1}\\nInertia: {state['inertia']:.1f}\", fontsize=11)\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"K-Means Convergence: Centroids Move Toward Cluster Centers\", fontsize=14, y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observe how:\")\n",
    "print(\n",
    "    \"• Centroids (red X) start in poor positions and MOVE toward the true cluster centers\"\n",
    ")\n",
    "print(\"• Inertia DECREASES as the algorithm converges\")\n",
    "print(\"• Movement continues until centroids stabilize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a906de",
   "metadata": {},
   "source": [
    "### 1.2 K-Means on Well-Separated Blobs\n",
    "\n",
    "K-Means works best on **spherical, well-separated clusters** of similar size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2154506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train K-Means with K=3 (the true number of clusters)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "kmeans.fit(X_blobs)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original data\n",
    "axes[0].scatter(\n",
    "    X_blobs[:, 0],\n",
    "    X_blobs[:, 1],\n",
    "    c=y_blobs,\n",
    "    cmap=\"viridis\",\n",
    "    s=50,\n",
    "    edgecolors=\"k\",\n",
    "    linewidths=0.5,\n",
    ")\n",
    "axes[0].set_title(\"True Labels (Ground Truth)\", fontsize=12)\n",
    "axes[0].set_xlabel(\"Feature 1\")\n",
    "axes[0].set_ylabel(\"Feature 2\")\n",
    "\n",
    "# K-Means result\n",
    "plot_kmeans_centroids(X_blobs, kmeans, ax=axes[1], title=\"K-Means Clustering (K=3)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Inertia (within-cluster sum of squares): {kmeans.inertia_:.2f}\")\n",
    "print(f\"Silhouette Score: {silhouette_score(X_blobs, kmeans.labels_):.3f}\")\n",
    "print(\"\\nK-Means successfully identifies the three clusters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592c6ccc",
   "metadata": {},
   "source": [
    "### 1.3 Choosing K: The Elbow Method\n",
    "\n",
    "In practice, we often don't know the true number of clusters. The **elbow method** helps us choose K:\n",
    "\n",
    "1. Run K-Means for K = 1, 2, 3, ...\n",
    "2. Plot inertia vs K\n",
    "3. Look for the \"elbow\" — the point where adding more clusters gives diminishing returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6d3453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method\n",
    "K_range = range(1, 11)\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_blobs)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    if k > 1:  # Silhouette score requires at least 2 clusters\n",
    "        silhouettes.append(silhouette_score(X_blobs, kmeans.labels_))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow plot\n",
    "axes[0].plot(K_range, inertias, \"bo-\", linewidth=2, markersize=8)\n",
    "axes[0].axvline(x=3, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Elbow at K=3\")\n",
    "axes[0].set_xlabel(\"Number of Clusters (K)\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Inertia\", fontsize=12)\n",
    "axes[0].set_title(\"Elbow Method\", fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette plot\n",
    "axes[1].plot(range(2, 11), silhouettes, \"go-\", linewidth=2, markersize=8)\n",
    "axes[1].axvline(x=3, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Best at K=3\")\n",
    "axes[1].set_xlabel(\"Number of Clusters (K)\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Silhouette Score\", fontsize=12)\n",
    "axes[1].set_title(\"Silhouette Score (Higher is Better)\", fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The 'elbow' in the inertia plot suggests K=3 is optimal.\")\n",
    "print(\"The silhouette score also peaks at K=3, confirming this choice.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeca652",
   "metadata": {},
   "source": [
    "### 1.4 Sensitivity to K\n",
    "\n",
    "Let's see what happens when we choose the wrong number of clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5149c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means with different K values\n",
    "K_values = [2, 3, 4, 5]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "for ax, k in zip(axes, K_values):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_blobs)\n",
    "\n",
    "    plot_kmeans_centroids(X_blobs, kmeans, ax=ax, title=f\"K = {k}\")\n",
    "    ax.set_title(f\"K = {k}\\nInertia: {kmeans.inertia_:.1f}\", fontsize=11)\n",
    "\n",
    "plt.suptitle(\"Effect of K on Clustering Results (True K = 3)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"K=2: Under-clustering — two natural clusters are merged\")\n",
    "print(\"K=3: Correct — matches the true structure\")\n",
    "print(\"K=4,5: Over-clustering — natural clusters are unnecessarily split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece2f40",
   "metadata": {},
   "source": [
    "### 1.5 Sensitivity to Initialization\n",
    "\n",
    "K-Means can converge to different solutions depending on initial centroid positions. This is why `n_init` (number of random initializations) matters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5853f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means with different random initializations\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 9))\n",
    "\n",
    "seeds = [0, 1, 2, 3, 10, 20, 30, 42]\n",
    "\n",
    "for ax, seed in zip(axes.flatten(), seeds):\n",
    "    # Use n_init=1 to see effect of different initializations\n",
    "    kmeans = KMeans(n_clusters=3, init=\"random\", n_init=1, random_state=seed)\n",
    "    kmeans.fit(X_blobs)\n",
    "\n",
    "    plot_kmeans_centroids(X_blobs, kmeans, ax=ax, title=f\"Seed {seed}\")\n",
    "    ax.set_title(f\"Random Seed: {seed}\\nInertia: {kmeans.inertia_:.1f}\", fontsize=10)\n",
    "    ax.legend().remove()  # Remove legend for cleaner display\n",
    "\n",
    "plt.suptitle(\n",
    "    \"K-Means with Different Random Initializations (n_init=1)\", fontsize=14, y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Different initializations can lead to different results!\")\n",
    "print(\"Some have higher inertia (worse) than others.\")\n",
    "print(\"\\nSolution: Use n_init > 1 (sklearn default is 10) to run multiple\")\n",
    "print(\"initializations and keep the best result.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd1f2b5",
   "metadata": {},
   "source": [
    "### 1.6 K-Means Limitations: Non-Convex Shapes\n",
    "\n",
    "K-Means assumes clusters are **spherical** and **similar in size**. Let's see what happens on non-convex data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a84a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means on moons and circles datasets\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Moons - True labels\n",
    "axes[0, 0].scatter(\n",
    "    X_moons[:, 0],\n",
    "    X_moons[:, 1],\n",
    "    c=y_moons,\n",
    "    cmap=\"viridis\",\n",
    "    s=50,\n",
    "    edgecolors=\"k\",\n",
    "    linewidths=0.5,\n",
    ")\n",
    "axes[0, 0].set_title(\"Moons: True Labels\", fontsize=12)\n",
    "\n",
    "# Moons - K-Means\n",
    "kmeans_moons = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans_moons.fit(X_moons)\n",
    "plot_kmeans_centroids(\n",
    "    X_moons, kmeans_moons, ax=axes[0, 1], title=\"Moons: K-Means (K=2)\"\n",
    ")\n",
    "\n",
    "# Circles - True labels\n",
    "axes[1, 0].scatter(\n",
    "    X_circles[:, 0],\n",
    "    X_circles[:, 1],\n",
    "    c=y_circles,\n",
    "    cmap=\"viridis\",\n",
    "    s=50,\n",
    "    edgecolors=\"k\",\n",
    "    linewidths=0.5,\n",
    ")\n",
    "axes[1, 0].set_title(\"Circles: True Labels\", fontsize=12)\n",
    "\n",
    "# Circles - K-Means\n",
    "kmeans_circles = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans_circles.fit(X_circles)\n",
    "plot_kmeans_centroids(\n",
    "    X_circles, kmeans_circles, ax=axes[1, 1], title=\"Circles: K-Means (K=2)\"\n",
    ")\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "\n",
    "plt.suptitle(\"K-Means Fails on Non-Convex Shapes\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"K-Means fails because it draws LINEAR boundaries between centroids.\")\n",
    "print(\n",
    "    \"It cannot capture the curved structure of moons or the nested structure of circles.\"\n",
    ")\n",
    "print(\"\\nThis is where DBSCAN shines!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5376a9",
   "metadata": {},
   "source": [
    "### 1.7 Your Turn: Find Optimal K\n",
    "\n",
    "**Task:** Use the elbow method to find the optimal K for a mystery dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b0bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mystery dataset\n",
    "X_mystery, _ = make_blobs(n_samples=400, centers=5, cluster_std=0.8, random_state=123)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_mystery[:, 0], X_mystery[:, 1], s=30, edgecolors=\"k\", linewidths=0.5)\n",
    "plt.title(\"Mystery Dataset - How many clusters?\", fontsize=12)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cffbe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the elbow method to find the optimal K\n",
    "# 1. Run K-Means for K = 1 to 10\n",
    "# 2. Collect inertias\n",
    "# 3. Plot the elbow curve\n",
    "# 4. Identify the optimal K\n",
    "\n",
    "# K_range = range(1, 11)\n",
    "# inertias = []\n",
    "\n",
    "# for k in K_range:\n",
    "#     TODO: Fit K-Means and record inertia\n",
    "\n",
    "# TODO: Plot the elbow curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46002942",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: DBSCAN Clustering\n",
    "\n",
    "### How DBSCAN Works\n",
    "\n",
    "**DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) groups together points that are closely packed and marks points in low-density regions as outliers.\n",
    "\n",
    "**Key parameters:**\n",
    "\n",
    "- **eps (ε):** The maximum distance between two points to be considered neighbors\n",
    "- **MinPts (min_samples):** Minimum number of points required to form a dense region\n",
    "\n",
    "**Point types:**\n",
    "\n",
    "- **Core point:** Has at least MinPts neighbors within eps distance\n",
    "- **Border point:** Within eps of a core point but has fewer than MinPts neighbors\n",
    "- **Noise point:** Not within eps of any core point (labeled as -1)\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- No need to specify K (number of clusters)\n",
    "- Can find arbitrarily shaped clusters\n",
    "- Automatically identifies outliers/noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a42bd3f",
   "metadata": {},
   "source": [
    "### 2.1 Core, Border, and Noise Points\n",
    "\n",
    "Let's visualize these different point types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate core, border, and noise points\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Use blobs with some outliers\n",
    "X = X_outliers.copy()\n",
    "\n",
    "# Fit DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "# Identify core samples\n",
    "core_mask = np.zeros(len(X), dtype=bool)\n",
    "core_mask[dbscan.core_sample_indices_] = True\n",
    "\n",
    "# Identify noise, border points\n",
    "noise_mask = labels == -1\n",
    "border_mask = ~core_mask & ~noise_mask\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.scatter(\n",
    "    X[core_mask, 0],\n",
    "    X[core_mask, 1],\n",
    "    c=\"blue\",\n",
    "    s=100,\n",
    "    edgecolors=\"k\",\n",
    "    linewidths=1,\n",
    "    label=f\"Core Points ({core_mask.sum()})\",\n",
    ")\n",
    "ax.scatter(\n",
    "    X[border_mask, 0],\n",
    "    X[border_mask, 1],\n",
    "    c=\"yellow\",\n",
    "    s=80,\n",
    "    edgecolors=\"k\",\n",
    "    linewidths=1,\n",
    "    label=f\"Border Points ({border_mask.sum()})\",\n",
    ")\n",
    "ax.scatter(\n",
    "    X[noise_mask, 0],\n",
    "    X[noise_mask, 1],\n",
    "    c=\"red\",\n",
    "    marker=\"x\",\n",
    "    s=100,\n",
    "    linewidths=2,\n",
    "    label=f\"Noise Points ({noise_mask.sum()})\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Feature 1\", fontsize=12)\n",
    "ax.set_ylabel(\"Feature 2\", fontsize=12)\n",
    "ax.set_title(\n",
    "    f\"DBSCAN: Core, Border, and Noise Points\\n(eps={0.5}, min_samples={5})\", fontsize=14\n",
    ")\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Core points (blue): Have ≥ min_samples neighbors within eps distance\")\n",
    "print(\n",
    "    \"Border points (yellow): Within eps of a core point, but have < min_samples neighbors\"\n",
    ")\n",
    "print(\"Noise points (red X): Not within eps of any core point — these are outliers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b9ba82",
   "metadata": {},
   "source": [
    "### 2.2 DBSCAN on Non-Convex Data\n",
    "\n",
    "DBSCAN can find **arbitrarily shaped clusters** — exactly where K-Means failed!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6620979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN on moons and circles\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Moons - True labels\n",
    "axes[0, 0].scatter(\n",
    "    X_moons[:, 0],\n",
    "    X_moons[:, 1],\n",
    "    c=y_moons,\n",
    "    cmap=\"viridis\",\n",
    "    s=50,\n",
    "    edgecolors=\"k\",\n",
    "    linewidths=0.5,\n",
    ")\n",
    "axes[0, 0].set_title(\"Moons: True Labels\", fontsize=12)\n",
    "\n",
    "# Moons - DBSCAN\n",
    "dbscan_moons = DBSCAN(eps=0.2, min_samples=5)\n",
    "labels_moons = dbscan_moons.fit_predict(X_moons)\n",
    "plot_clusters(X_moons, labels_moons, ax=axes[0, 1], title=\"Moons: DBSCAN\")\n",
    "\n",
    "# Circles - True labels\n",
    "axes[1, 0].scatter(\n",
    "    X_circles[:, 0],\n",
    "    X_circles[:, 1],\n",
    "    c=y_circles,\n",
    "    cmap=\"viridis\",\n",
    "    s=50,\n",
    "    edgecolors=\"k\",\n",
    "    linewidths=0.5,\n",
    ")\n",
    "axes[1, 0].set_title(\"Circles: True Labels\", fontsize=12)\n",
    "\n",
    "# Circles - DBSCAN\n",
    "dbscan_circles = DBSCAN(eps=0.2, min_samples=5)\n",
    "labels_circles = dbscan_circles.fit_predict(X_circles)\n",
    "plot_clusters(X_circles, labels_circles, ax=axes[1, 1], title=\"Circles: DBSCAN\")\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "\n",
    "plt.suptitle(\"DBSCAN Successfully Clusters Non-Convex Shapes!\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"DBSCAN correctly identifies both moons and both circles!\")\n",
    "print(\n",
    "    \"Unlike K-Means, DBSCAN follows the DENSITY of points, not distance to centroids.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50752dd5",
   "metadata": {},
   "source": [
    "### 2.3 Sensitivity to eps\n",
    "\n",
    "The **eps** parameter defines the neighborhood size. Too small = everything is noise. Too large = everything is one cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f86fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN with different eps values\n",
    "eps_values = [0.05, 0.1, 0.2, 0.3, 0.5, 1.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, eps in zip(axes, eps_values):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=5)\n",
    "    labels = dbscan.fit_predict(X_moons)\n",
    "\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = (labels == -1).sum()\n",
    "\n",
    "    plot_clusters(X_moons, labels, ax=ax, title=f\"eps = {eps}\")\n",
    "    ax.set_title(f\"eps = {eps}\\nClusters: {n_clusters}, Noise: {n_noise}\", fontsize=11)\n",
    "\n",
    "plt.suptitle(\"DBSCAN Sensitivity to eps (min_samples=5)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"eps too small (0.05): Almost everything is noise — neighborhoods are too tight\")\n",
    "print(\"eps too large (1.0): Everything merges into one cluster\")\n",
    "print(\"eps just right (0.2): Correctly identifies the two moons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdb9366",
   "metadata": {},
   "source": [
    "### 2.4 Sensitivity to min_samples (MinPts)\n",
    "\n",
    "The **min_samples** parameter controls how many neighbors a point needs to be considered a core point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d54102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN with different min_samples values\n",
    "min_samples_values = [2, 3, 5, 10, 20, 50]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, min_samples in zip(axes, min_samples_values):\n",
    "    dbscan = DBSCAN(eps=0.2, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(X_moons)\n",
    "\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = (labels == -1).sum()\n",
    "\n",
    "    plot_clusters(X_moons, labels, ax=ax, title=f\"min_samples = {min_samples}\")\n",
    "    ax.set_title(\n",
    "        f\"min_samples = {min_samples}\\nClusters: {n_clusters}, Noise: {n_noise}\",\n",
    "        fontsize=11,\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"DBSCAN Sensitivity to min_samples (eps=0.2)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"min_samples too low (2): May create spurious clusters from noise\")\n",
    "print(\"min_samples too high (50): Too strict — sparse regions become noise\")\n",
    "print(\"min_samples just right (5-10): Good balance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d71ff",
   "metadata": {},
   "source": [
    "### 2.5 DBSCAN Limitation: Varying Density\n",
    "\n",
    "DBSCAN uses a **global eps** parameter. When clusters have very different densities, one eps value can't fit all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f884f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN on varying density clusters\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# True labels\n",
    "axes[0].scatter(\n",
    "    X_varied[:, 0],\n",
    "    X_varied[:, 1],\n",
    "    c=y_varied,\n",
    "    cmap=\"viridis\",\n",
    "    s=50,\n",
    "    edgecolors=\"k\",\n",
    "    linewidths=0.5,\n",
    ")\n",
    "axes[0].set_title(\"True Labels\\n(3 clusters of different densities)\", fontsize=11)\n",
    "\n",
    "# DBSCAN with small eps (good for dense cluster)\n",
    "dbscan_small = DBSCAN(eps=0.3, min_samples=5)\n",
    "labels_small = dbscan_small.fit_predict(X_varied)\n",
    "n_clusters = len(set(labels_small)) - (1 if -1 in labels_small else 0)\n",
    "plot_clusters(X_varied, labels_small, ax=axes[1])\n",
    "axes[1].set_title(\n",
    "    f\"DBSCAN (eps=0.3)\\nFinds {n_clusters} clusters, misses sparse one\", fontsize=11\n",
    ")\n",
    "\n",
    "# DBSCAN with large eps (good for sparse cluster)\n",
    "dbscan_large = DBSCAN(eps=0.8, min_samples=5)\n",
    "labels_large = dbscan_large.fit_predict(X_varied)\n",
    "n_clusters = len(set(labels_large)) - (1 if -1 in labels_large else 0)\n",
    "plot_clusters(X_varied, labels_large, ax=axes[2])\n",
    "axes[2].set_title(\n",
    "    f\"DBSCAN (eps=0.8)\\nFinds {n_clusters} clusters, merges dense ones\", fontsize=11\n",
    ")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "\n",
    "plt.suptitle(\"DBSCAN Struggles with Varying Density\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The dilemma: No single eps works for all clusters!\")\n",
    "print(\"• Small eps: Misses the sparse (spread out) cluster\")\n",
    "print(\"• Large eps: Merges the dense (tight) clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb090e6",
   "metadata": {},
   "source": [
    "### 2.6 Your Turn: Tune DBSCAN Parameters\n",
    "\n",
    "**Task:** Find good eps and min_samples values to correctly cluster this noisy dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99701ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with noise\n",
    "X_noisy, y_noisy = make_blobs(\n",
    "    n_samples=200, centers=3, cluster_std=0.4, random_state=42\n",
    ")\n",
    "# Add uniform noise\n",
    "noise = np.random.uniform(low=-6, high=6, size=(30, 2))\n",
    "X_noisy = np.vstack([X_noisy, noise])\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_noisy[:, 0], X_noisy[:, 1], s=30, edgecolors=\"k\", linewidths=0.5)\n",
    "plt.title(\"Noisy Dataset - Find the 3 clusters and identify noise\", fontsize=12)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Goal: Identify 3 clusters and label the scattered points as noise (-1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6d9f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different eps and min_samples values\n",
    "# Try to find parameters that:\n",
    "# 1. Correctly identify 3 clusters\n",
    "# 2. Label scattered points as noise (gray X markers)\n",
    "\n",
    "# eps = ???\n",
    "# min_samples = ???\n",
    "\n",
    "# dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "# labels = dbscan.fit_predict(X_noisy)\n",
    "\n",
    "# plot_clusters(X_noisy, labels, title=f'DBSCAN (eps={eps}, min_samples={min_samples})')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ff769",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Hierarchical Clustering & Dendrograms\n",
    "\n",
    "### How Hierarchical Clustering Works\n",
    "\n",
    "**Agglomerative (bottom-up) clustering:**\n",
    "\n",
    "1. Start with each point as its own cluster\n",
    "2. Repeatedly merge the two closest clusters\n",
    "3. Stop when all points are in one cluster (or desired number reached)\n",
    "\n",
    "**Linkage methods** (how to measure distance between clusters):\n",
    "\n",
    "- **Single linkage:** Distance between closest points (tends to create elongated clusters)\n",
    "- **Complete linkage:** Distance between farthest points (tends to create compact clusters)\n",
    "- **Average linkage:** Average distance between all pairs\n",
    "- **Ward linkage:** Minimizes increase in total within-cluster variance (often best for spherical)\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- No need to specify K upfront\n",
    "- Produces a **dendrogram** — a tree showing the merge hierarchy\n",
    "- Can cut the tree at different heights to get different numbers of clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f84290",
   "metadata": {},
   "source": [
    "### 3.1 Understanding Dendrograms\n",
    "\n",
    "A **dendrogram** shows the hierarchical relationships between clusters. The **y-axis** shows the distance (or dissimilarity) at which clusters are merged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f148c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a smaller dataset for clearer visualization\n",
    "X_small, y_small = make_blobs(n_samples=50, centers=3, cluster_std=0.6, random_state=42)\n",
    "\n",
    "# Compute linkage matrix\n",
    "Z = linkage(X_small, method=\"ward\")\n",
    "\n",
    "# Plot dendrogram\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "dendrogram(Z, ax=ax, leaf_rotation=90, leaf_font_size=8)\n",
    "\n",
    "ax.set_xlabel(\"Sample Index\", fontsize=12)\n",
    "ax.set_ylabel(\"Distance (Ward)\", fontsize=12)\n",
    "ax.set_title(\"Dendrogram: Hierarchical Clustering with Ward Linkage\", fontsize=14)\n",
    "\n",
    "# Add horizontal lines to show different cuts\n",
    "ax.axhline(y=5, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Cut for 3 clusters\")\n",
    "ax.axhline(y=10, color=\"blue\", linestyle=\"--\", alpha=0.7, label=\"Cut for 2 clusters\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"How to read a dendrogram:\")\n",
    "print(\"• Each leaf at the bottom is a data point\")\n",
    "print(\"• Vertical lines show merges; height = distance at merge\")\n",
    "print(\"• Cutting horizontally gives different numbers of clusters\")\n",
    "print(\"• Red line (height 5) → 3 clusters | Blue line (height 10) → 2 clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccb0292",
   "metadata": {},
   "source": [
    "### 3.2 Cutting the Dendrogram\n",
    "\n",
    "We can cut the dendrogram at different heights to obtain different numbers of clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d2a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut dendrogram at different heights / numbers of clusters\n",
    "n_clusters_list = [2, 3, 4, 5]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "for ax, n_clusters in zip(axes, n_clusters_list):\n",
    "    # Agglomerative clustering\n",
    "    agg = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"ward\")\n",
    "    labels = agg.fit_predict(X_small)\n",
    "\n",
    "    # Plot\n",
    "    scatter = ax.scatter(\n",
    "        X_small[:, 0],\n",
    "        X_small[:, 1],\n",
    "        c=labels,\n",
    "        cmap=\"viridis\",\n",
    "        s=60,\n",
    "        edgecolors=\"k\",\n",
    "        linewidths=0.5,\n",
    "    )\n",
    "    ax.set_title(f\"{n_clusters} Clusters\", fontsize=12)\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "\n",
    "plt.suptitle(\"Cutting the Dendrogram at Different Levels\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The dendrogram lets us explore clustering at multiple levels of granularity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9404e52f",
   "metadata": {},
   "source": [
    "### 3.3 Linkage Methods Comparison\n",
    "\n",
    "Different linkage methods can produce very different cluster structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db69571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare linkage methods\n",
    "linkage_methods = [\"single\", \"complete\", \"average\", \"ward\"]\n",
    "\n",
    "# Create a dataset that shows the differences\n",
    "X_link = X_blobs.copy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 10))\n",
    "\n",
    "for i, method in enumerate(linkage_methods):\n",
    "    # Compute linkage\n",
    "    Z = linkage(X_link, method=method)\n",
    "\n",
    "    # Dendrogram\n",
    "    dendrogram(\n",
    "        Z,\n",
    "        ax=axes[0, i],\n",
    "        leaf_rotation=90,\n",
    "        leaf_font_size=6,\n",
    "        truncate_mode=\"lastp\",\n",
    "        p=30,\n",
    "    )  # Show only last 30 merges\n",
    "    axes[0, i].set_title(f\"{method.capitalize()} Linkage\", fontsize=12)\n",
    "    axes[0, i].set_xlabel(\"Cluster\")\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel(\"Distance\")\n",
    "\n",
    "    # Clustering result with 3 clusters\n",
    "    agg = AgglomerativeClustering(n_clusters=3, linkage=method)\n",
    "    labels = agg.fit_predict(X_link)\n",
    "\n",
    "    axes[1, i].scatter(\n",
    "        X_link[:, 0],\n",
    "        X_link[:, 1],\n",
    "        c=labels,\n",
    "        cmap=\"viridis\",\n",
    "        s=40,\n",
    "        edgecolors=\"k\",\n",
    "        linewidths=0.5,\n",
    "    )\n",
    "    axes[1, i].set_xlabel(\"Feature 1\")\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel(\"Feature 2\")\n",
    "\n",
    "plt.suptitle(\"Comparison of Linkage Methods (3 Clusters)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Linkage method effects:\")\n",
    "print(\"• Single: Tends to create 'chaining' — elongated clusters\")\n",
    "print(\"• Complete: Creates compact, similar-diameter clusters\")\n",
    "print(\"• Average: Compromise between single and complete\")\n",
    "print(\"• Ward: Minimizes variance — often best for spherical clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d7e722",
   "metadata": {},
   "source": [
    "### 3.4 The Chaining Effect (Single Linkage)\n",
    "\n",
    "**Single linkage** can create long, chain-like clusters because it only considers the closest pair of points between clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create elongated clusters to demonstrate chaining\n",
    "np.random.seed(42)\n",
    "n_points = 100\n",
    "\n",
    "# Two elongated clusters with a potential bridge\n",
    "cluster1_x = np.linspace(0, 5, n_points) + np.random.normal(0, 0.2, n_points)\n",
    "cluster1_y = np.random.normal(0, 0.3, n_points)\n",
    "\n",
    "cluster2_x = np.linspace(0, 5, n_points) + np.random.normal(0, 0.2, n_points)\n",
    "cluster2_y = np.random.normal(2, 0.3, n_points)\n",
    "\n",
    "# Add a few points that bridge the clusters\n",
    "bridge_x = np.array([2.5, 2.6])\n",
    "bridge_y = np.array([0.7, 1.3])\n",
    "\n",
    "X_chain = np.vstack(\n",
    "    [\n",
    "        np.column_stack([cluster1_x, cluster1_y]),\n",
    "        np.column_stack([cluster2_x, cluster2_y]),\n",
    "        np.column_stack([bridge_x, bridge_y]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compare single vs ward linkage\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Original data\n",
    "axes[0].scatter(X_chain[:, 0], X_chain[:, 1], s=30, edgecolors=\"k\", linewidths=0.5)\n",
    "axes[0].scatter(bridge_x, bridge_y, c=\"red\", s=100, marker=\"*\", label=\"Bridge points\")\n",
    "axes[0].set_title(\"Data with Bridge Points\", fontsize=12)\n",
    "axes[0].legend()\n",
    "\n",
    "# Single linkage\n",
    "agg_single = AgglomerativeClustering(n_clusters=2, linkage=\"single\")\n",
    "labels_single = agg_single.fit_predict(X_chain)\n",
    "axes[1].scatter(\n",
    "    X_chain[:, 0],\n",
    "    X_chain[:, 1],\n",
    "    c=labels_single,\n",
    "    cmap=\"viridis\",\n",
    "    s=30,\n",
    "    edgecolors=\"k\",\n",
    "    linewidths=0.5,\n",
    ")\n",
    "axes[1].set_title(\n",
    "    \"Single Linkage (2 clusters)\\nChaining creates ONE cluster!\", fontsize=11\n",
    ")\n",
    "\n",
    "# Ward linkage\n",
    "agg_ward = AgglomerativeClustering(n_clusters=2, linkage=\"ward\")\n",
    "labels_ward = agg_ward.fit_predict(X_chain)\n",
    "axes[2].scatter(\n",
    "    X_chain[:, 0],\n",
    "    X_chain[:, 1],\n",
    "    c=labels_ward,\n",
    "    cmap=\"viridis\",\n",
    "    s=30,\n",
    "    edgecolors=\"k\",\n",
    "    linewidths=0.5,\n",
    ")\n",
    "axes[2].set_title(\n",
    "    \"Ward Linkage (2 clusters)\\nCorrectly separates clusters\", fontsize=11\n",
    ")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "\n",
    "plt.suptitle(\"The Chaining Effect in Single Linkage\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    \"Single linkage chains through the bridge points, merging both elongated clusters.\"\n",
    ")\n",
    "print(\"Ward linkage is more robust to such 'bridges'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb89e5",
   "metadata": {},
   "source": [
    "### 3.5 Using Dendrograms to Choose K\n",
    "\n",
    "Look for **large jumps** in the dendrogram — they indicate natural cluster boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd158a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use blobs dataset\n",
    "Z = linkage(X_blobs, method=\"ward\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Full dendrogram\n",
    "dendrogram(Z, ax=axes[0], truncate_mode=\"lastp\", p=50, leaf_rotation=90)\n",
    "axes[0].axhline(y=15, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "axes[0].set_title(\"Dendrogram (Ward Linkage)\\nLook for large gaps!\", fontsize=12)\n",
    "axes[0].set_xlabel(\"Cluster\")\n",
    "axes[0].set_ylabel(\"Distance\")\n",
    "\n",
    "# Distance vs number of clusters\n",
    "# Get the distances at which merges occur\n",
    "distances = Z[:, 2]\n",
    "n_clusters_range = range(1, 11)\n",
    "\n",
    "# For n clusters, we need to look at the (n-1)th largest distance\n",
    "sorted_distances = np.sort(distances)[::-1]\n",
    "cluster_distances = sorted_distances[:10]\n",
    "\n",
    "axes[1].plot(n_clusters_range, cluster_distances, \"bo-\", linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel(\"Number of Clusters\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Distance at Merge\", fontsize=12)\n",
    "axes[1].set_title(\n",
    "    \"Merge Distance vs Number of Clusters\\n(Large drop suggests natural boundary)\",\n",
    "    fontsize=12,\n",
    ")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axvline(x=3, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Suggested K=3\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The large gap in the dendrogram around height 15 suggests K=3 clusters.\")\n",
    "print(\"The right plot shows a significant drop in merge distance after 3 clusters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66facd1a",
   "metadata": {},
   "source": [
    "### 3.6 Hierarchical Clustering on the Iris Dataset\n",
    "\n",
    "Let's apply hierarchical clustering to a real dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acbd94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris)\n",
    "\n",
    "# Compute linkage\n",
    "Z_iris = linkage(X_iris_scaled, method=\"ward\")\n",
    "\n",
    "# Plot dendrogram\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "dendrogram(Z_iris, ax=ax, truncate_mode=\"lastp\", p=30, leaf_rotation=90)\n",
    "ax.axhline(y=7, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Cut for 3 clusters\")\n",
    "ax.set_xlabel(\"Cluster\", fontsize=12)\n",
    "ax.set_ylabel(\"Distance (Ward)\", fontsize=12)\n",
    "ax.set_title(\"Dendrogram of Iris Dataset\", fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Iris dataset has 3 species: {list(target_names)}\")\n",
    "print(\"The dendrogram shows a natural split into 2 or 3 clusters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60089c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare clustering result with true labels\n",
    "agg_iris = AgglomerativeClustering(n_clusters=3, linkage=\"ward\")\n",
    "labels_iris = agg_iris.fit_predict(X_iris_scaled)\n",
    "\n",
    "# Visualize using first two principal components for 2D projection\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_iris_2d = pca.fit_transform(X_iris_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# True labels\n",
    "for i, name in enumerate(target_names):\n",
    "    mask = y_iris == i\n",
    "    axes[0].scatter(\n",
    "        X_iris_2d[mask, 0],\n",
    "        X_iris_2d[mask, 1],\n",
    "        s=50,\n",
    "        edgecolors=\"k\",\n",
    "        linewidths=0.5,\n",
    "        label=name,\n",
    "    )\n",
    "axes[0].set_title(\"True Species Labels\", fontsize=12)\n",
    "axes[0].legend()\n",
    "\n",
    "# Hierarchical clustering result\n",
    "for i in range(3):\n",
    "    mask = labels_iris == i\n",
    "    axes[1].scatter(\n",
    "        X_iris_2d[mask, 0],\n",
    "        X_iris_2d[mask, 1],\n",
    "        s=50,\n",
    "        edgecolors=\"k\",\n",
    "        linewidths=0.5,\n",
    "        label=f\"Cluster {i}\",\n",
    "    )\n",
    "axes[1].set_title(\"Hierarchical Clustering (Ward, K=3)\", fontsize=12)\n",
    "axes[1].legend()\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Iris Dataset: True Labels vs Hierarchical Clustering\", fontsize=14, y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute accuracy (note: cluster labels may not match species labels directly)\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "ari = adjusted_rand_score(y_iris, labels_iris)\n",
    "print(f\"Adjusted Rand Index: {ari:.3f}\")\n",
    "print(\"(1.0 = perfect match, 0.0 = random)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f617ba",
   "metadata": {},
   "source": [
    "### 3.7 Your Turn: Interpret a Dendrogram\n",
    "\n",
    "**Task:** Use the dendrogram to determine a good number of clusters for this mystery dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa56b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mystery dataset\n",
    "X_hier_mystery, _ = make_blobs(\n",
    "    n_samples=200, centers=4, cluster_std=[0.5, 0.8, 0.6, 0.7], random_state=77\n",
    ")\n",
    "\n",
    "# TODO: Compute linkage (try 'ward')\n",
    "# Z = linkage(X_hier_mystery, method='ward')\n",
    "\n",
    "# TODO: Plot dendrogram\n",
    "# dendrogram(Z, ...)\n",
    "\n",
    "# TODO: Based on the dendrogram, what's a good number of clusters?\n",
    "# Look for large gaps in the merge distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efdd78f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Algorithm Comparison\n",
    "\n",
    "Now let's compare all three clustering algorithms head-to-head on different types of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921a4c2c",
   "metadata": {},
   "source": [
    "### 4.1 Visual Comparison Grid\n",
    "\n",
    "Let's see how each algorithm performs on each dataset type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9070b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and their characteristics\n",
    "datasets = [\n",
    "    (X_blobs, \"Spherical Blobs\", 3),\n",
    "    (X_moons, \"Two Moons\", 2),\n",
    "    (X_circles, \"Concentric Circles\", 2),\n",
    "    (X_varied, \"Varying Density\", 3),\n",
    "    (X_outliers, \"With Outliers\", 2),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(5, 4, figsize=(18, 22))\n",
    "\n",
    "for row, (X, name, true_k) in enumerate(datasets):\n",
    "    # Standardize for fair comparison\n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Original data\n",
    "    axes[row, 0].scatter(X_std[:, 0], X_std[:, 1], s=30, edgecolors=\"k\", linewidths=0.5)\n",
    "    axes[row, 0].set_title(f\"{name}\\n(Original Data)\", fontsize=10)\n",
    "\n",
    "    # K-Means\n",
    "    kmeans = KMeans(n_clusters=true_k, random_state=42, n_init=10)\n",
    "    labels_km = kmeans.fit_predict(X_std)\n",
    "    axes[row, 1].scatter(\n",
    "        X_std[:, 0],\n",
    "        X_std[:, 1],\n",
    "        c=labels_km,\n",
    "        cmap=\"viridis\",\n",
    "        s=30,\n",
    "        edgecolors=\"k\",\n",
    "        linewidths=0.5,\n",
    "    )\n",
    "    axes[row, 1].scatter(\n",
    "        kmeans.cluster_centers_[:, 0],\n",
    "        kmeans.cluster_centers_[:, 1],\n",
    "        c=\"red\",\n",
    "        marker=\"X\",\n",
    "        s=150,\n",
    "        edgecolors=\"k\",\n",
    "    )\n",
    "    axes[row, 1].set_title(f\"K-Means (K={true_k})\", fontsize=10)\n",
    "\n",
    "    # DBSCAN (with tuned parameters)\n",
    "    # Different eps for different datasets\n",
    "    if name == \"Spherical Blobs\":\n",
    "        eps = 0.5\n",
    "    elif name == \"Varying Density\":\n",
    "        eps = 0.5\n",
    "    else:\n",
    "        eps = 0.3\n",
    "\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=5)\n",
    "    labels_db = dbscan.fit_predict(X_std)\n",
    "    n_clusters_db = len(set(labels_db)) - (1 if -1 in labels_db else 0)\n",
    "    n_noise = (labels_db == -1).sum()\n",
    "\n",
    "    # Color noise differently\n",
    "    colors = labels_db.copy().astype(float)\n",
    "    colors[labels_db == -1] = -1  # Noise\n",
    "    scatter = axes[row, 2].scatter(\n",
    "        X_std[:, 0],\n",
    "        X_std[:, 1],\n",
    "        c=colors,\n",
    "        cmap=\"viridis\",\n",
    "        s=30,\n",
    "        edgecolors=\"k\",\n",
    "        linewidths=0.5,\n",
    "    )\n",
    "    # Mark noise points\n",
    "    noise_mask = labels_db == -1\n",
    "    if noise_mask.any():\n",
    "        axes[row, 2].scatter(\n",
    "            X_std[noise_mask, 0],\n",
    "            X_std[noise_mask, 1],\n",
    "            c=\"gray\",\n",
    "            marker=\"x\",\n",
    "            s=50,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "    axes[row, 2].set_title(\n",
    "        f\"DBSCAN (eps={eps})\\n{n_clusters_db} clusters, {n_noise} noise\", fontsize=10\n",
    "    )\n",
    "\n",
    "    # Hierarchical (Ward)\n",
    "    agg = AgglomerativeClustering(n_clusters=true_k, linkage=\"ward\")\n",
    "    labels_agg = agg.fit_predict(X_std)\n",
    "    axes[row, 3].scatter(\n",
    "        X_std[:, 0],\n",
    "        X_std[:, 1],\n",
    "        c=labels_agg,\n",
    "        cmap=\"viridis\",\n",
    "        s=30,\n",
    "        edgecolors=\"k\",\n",
    "        linewidths=0.5,\n",
    "    )\n",
    "    axes[row, 3].set_title(f\"Hierarchical (Ward, K={true_k})\", fontsize=10)\n",
    "\n",
    "# Add column headers\n",
    "col_titles = [\"Data\", \"K-Means\", \"DBSCAN\", \"Hierarchical\"]\n",
    "for ax, title in zip(axes[0], col_titles):\n",
    "    ax.annotate(\n",
    "        title,\n",
    "        xy=(0.5, 1.15),\n",
    "        xycoords=\"axes fraction\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        ha=\"center\",\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"Clustering Algorithm Comparison\", fontsize=16, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b533f11",
   "metadata": {},
   "source": [
    "### 4.2 Performance Summary\n",
    "\n",
    "Let's summarize how each algorithm performed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d16b08",
   "metadata": {},
   "source": [
    "| Dataset            | K-Means     | DBSCAN      | Hierarchical | Best Choice            |\n",
    "| ------------------ | ----------- | ----------- | ------------ | ---------------------- |\n",
    "| Spherical Blobs    | ✓ Excellent | ✓ Good      | ✓ Excellent  | K-Means / Hierarchical |\n",
    "| Two Moons          | ✗ Fails     | ✓ Excellent | ⚠ Okay       | DBSCAN                 |\n",
    "| Concentric Circles | ✗ Fails     | ✓ Excellent | ⚠ Okay       | DBSCAN                 |\n",
    "| Varying Density    | ✓ Good      | ⚠ Struggles | ✓ Good       | K-Means                |\n",
    "| With Outliers      | ⚠ Affected  | ✓ Robust    | ⚠ Affected   | DBSCAN                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f87144f",
   "metadata": {},
   "source": [
    "### 4.3 When to Use Each Algorithm\n",
    "\n",
    "Here's a decision guide based on your data characteristics:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab49126",
   "metadata": {},
   "source": [
    "| Algorithm        | Use When                               | Avoid When                             |\n",
    "| ---------------- | -------------------------------------- | -------------------------------------- |\n",
    "| **K-Means**      | Spherical clusters of similar size     | Non-convex shapes (moons, circles)     |\n",
    "|                  | You know or can estimate K             | Data with many outliers                |\n",
    "|                  | Large datasets (scales well)           |                                        |\n",
    "| **DBSCAN**       | Arbitrarily shaped clusters            | Clusters of very different densities   |\n",
    "|                  | Data has outliers/noise                | High-dimensional data                  |\n",
    "|                  | Unknown number of clusters             |                                        |\n",
    "| **Hierarchical** | Want to explore multiple granularities | Very large datasets (O(n²) complexity) |\n",
    "|                  | Need to understand cluster hierarchy   | Non-convex shapes (with Ward)          |\n",
    "|                  | Don't know K upfront                   |                                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d5bd1",
   "metadata": {},
   "source": [
    "### 4.4 Your Turn: Choose the Right Algorithm\n",
    "\n",
    "**Task:** For each dataset below, choose the most appropriate clustering algorithm and justify your choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c03843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mystery dataset 1\n",
    "np.random.seed(55)\n",
    "X_m1 = np.vstack(\n",
    "    [\n",
    "        np.random.randn(100, 2) * 0.5 + [0, 0],\n",
    "        np.random.randn(100, 2) * 0.5 + [4, 0],\n",
    "        np.random.randn(100, 2) * 0.5 + [2, 3],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Mystery dataset 2\n",
    "t = np.linspace(0, 2 * np.pi, 200)\n",
    "X_m2 = np.vstack(\n",
    "    [\n",
    "        np.column_stack(\n",
    "            [\n",
    "                np.cos(t) + np.random.randn(200) * 0.1,\n",
    "                np.sin(t) + np.random.randn(200) * 0.1,\n",
    "            ]\n",
    "        ),\n",
    "        np.column_stack(\n",
    "            [\n",
    "                3 * np.cos(t) + np.random.randn(200) * 0.15,\n",
    "                3 * np.sin(t) + np.random.randn(200) * 0.15,\n",
    "            ]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(X_m1[:, 0], X_m1[:, 1], s=30, edgecolors=\"k\", linewidths=0.5)\n",
    "axes[0].set_title(\"Mystery Dataset 1\\nWhich algorithm?\", fontsize=12)\n",
    "\n",
    "axes[1].scatter(X_m2[:, 0], X_m2[:, 1], s=30, edgecolors=\"k\", linewidths=0.5)\n",
    "axes[1].set_title(\"Mystery Dataset 2\\nWhich algorithm?\", fontsize=12)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Think about:\")\n",
    "print(\"• What shape are the clusters?\")\n",
    "print(\"• How many clusters do you see?\")\n",
    "print(\"• Are there outliers?\")\n",
    "print(\"• Are the clusters similar in density/size?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87017c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For Mystery Dataset 1\n",
    "# What algorithm would you choose? Why?\n",
    "# Hint: What shape are the clusters?\n",
    "\n",
    "# TODO: For Mystery Dataset 2\n",
    "# What algorithm would you choose? Why?\n",
    "# Hint: Can a straight line separate these clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b4c89",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **Why does K-Means fail on the moons dataset?** What assumption does K-Means make about cluster shapes?\n",
    "\n",
    "2. **What happens to DBSCAN when clusters have very different densities?** Why is this a fundamental limitation?\n",
    "\n",
    "3. **How would you choose between single linkage and Ward linkage** for hierarchical clustering? When might single linkage be appropriate despite the chaining effect?\n",
    "\n",
    "4. **If you don't know anything about your data**, which algorithm would you try first, and why?\n",
    "\n",
    "5. **How could you validate clustering results** when you don't have true labels?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa6bf9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **K-Means** partitions data by minimizing within-cluster variance. It works best on spherical clusters of similar size, but requires specifying K and is sensitive to initialization.\n",
    "\n",
    "2. **DBSCAN** finds density-based clusters of arbitrary shape and automatically identifies outliers. It doesn't require K but needs eps and min_samples parameters, and struggles with varying-density clusters.\n",
    "\n",
    "3. **Hierarchical clustering** builds a tree of clusters (dendrogram) that shows relationships at multiple levels. It's useful when you want to explore different granularities but doesn't scale well to large datasets.\n",
    "\n",
    "4. **No single algorithm is best for all data** — the choice depends on cluster shapes, density, outliers, and whether you know K.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Use the **elbow method** or **silhouette score** to choose K for K-Means\n",
    "- DBSCAN excels at **non-convex shapes** and **outlier detection**\n",
    "- **Dendrograms** help visualize hierarchical relationships and choose the number of clusters\n",
    "- Always **visualize your clustering results** to validate they make sense\n",
    "- Consider **scaling your features** before clustering (especially for K-Means and hierarchical)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ing3513",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
