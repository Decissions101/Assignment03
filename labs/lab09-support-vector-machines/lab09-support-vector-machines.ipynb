{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca526848",
   "metadata": {},
   "source": [
    "# Lab 09: Support Vector Machines (SVM)\n",
    "\n",
    "In this lab, you'll develop intuition for how Support Vector Machines work, explore different kernels, and learn to tune hyperparameters using cross-validation.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Understand the geometric intuition of SVM (margins, support vectors)\n",
    "- Know when to use linear vs. non-linear kernels\n",
    "- Apply cross-validation for hyperparameter tuning\n",
    "- Interpret classifier performance metrics (precision, recall, ROC curves)\n",
    "\n",
    "### Overview\n",
    "\n",
    "| Part | Topic                                    | Time    |\n",
    "| ---- | ---------------------------------------- | ------- |\n",
    "| 1    | Linear SVM & Margins                     | ~30 min |\n",
    "| 2    | Kernels (RBF, Polynomial)                | ~30 min |\n",
    "| 3    | Hyperparameter Tuning + Cross-Validation | ~30 min |\n",
    "| 4    | Real-World Application (Stretch)         | ~30 min |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ebd2ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_blobs, make_circles, make_moons, load_digits\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    RocCurveDisplay,\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce5f76",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "We'll use these functions throughout the lab to visualize SVM decision boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae9e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm_decision_boundary(\n",
    "    clf, X, y, ax=None, title=\"SVM Decision Boundary\", show_margin=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary, margins, and support vectors for an SVM classifier.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    clf : fitted SVM classifier\n",
    "    X : feature array (n_samples, 2)\n",
    "    y : target array\n",
    "    ax : matplotlib axis (optional)\n",
    "    title : plot title\n",
    "    show_margin : whether to show margin lines (only works for linear kernel)\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "\n",
    "    # Get decision function values\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot decision boundary and margins\n",
    "    ax.contourf(xx, yy, Z, levels=50, cmap=\"RdBu\", alpha=0.3)\n",
    "    ax.contour(xx, yy, Z, levels=[0], colors=\"k\", linewidths=2)  # Decision boundary\n",
    "\n",
    "    if show_margin and clf.kernel == \"linear\":\n",
    "        ax.contour(\n",
    "            xx, yy, Z, levels=[-1, 1], colors=\"k\", linewidths=1, linestyles=\"--\"\n",
    "        )  # Margins\n",
    "\n",
    "    # Plot data points\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"RdBu\", edgecolors=\"k\", s=50)\n",
    "\n",
    "    # Highlight support vectors\n",
    "    ax.scatter(\n",
    "        clf.support_vectors_[:, 0],\n",
    "        clf.support_vectors_[:, 1],\n",
    "        s=200,\n",
    "        facecolors=\"none\",\n",
    "        edgecolors=\"green\",\n",
    "        linewidths=2,\n",
    "        label=f\"Support Vectors (n={len(clf.support_vectors_)})\",\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd435336",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Visual Intuition of SVM in 2D\n",
    "\n",
    "### The SVM Idea\n",
    "\n",
    "A **Support Vector Machine** finds the hyperplane that maximizes the **margin** — the distance between the hyperplane and the nearest data points from each class. These nearest points are called **support vectors**.\n",
    "\n",
    "For linearly separable data in 2D, we're looking for a line:\n",
    "$$\\mathbf{w}^T \\mathbf{x} + b = 0$$\n",
    "\n",
    "The margin is $\\frac{2}{\\|\\mathbf{w}\\|}$, so maximizing the margin means minimizing $\\|\\mathbf{w}\\|$.\n",
    "\n",
    "The **regularization parameter C** controls the trade-off between:\n",
    "\n",
    "- Maximizing the margin (small C → wider margin, more misclassifications allowed)\n",
    "- Minimizing classification errors (large C → narrower margin, fewer misclassifications)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a3dcb",
   "metadata": {},
   "source": [
    "### 1.1 Generate and Plot Linearly Separable Data\n",
    "\n",
    "Let's create a simple 2D dataset with two linearly separable classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6774ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate linearly separable data\n",
    "X_linear, y_linear = make_blobs(\n",
    "    n_samples=100, centers=2, cluster_std=1.5, random_state=42\n",
    ")\n",
    "\n",
    "# Plot the raw data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(\n",
    "    X_linear[:, 0], X_linear[:, 1], c=y_linear, cmap=\"RdBu\", edgecolors=\"k\", s=50\n",
    ")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Linearly Separable Data\")\n",
    "plt.colorbar(label=\"Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8505b9",
   "metadata": {},
   "source": [
    "### 1.2 Fit a Linear SVM\n",
    "\n",
    "**Task:** Fit a linear SVM to the data and visualize the decision boundary, margins, and support vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d452695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear SVM with default C=1\n",
    "svm_linear = SVC(kernel=\"linear\", C=1.0)\n",
    "svm_linear.fit(X_linear, y_linear)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "plot_svm_decision_boundary(\n",
    "    svm_linear,\n",
    "    X_linear,\n",
    "    y_linear,\n",
    "    ax=ax,\n",
    "    title=f\"Linear SVM (C=1.0)\\nSupport Vectors: {len(svm_linear.support_vectors_)}\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Number of support vectors: {len(svm_linear.support_vectors_)}\")\n",
    "print(f\"Training accuracy: {svm_linear.score(X_linear, y_linear):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3bd453",
   "metadata": {},
   "source": [
    "### 1.3 Effect of the Regularization Parameter C\n",
    "\n",
    "The parameter **C** controls the trade-off between a smooth decision boundary and classifying training points correctly.\n",
    "\n",
    "- **Small C:** Wider margin, more tolerant of misclassifications (soft margin)\n",
    "- **Large C:** Narrower margin, less tolerant of misclassifications (hard margin)\n",
    "\n",
    "**Task:** Fit SVMs with different values of C and observe how the margin and support vectors change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a80ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different values of C\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "for ax, C in zip(axes, C_values):\n",
    "    svm = SVC(kernel=\"linear\", C=C)\n",
    "    svm.fit(X_linear, y_linear)\n",
    "    plot_svm_decision_boundary(\n",
    "        svm, X_linear, y_linear, ax=ax, title=f\"C={C}\\nSV: {len(svm.support_vectors_)}\"\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3734b2a7",
   "metadata": {},
   "source": [
    "**Discussion Questions:**\n",
    "\n",
    "1. What happens to the number of support vectors as C increases?\n",
    "2. What happens to the margin width as C increases?\n",
    "3. Why might a very large C lead to overfitting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5014953c",
   "metadata": {},
   "source": [
    "### 1.4 Your Turn: Experiment with C\n",
    "\n",
    "**Task:** Create a dataset with some overlap between classes (increase `cluster_std`), then compare how different C values affect the decision boundary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba21c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate data with more overlap (try cluster_std=2.5 or higher)\n",
    "# X_overlap, y_overlap = make_blobs(...)\n",
    "\n",
    "# TODO: Fit SVMs with C=0.1 and C=100 and compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a04c63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Nonlinear SVM with Kernels\n",
    "\n",
    "### The Kernel Trick\n",
    "\n",
    "What if the data isn't linearly separable? The **kernel trick** allows SVM to find nonlinear decision boundaries by implicitly mapping data to a higher-dimensional space.\n",
    "\n",
    "Common kernels:\n",
    "\n",
    "- **Linear:** $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j$\n",
    "- **Polynomial:** $K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i^T \\mathbf{x}_j + r)^d$\n",
    "- **RBF (Radial Basis Function):** $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d3ce80",
   "metadata": {},
   "source": [
    "### 2.1 Non-Linearly Separable Data\n",
    "\n",
    "Let's create datasets that cannot be separated by a straight line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e218031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-linearly separable datasets\n",
    "# Note: We use moderate noise to create realistic overlap between classes\n",
    "X_circles, y_circles = make_circles(\n",
    "    n_samples=300, noise=0.2, factor=0.5, random_state=42\n",
    ")\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.25, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(\n",
    "    X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap=\"RdBu\", edgecolors=\"k\"\n",
    ")\n",
    "axes[0].set_title(\"Circles Dataset\")\n",
    "axes[0].set_xlabel(\"Feature 1\")\n",
    "axes[0].set_ylabel(\"Feature 2\")\n",
    "\n",
    "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=\"RdBu\", edgecolors=\"k\")\n",
    "axes[1].set_title(\"Moons Dataset\")\n",
    "axes[1].set_xlabel(\"Feature 1\")\n",
    "axes[1].set_ylabel(\"Feature 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da33f6cf",
   "metadata": {},
   "source": [
    "### 2.2 Linear SVM Fails\n",
    "\n",
    "Let's see what happens when we try to fit a linear SVM to non-linearly separable data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try linear SVM on circles data\n",
    "svm_linear_circles = SVC(kernel=\"linear\", C=1.0)\n",
    "svm_linear_circles.fit(X_circles, y_circles)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_svm_decision_boundary(\n",
    "    svm_linear_circles,\n",
    "    X_circles,\n",
    "    y_circles,\n",
    "    ax=ax,\n",
    "    title=f\"Linear SVM on Circles\\nAccuracy: {svm_linear_circles.score(X_circles, y_circles):.3f}\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c9f3d",
   "metadata": {},
   "source": [
    "### 2.3 RBF Kernel\n",
    "\n",
    "The **RBF (Radial Basis Function)** kernel can create circular or blob-like decision boundaries.\n",
    "\n",
    "The parameter **gamma** ($\\gamma$) controls the \"reach\" of each training example:\n",
    "\n",
    "- **Small gamma:** Larger similarity radius, smoother decision boundary\n",
    "- **Large gamma:** Smaller similarity radius, more complex decision boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d0e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit RBF SVM on circles data\n",
    "svm_rbf = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\")\n",
    "svm_rbf.fit(X_circles, y_circles)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_svm_decision_boundary(\n",
    "    svm_rbf,\n",
    "    X_circles,\n",
    "    y_circles,\n",
    "    ax=ax,\n",
    "    show_margin=False,\n",
    "    title=f\"RBF SVM on Circles\\nAccuracy: {svm_rbf.score(X_circles, y_circles):.3f}\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cd02f9",
   "metadata": {},
   "source": [
    "### 2.4 Effect of Gamma\n",
    "\n",
    "**Task:** Explore how gamma affects the decision boundary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c105f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different values of gamma\n",
    "gamma_values = [0.1, 0.5, 1, 5, 10]\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "for ax, gamma in zip(axes, gamma_values):\n",
    "    svm = SVC(kernel=\"rbf\", C=1.0, gamma=gamma)\n",
    "    svm.fit(X_circles, y_circles)\n",
    "    plot_svm_decision_boundary(\n",
    "        svm,\n",
    "        X_circles,\n",
    "        y_circles,\n",
    "        ax=ax,\n",
    "        show_margin=False,\n",
    "        title=f\"gamma={gamma}\\nAcc: {svm.score(X_circles, y_circles):.2f}\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c5b046",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "- What happens with very large gamma? (Hint: look for overfitting)\n",
    "- What happens with very small gamma?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ab156e",
   "metadata": {},
   "source": [
    "### 2.5 Polynomial Kernel\n",
    "\n",
    "The **polynomial kernel** can create polynomial decision boundaries. The **degree** parameter controls the complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f133621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare polynomial degrees on moons data\n",
    "degrees = [2, 3, 4, 5]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for ax, degree in zip(axes, degrees):\n",
    "    svm = SVC(kernel=\"poly\", degree=degree, C=1.0, coef0=1)\n",
    "    svm.fit(X_moons, y_moons)\n",
    "    plot_svm_decision_boundary(\n",
    "        svm,\n",
    "        X_moons,\n",
    "        y_moons,\n",
    "        ax=ax,\n",
    "        show_margin=False,\n",
    "        title=f\"Poly (degree={degree})\\nAcc: {svm.score(X_moons, y_moons):.2f}\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a5e04",
   "metadata": {},
   "source": [
    "### 2.6 Your Turn: Compare Kernels\n",
    "\n",
    "**Task:** Fit linear, RBF, and polynomial SVMs to the moons dataset. Which works best?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e4af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit three SVMs with different kernels on X_moons, y_moons\n",
    "# Compare their accuracies and decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75587be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Hyperparameter Tuning with Cross-Validation\n",
    "\n",
    "### The Problem\n",
    "\n",
    "We have multiple hyperparameters to tune:\n",
    "\n",
    "- **C** (regularization strength)\n",
    "- **gamma** (for RBF kernel)\n",
    "- **degree** (for polynomial kernel)\n",
    "\n",
    "How do we find the best combination without overfitting to our test set?\n",
    "\n",
    "### Grid Search with Cross-Validation\n",
    "\n",
    "**Grid Search** systematically tries all combinations of hyperparameters.\n",
    "**Cross-Validation** gives us a robust estimate of performance without touching the test set.\n",
    "\n",
    "Together, they help us find optimal hyperparameters while avoiding overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d07262",
   "metadata": {},
   "source": [
    "### 3.1 Prepare Data with Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabab721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the circles dataset - split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_circles, y_circles, test_size=0.2, random_state=42, stratify=y_circles\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59078c0c",
   "metadata": {},
   "source": [
    "### 3.2 Grid Search for RBF SVM\n",
    "\n",
    "Let's search over a grid of C and gamma values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e05a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\"C\": [0.1, 1, 10, 100], \"gamma\": [0.01, 0.1, 1, 10]}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(kernel=\"rbf\"),\n",
    "    param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring=\"accuracy\",\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b312c1",
   "metadata": {},
   "source": [
    "### 3.3 Visualize Grid Search Results\n",
    "\n",
    "A heatmap helps us understand how performance varies across the parameter space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results into a DataFrame\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Create a pivot table for the heatmap\n",
    "scores = results.pivot(index=\"param_gamma\", columns=\"param_C\", values=\"mean_test_score\")\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    scores, annot=True, fmt=\".3f\", cmap=\"viridis\", cbar_kws={\"label\": \"CV Accuracy\"}\n",
    ")\n",
    "plt.title(\"Grid Search Results: RBF SVM\")\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"gamma\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82032452",
   "metadata": {},
   "source": [
    "### 3.4 Evaluate on Test Set\n",
    "\n",
    "Now we evaluate the best model on our held-out test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738bca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = best_svm.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test accuracy: {test_accuracy:.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Class 0\", \"Class 1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d63f3",
   "metadata": {},
   "source": [
    "### 3.5 Confusion Matrix and ROC Curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_svm,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    display_labels=[\"Class 0\", \"Class 1\"],\n",
    "    cmap=\"Blues\",\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set_title(\"Confusion Matrix\")\n",
    "\n",
    "# ROC Curve\n",
    "RocCurveDisplay.from_estimator(best_svm, X_test, y_test, ax=axes[1])\n",
    "axes[1].plot([0, 1], [0, 1], \"k--\", label=\"Random Classifier\")\n",
    "axes[1].set_title(\"ROC Curve\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7f5525",
   "metadata": {},
   "source": [
    "### 3.6 Your Turn: Grid Search for Polynomial SVM\n",
    "\n",
    "**Task:** Perform grid search to tune C and degree for a polynomial SVM on the moons dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58737932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split moons data into train/test\n",
    "# X_train_m, X_test_m, y_train_m, y_test_m = ...\n",
    "\n",
    "# TODO: Define parameter grid for polynomial SVM (C and degree)\n",
    "# param_grid_poly = {...}\n",
    "\n",
    "# TODO: Run GridSearchCV\n",
    "\n",
    "# TODO: Create a heatmap of results\n",
    "\n",
    "# TODO: Evaluate best model on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6b892b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Real-World Application\n",
    "\n",
    "Now let's apply what you've learned to a real dataset: the **Digits** dataset. This dataset contains 8×8 pixel images of handwritten digits — a classic machine learning benchmark where SVMs have historically performed well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e788c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits dataset\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "print(f\"Dataset shape: {X_digits.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_digits))}\")\n",
    "print(f\"Class distribution: {np.bincount(y_digits)}\")\n",
    "\n",
    "# Visualize some sample digits\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap=\"gray\")\n",
    "    ax.set_title(f\"Label: {y_digits[i]}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Sample Digits from the Dataset\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdefde6c",
   "metadata": {},
   "source": [
    "### 4.1 Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa4acc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(\n",
    "    X_digits, y_digits, test_size=0.2, random_state=42, stratify=y_digits\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train_d)} samples\")\n",
    "print(f\"Test set: {len(X_test_d)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024d0d3b",
   "metadata": {},
   "source": [
    "### 4.2 Baseline Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5430aea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: most frequent class\n",
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train_d, y_train_d)\n",
    "print(f\"Baseline accuracy: {dummy.score(X_test_d, y_test_d):.3f}\")\n",
    "print(\"(With 10 classes, random guessing would give ~10% accuracy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b14aae",
   "metadata": {},
   "source": [
    "### 4.3 SVM with Grid Search\n",
    "\n",
    "Now let's tune hyperparameters using cross-validation, just like we did in Part 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ff1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with scaling and SVM\n",
    "pipe = Pipeline([(\"scaler\", StandardScaler()), (\"svm\", SVC(kernel=\"rbf\"))])\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\"svm__C\": [0.1, 1, 10], \"svm__gamma\": [\"scale\", 0.001, 0.01]}\n",
    "\n",
    "# Grid search (may take a minute with 10 classes)\n",
    "print(\"Running grid search (this may take a moment)...\")\n",
    "grid_search_digits = GridSearchCV(\n",
    "    pipe, param_grid, cv=5, scoring=\"accuracy\", return_train_score=True\n",
    ")\n",
    "grid_search_digits.fit(X_train_d, y_train_d)\n",
    "\n",
    "print(f\"Best parameters: {grid_search_digits.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search_digits.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0539982",
   "metadata": {},
   "source": [
    "### 4.4 Visualize Grid Search Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47217175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results and create heatmap\n",
    "results_digits = pd.DataFrame(grid_search_digits.cv_results_)\n",
    "\n",
    "# Create pivot table - need to handle 'scale' as a string\n",
    "results_digits[\"param_svm__gamma_str\"] = results_digits[\"param_svm__gamma\"].astype(str)\n",
    "scores_digits = results_digits.pivot(\n",
    "    index=\"param_svm__gamma_str\", columns=\"param_svm__C\", values=\"mean_test_score\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(\n",
    "    scores_digits,\n",
    "    annot=True,\n",
    "    fmt=\".3f\",\n",
    "    cmap=\"viridis\",\n",
    "    cbar_kws={\"label\": \"CV Accuracy\"},\n",
    ")\n",
    "plt.title(\"Grid Search Results: Digits Dataset\")\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"gamma\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a11749",
   "metadata": {},
   "source": [
    "### 4.5 Final Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf2c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "best_model = grid_search_digits.best_estimator_\n",
    "y_pred_digits = best_model.predict(X_test_d)\n",
    "\n",
    "print(f\"Test accuracy: {accuracy_score(y_test_d, y_pred_digits):.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_d, y_pred_digits))\n",
    "\n",
    "# Per-digit accuracy\n",
    "print(\"\\nPer-digit accuracy:\")\n",
    "for digit in range(10):\n",
    "    mask = y_test_d == digit\n",
    "    digit_acc = accuracy_score(y_test_d[mask], y_pred_digits[mask])\n",
    "    n_samples = mask.sum()\n",
    "    print(f\"  Digit {digit}: {digit_acc:.2%} ({n_samples} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7b66ec",
   "metadata": {},
   "source": [
    "### 4.6 Confusion Matrix and Error Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ba9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for multiclass\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_model, X_test_d, y_test_d, cmap=\"Blues\", ax=ax\n",
    ")\n",
    "ax.set_title(\"Confusion Matrix - Digits Dataset\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show some misclassified examples\n",
    "misclassified_idx = np.where(y_pred_digits != y_test_d)[0]\n",
    "if len(misclassified_idx) > 0:\n",
    "    print(f\"\\nNumber of misclassified samples: {len(misclassified_idx)}\")\n",
    "\n",
    "    # Show up to 5 misclassified examples\n",
    "    n_show = min(5, len(misclassified_idx))\n",
    "    fig, axes = plt.subplots(1, n_show, figsize=(2.5 * n_show, 3))\n",
    "    if n_show == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        idx = misclassified_idx[i]\n",
    "        img = X_test_d[idx].reshape(8, 8)\n",
    "        ax.imshow(img, cmap=\"gray\")\n",
    "        ax.set_title(f\"True: {y_test_d[idx]}\\nPred: {y_pred_digits[idx]}\", fontsize=10)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Misclassified Examples\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08794206",
   "metadata": {},
   "source": [
    "### 4.7 Reflection Questions\n",
    "\n",
    "1. Looking at the confusion matrix, which digit pairs are most often confused? Why might that be?\n",
    "2. Examine the misclassified examples — do they look ambiguous to you as well?\n",
    "3. How does the SVM's accuracy compare to the baseline? What does this tell you about the task?\n",
    "4. Which digits have the lowest per-class accuracy? Can you hypothesize why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ec9c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **Linear SVM** finds the maximum-margin hyperplane; the parameter C controls the trade-off between margin width and misclassification.\n",
    "\n",
    "2. **Kernel trick** allows SVM to handle non-linearly separable data by implicitly mapping to higher dimensions.\n",
    "\n",
    "3. **RBF kernel** creates circular/blob-like boundaries (controlled by gamma); **Polynomial kernel** creates polynomial boundaries (controlled by degree).\n",
    "\n",
    "4. **Grid Search + Cross-Validation** helps find optimal hyperparameters without overfitting to the test set.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ing3513",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
