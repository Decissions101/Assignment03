{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dfb8f0c",
   "metadata": {},
   "source": [
    "# Lab 05: Feature Spaces & Separability\n",
    "\n",
    "**ING3513 - Introduction to Artificial Intelligence and Machine Learning**\n",
    "\n",
    "In Lab 04, we learned that bad data beats good models. Now we'll explore another fundamental concept: **how do machine learning algorithms \"see\" your data?**\n",
    "\n",
    "**What you'll learn:**\n",
    "\n",
    "- Feature spaces ‚Äî the geometric world where ML algorithms operate\n",
    "- Linear separability ‚Äî when classes can be divided by a line or plane\n",
    "- Why some features help classification and others don't\n",
    "- How a simple perceptron learns by adjusting weights\n",
    "- The limits of linear models (and why we need more powerful ones)\n",
    "\n",
    "**The scenario:** A lumber mill wants to automate sorting wooden blocks into Pine (P) or Birch (B) using sensors that measure physical properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2839b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotly for interactive visualizations\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ipywidgets for interactive sliders\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# sklearn for the non-linear classifier demo\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Configure plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca61e30",
   "metadata": {},
   "source": [
    "## 1. The Wood Classification Problem\n",
    "\n",
    "A lumber mill receives mixed batches of wooden blocks ‚Äî some Pine (P), some Birch (B). Currently, workers sort them by hand, but management wants to automate this using machine learning.\n",
    "\n",
    "**What is wood grain?** When you look at a piece of wood, you see patterns of lines ‚Äî these are the _grain_, formed by the tree's annual growth rings. Different wood types have distinctive grain patterns.\n",
    "\n",
    "**The sensors measure three properties:**\n",
    "\n",
    "| Feature              | Symbol | What it measures                                   | Think of it as...                                                  |\n",
    "| -------------------- | ------ | -------------------------------------------------- | ------------------------------------------------------------------ |\n",
    "| **Brightness**       | `b`    | Average lightness of the wood surface              | How light or dark the wood looks overall (0=black, 10=white)       |\n",
    "| **Grain Prominence** | `gp`   | Contrast between light and dark bands in the grain | How much the grain pattern \"pops\" ‚Äî subtle (0) vs bold stripes (1) |\n",
    "| **Grain Frequency**  | `f`    | How closely spaced the grain lines are             | Tight/fine grain (high) vs wide/coarse grain (low)                 |\n",
    "\n",
    "**The question:** Which features should we use to build our classifier?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388e1548",
   "metadata": {},
   "source": [
    "### 1.1 Generating the Wood Block Dataset\n",
    "\n",
    "We'll create synthetic data that mimics real measurements. The key insight is:\n",
    "\n",
    "- **Grain Prominence (gp)** ‚Üí Different for Pine vs Birch (useful!)\n",
    "- **Brightness (b)** ‚Üí Similar for both wood types (useless alone)\n",
    "- **Grain Frequency (f)** ‚Üí Similar for both wood types (useless alone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabdd4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic wood block data\n",
    "np.random.seed(42)\n",
    "n_samples = 8  # Per class\n",
    "\n",
    "# PINE: Lower grain prominence\n",
    "pine_b = np.random.normal(5.5, 1.2, n_samples)  # Brightness: centered around 5.5\n",
    "pine_gp = np.random.normal(\n",
    "    0.2, 0.08, n_samples\n",
    ")  # Grain prominence: centered around 0.2 (low)\n",
    "pine_f = np.random.normal(0.5, 0.15, n_samples)  # Grain frequency: centered around 0.5\n",
    "\n",
    "# BIRCH: Higher grain prominence (same brightness and frequency as Pine!)\n",
    "birch_b = np.random.normal(5.5, 1.2, n_samples)  # Brightness: SAME as pine!\n",
    "birch_gp = np.random.normal(\n",
    "    0.6, 0.08, n_samples\n",
    ")  # Grain prominence: centered around 0.6 (high)\n",
    "birch_f = np.random.normal(0.5, 0.15, n_samples)  # Grain frequency: same as pine!\n",
    "\n",
    "# Clip values to valid ranges\n",
    "pine_b = np.clip(pine_b, 0, 10)\n",
    "pine_gp = np.clip(pine_gp, 0, 1)\n",
    "pine_f = np.clip(pine_f, 0, 1)\n",
    "birch_b = np.clip(birch_b, 0, 10)\n",
    "birch_gp = np.clip(birch_gp, 0, 1)\n",
    "birch_f = np.clip(birch_f, 0, 1)\n",
    "\n",
    "# Create DataFrame\n",
    "pine_df = pd.DataFrame(\n",
    "    {\n",
    "        \"brightness\": pine_b,\n",
    "        \"grain_prominence\": pine_gp,\n",
    "        \"grain_frequency\": pine_f,\n",
    "        \"wood_type\": \"Pine\",\n",
    "    }\n",
    ")\n",
    "\n",
    "birch_df = pd.DataFrame(\n",
    "    {\n",
    "        \"brightness\": birch_b,\n",
    "        \"grain_prominence\": birch_gp,\n",
    "        \"grain_frequency\": birch_f,\n",
    "        \"wood_type\": \"Birch\",\n",
    "    }\n",
    ")\n",
    "\n",
    "wood_data = pd.concat([pine_df, birch_df], ignore_index=True)\n",
    "\n",
    "print(\"Wood Block Dataset\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total samples: {len(wood_data)} ({n_samples} Pine, {n_samples} Birch)\")\n",
    "print(\"\\nFirst few samples:\")\n",
    "wood_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292eac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by wood type\n",
    "print(\"Summary Statistics by Wood Type\")\n",
    "print(\"=\" * 50)\n",
    "print(\n",
    "    wood_data.groupby(\"wood_type\")[\n",
    "        [\"brightness\", \"grain_prominence\", \"grain_frequency\"]\n",
    "    ]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .round(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ea743",
   "metadata": {},
   "source": [
    "## 2. Watching Clusters Form in Feature Space\n",
    "\n",
    "**What is a feature space?**\n",
    "\n",
    "When we measure properties of objects, each object becomes a _point_ in a multi-dimensional space. For 2 features, this is a 2D plane. For 3 features, it's a 3D volume.\n",
    "\n",
    "**The key insight:** Classification is about finding boundaries in this space that separate different classes.\n",
    "\n",
    "Let's watch our feature space \"fill up\" as we collect training data, one sample at a time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773bffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create animated scatter plot showing data points appearing one by one\n",
    "# Using Brightness (b) vs Grain Prominence (gp) - the GOOD features\n",
    "\n",
    "# Interleave Pine and Birch samples so both appear from the start\n",
    "pine_samples = wood_data[wood_data[\"wood_type\"] == \"Pine\"].reset_index(drop=True)\n",
    "birch_samples = wood_data[wood_data[\"wood_type\"] == \"Birch\"].reset_index(drop=True)\n",
    "wood_data_interleaved = pd.concat(\n",
    "    [\n",
    "        pine_samples.iloc[[i // 2]] if i % 2 == 0 else birch_samples.iloc[[i // 2]]\n",
    "        for i in range(len(wood_data))\n",
    "    ]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Prepare data for animation - start from frame 2 so both categories exist\n",
    "frames_data = []\n",
    "for i in range(2, len(wood_data_interleaved) + 1):  # Start at 2, not 1\n",
    "    subset = wood_data_interleaved.iloc[:i].copy()\n",
    "    subset[\"frame\"] = i\n",
    "    frames_data.append(subset)\n",
    "\n",
    "animation_df = pd.concat(frames_data, ignore_index=True)\n",
    "\n",
    "# Create animated scatter plot\n",
    "fig = px.scatter(\n",
    "    animation_df,\n",
    "    x=\"grain_prominence\",\n",
    "    y=\"brightness\",\n",
    "    color=\"wood_type\",\n",
    "    animation_frame=\"frame\",\n",
    "    range_x=[-0.1, 1.1],\n",
    "    range_y=[0, 10],\n",
    "    title=\"Training the Wood Classifier: Watch Clusters Form\",\n",
    "    labels={\n",
    "        \"grain_prominence\": \"Grain Prominence (gp)\",\n",
    "        \"brightness\": \"Brightness (b)\",\n",
    "        \"wood_type\": \"Wood Type\",\n",
    "    },\n",
    "    color_discrete_map={\"Pine\": \"#2E86AB\", \"Birch\": \"#A23B72\"},\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=15, line=dict(width=2, color=\"black\")))\n",
    "fig.update_layout(\n",
    "    width=700,\n",
    "    height=550,\n",
    "    font=dict(size=14),\n",
    "    legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99),\n",
    ")\n",
    "\n",
    "# Slow down the animation\n",
    "fig.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = 600\n",
    "fig.layout.updatemenus[0].buttons[0].args[1][\"transition\"][\"duration\"] = 200\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da9ca54",
   "metadata": {},
   "source": [
    "### 2.1 Can You Predict the Unknown Sample?\n",
    "\n",
    "After training, an unknown wood block arrives. The sensors measure:\n",
    "\n",
    "- **Brightness (b) = 7.5**\n",
    "- **Grain Prominence (gp) = 0.35**\n",
    "\n",
    "Based on where this point falls in the feature space, is it **Pine** or **Birch**?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015cb159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the final training data with the unknown sample X\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add Pine samples\n",
    "pine_data = wood_data[wood_data[\"wood_type\"] == \"Pine\"]\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=pine_data[\"grain_prominence\"],\n",
    "        y=pine_data[\"brightness\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=15, color=\"#2E86AB\", line=dict(width=2, color=\"black\")),\n",
    "        name=\"Pine\",\n",
    "        showlegend=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add Birch samples\n",
    "birch_data = wood_data[wood_data[\"wood_type\"] == \"Birch\"]\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=birch_data[\"grain_prominence\"],\n",
    "        y=birch_data[\"brightness\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=15, color=\"#A23B72\", line=dict(width=2, color=\"black\")),\n",
    "        name=\"Birch\",\n",
    "        showlegend=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add the unknown sample X\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0.35],\n",
    "        y=[7.5],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=20, color=\"gold\", line=dict(width=3, color=\"red\"), symbol=\"x\"),\n",
    "        name=\"Unknown (X)\",\n",
    "        showlegend=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Using the Wood Classifier: What is X?\",\n",
    "    xaxis_title=\"Grain Prominence (gp)\",\n",
    "    yaxis_title=\"Brightness (b)\",\n",
    "    xaxis=dict(range=[-0.1, 1.1]),\n",
    "    yaxis=dict(range=[0, 10]),\n",
    "    width=700,\n",
    "    height=550,\n",
    "    font=dict(size=14),\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\n",
    "    \"\\nü§î QUESTION: Based on its position in the feature space, is X more likely Pine or Birch?\"\n",
    ")\n",
    "print(\"   (Think about which cluster X is closer to...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97dfe9",
   "metadata": {},
   "source": [
    "### 2.2 The Decision Boundary\n",
    "\n",
    "If the two classes can be separated by a straight line, we say they are **linearly separable**.\n",
    "\n",
    "In this case, we can draw a line such that:\n",
    "\n",
    "- All **Pine** samples are on one side\n",
    "- All **Birch** samples are on the other side\n",
    "\n",
    "This line is called the **decision boundary**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9eb6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the decision boundary (a line that separates the classes)\n",
    "fig = go.Figure()\n",
    "\n",
    "# Decision boundary line: vertical line at gp = 0.4\n",
    "# Since only grain prominence distinguishes the classes, we use a vertical boundary\n",
    "gp_boundary = 0.4\n",
    "b_line = np.linspace(0, 10, 100)\n",
    "gp_line = np.full_like(b_line, gp_boundary)\n",
    "\n",
    "# Add decision boundary\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=gp_line,\n",
    "        y=b_line,\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"red\", width=3),\n",
    "        name=\"Decision Boundary: f(a) = 0\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add Pine samples\n",
    "pine_data = wood_data[wood_data[\"wood_type\"] == \"Pine\"]\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=pine_data[\"grain_prominence\"],\n",
    "        y=pine_data[\"brightness\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=15, color=\"#2E86AB\", line=dict(width=2, color=\"black\")),\n",
    "        name=\"Pine\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add Birch samples\n",
    "birch_data = wood_data[wood_data[\"wood_type\"] == \"Birch\"]\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=birch_data[\"grain_prominence\"],\n",
    "        y=birch_data[\"brightness\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=15, color=\"#A23B72\", line=dict(width=2, color=\"black\")),\n",
    "        name=\"Birch\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add annotation for the equation\n",
    "fig.add_annotation(\n",
    "    x=0.75,\n",
    "    y=8,\n",
    "    text=\"f(a) = w<sup>T</sup>a + Œ≤\",\n",
    "    showarrow=False,\n",
    "    font=dict(size=16, color=\"red\"),\n",
    "    bgcolor=\"white\",\n",
    "    bordercolor=\"red\",\n",
    "    borderwidth=2,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"A Two-Class Wood Classifier (Pine and Birch)\",\n",
    "    xaxis_title=\"Grain Prominence (gp)\",\n",
    "    yaxis_title=\"Brightness (b)\",\n",
    "    xaxis=dict(range=[-0.1, 1.1]),\n",
    "    yaxis=dict(range=[0, 10]),\n",
    "    width=750,\n",
    "    height=550,\n",
    "    font=dict(size=14),\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\n",
    "    \"‚úÖ This data is LINEARLY SEPARABLE ‚Äî a straight line can perfectly divide Pine from Birch!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924fbef0",
   "metadata": {},
   "source": [
    "## 3. The Perceptron: A Simple Learning Machine\n",
    "\n",
    "How does a machine learning algorithm find this decision boundary? Let's look at one of the simplest models: the **perceptron**.\n",
    "\n",
    "### The Perceptron Model\n",
    "\n",
    "The perceptron computes a weighted sum of the input features plus a bias:\n",
    "\n",
    "$$f(\\mathbf{w}, \\beta) = \\mathbf{w}^T \\mathbf{a} + \\beta = w_1 \\cdot a_1 + w_2 \\cdot a_2 + \\beta$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbf{a} = [a_1, a_2]^T$ = input features (grain prominence, brightness)\n",
    "- $\\mathbf{w} = [w_1, w_2]^T$ = weights (learned parameters)\n",
    "- $\\beta$ = bias/intercept (learned parameter)\n",
    "\n",
    "**Decision rule:**\n",
    "\n",
    "- If $f(\\mathbf{w}, \\beta) > 0$ ‚Üí predict **Pine**\n",
    "- If $f(\\mathbf{w}, \\beta) < 0$ ‚Üí predict **Birch**\n",
    "\n",
    "The boundary where $f = 0$ is a straight line!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58199d2",
   "metadata": {},
   "source": [
    "### 3.1 Be the Learning Algorithm!\n",
    "\n",
    "Now it's your turn. Adjust the weights ($w_1$, $w_2$) and bias ($\\beta$) to find a line that separates Pine from Birch.\n",
    "\n",
    "**Your goal:** Find values of $w_1$, $w_2$, and $\\beta$ such that:\n",
    "\n",
    "- All Pine samples are on one side of the line (positive side)\n",
    "- All Birch samples are on the other side (negative side)\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- $w_1$ and $w_2$ control the **slope** (direction) of the line\n",
    "- $\\beta$ controls the **offset** (shifts the line up/down)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817b8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive perceptron tuning with ipywidgets\n",
    "\n",
    "\n",
    "def plot_perceptron(w1, w2, beta):\n",
    "    \"\"\"Plot the decision boundary for given perceptron parameters.\"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Calculate decision boundary line\n",
    "    # w1 * gp + w2 * b + beta = 0\n",
    "    # b = (-w1 * gp - beta) / w2  (if w2 != 0)\n",
    "    gp_range = np.linspace(-0.1, 1.1, 100)\n",
    "\n",
    "    if abs(w2) > 0.01:\n",
    "        b_line = (-w1 * gp_range - beta) / w2\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=gp_range,\n",
    "                y=b_line,\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"red\", width=3),\n",
    "                name=f\"Boundary: {w1:.1f}¬∑gp + {w2:.1f}¬∑b + {beta:.1f} = 0\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Color points by prediction\n",
    "    pine_data = wood_data[wood_data[\"wood_type\"] == \"Pine\"]\n",
    "    birch_data = wood_data[wood_data[\"wood_type\"] == \"Birch\"]\n",
    "\n",
    "    # Calculate predictions\n",
    "    pine_pred = w1 * pine_data[\"grain_prominence\"] + w2 * pine_data[\"brightness\"] + beta\n",
    "    birch_pred = (\n",
    "        w1 * birch_data[\"grain_prominence\"] + w2 * birch_data[\"brightness\"] + beta\n",
    "    )\n",
    "\n",
    "    # Check if correctly classified (Pine should be positive, Birch should be negative)\n",
    "    pine_correct = (pine_pred > 0).sum()\n",
    "    birch_correct = (birch_pred < 0).sum()\n",
    "    total_correct = pine_correct + birch_correct\n",
    "\n",
    "    # Add Pine samples\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=pine_data[\"grain_prominence\"],\n",
    "            y=pine_data[\"brightness\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=15, color=\"#2E86AB\", line=dict(width=2, color=\"black\")),\n",
    "            name=\"Pine\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add Birch samples\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=birch_data[\"grain_prominence\"],\n",
    "            y=birch_data[\"brightness\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=15, color=\"#A23B72\", line=dict(width=2, color=\"black\")),\n",
    "            name=\"Birch\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Title with accuracy\n",
    "    accuracy = total_correct / len(wood_data) * 100\n",
    "    title_text = f\"Testing the Wood Classifier ‚Äî Accuracy: {total_correct}/{len(wood_data)} ({accuracy:.0f}%)\"\n",
    "    if total_correct == len(wood_data):\n",
    "        title_text += \" ‚úÖ Perfect!\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title_text,\n",
    "        xaxis_title=\"Grain Prominence (gp) = a‚ÇÅ\",\n",
    "        yaxis_title=\"Brightness (b) = a‚ÇÇ\",\n",
    "        xaxis=dict(range=[-0.1, 1.1]),\n",
    "        yaxis=dict(range=[0, 10]),\n",
    "        width=750,\n",
    "        height=500,\n",
    "        font=dict(size=14),\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Print equation\n",
    "    print(f\"\\nPerceptron equation: f(w, Œ≤) = {w1:.1f}¬∑a‚ÇÅ + {w2:.1f}¬∑a‚ÇÇ + {beta:.1f}\")\n",
    "    print(\n",
    "        f\"                   = {w1:.1f}¬∑(grain_prominence) + {w2:.1f}¬∑(brightness) + {beta:.1f}\"\n",
    "    )\n",
    "    print(\"\\nDecision: If f > 0 ‚Üí Pine, If f < 0 ‚Üí Birch\")\n",
    "    print(\n",
    "        f\"\\nCorrect: Pine {pine_correct}/{len(pine_data)}, Birch {birch_correct}/{len(birch_data)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Create interactive widgets\n",
    "w1_slider = widgets.FloatSlider(\n",
    "    value=-8.0, min=-20, max=10, step=0.5, description=\"w‚ÇÅ (gp):\"\n",
    ")\n",
    "w2_slider = widgets.FloatSlider(\n",
    "    value=1.0, min=-10, max=10, step=0.5, description=\"w‚ÇÇ (b):\"\n",
    ")\n",
    "beta_slider = widgets.FloatSlider(\n",
    "    value=0.0, min=-20, max=20, step=0.5, description=\"Œ≤ (bias):\"\n",
    ")\n",
    "\n",
    "# Create interactive output\n",
    "out = widgets.interactive_output(\n",
    "    plot_perceptron, {\"w1\": w1_slider, \"w2\": w2_slider, \"beta\": beta_slider}\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\n",
    "    \"üéÆ INTERACTIVE: Adjust the sliders to find a decision boundary that separates Pine from Birch!\"\n",
    ")\n",
    "print(\"=\" * 80)\n",
    "display(widgets.VBox([widgets.HBox([w1_slider, w2_slider, beta_slider]), out]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17971d94",
   "metadata": {},
   "source": [
    "### 3.2 What Did You Just Do?\n",
    "\n",
    "By adjusting $w_1$, $w_2$, and $\\beta$ until the line separated the classes, you did **exactly what a machine learning algorithm does** ‚Äî but manually!\n",
    "\n",
    "**Gradient descent** (the algorithm that trains neural networks) does this automatically:\n",
    "\n",
    "1. Start with random weights\n",
    "2. Check how many samples are misclassified\n",
    "3. Adjust weights slightly in the direction that reduces errors\n",
    "4. Repeat until the boundary separates the classes\n",
    "\n",
    "**Key insight:** \"Learning\" in ML is just finding the right parameters for a mathematical function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d283120f",
   "metadata": {},
   "source": [
    "## 4. The 3D Feature Space\n",
    "\n",
    "So far we've used 2 features: brightness and grain prominence. What happens when we add a third feature ‚Äî grain frequency?\n",
    "\n",
    "In 3D, the decision boundary becomes a **plane** instead of a line.\n",
    "\n",
    "$$f(\\mathbf{w}, \\beta) = w_1 \\cdot gp + w_2 \\cdot b + w_3 \\cdot f + \\beta = 0$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a963c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D scatter plot with all three features\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add Pine samples\n",
    "pine_data = wood_data[wood_data[\"wood_type\"] == \"Pine\"]\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=pine_data[\"grain_prominence\"],\n",
    "        y=pine_data[\"brightness\"],\n",
    "        z=pine_data[\"grain_frequency\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=8, color=\"#2E86AB\", line=dict(width=1, color=\"black\")),\n",
    "        name=\"Pine\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add Birch samples\n",
    "birch_data = wood_data[wood_data[\"wood_type\"] == \"Birch\"]\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=birch_data[\"grain_prominence\"],\n",
    "        y=birch_data[\"brightness\"],\n",
    "        z=birch_data[\"grain_frequency\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=8, color=\"#A23B72\", line=dict(width=1, color=\"black\")),\n",
    "        name=\"Birch\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add a separating plane\n",
    "# Vertical plane at gp = 0.4 (perpendicular to gp axis)\n",
    "# This plane extends across all brightness and frequency values\n",
    "b_plane = np.linspace(0, 10, 10)\n",
    "f_plane = np.linspace(0, 1, 10)\n",
    "b_mesh, f_mesh = np.meshgrid(b_plane, f_plane)\n",
    "# gp is constant at 0.4\n",
    "gp_mesh = np.full_like(b_mesh, 0.4)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Surface(\n",
    "        x=gp_mesh,\n",
    "        y=b_mesh,\n",
    "        z=f_mesh,\n",
    "        colorscale=[[0, \"rgba(255,0,0,0.3)\"], [1, \"rgba(255,0,0,0.3)\"]],\n",
    "        showscale=False,\n",
    "        name=\"Decision Plane\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"3D Feature Space: Brightness, Grain Prominence, Grain Frequency\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Grain Prominence (gp)\",\n",
    "        yaxis_title=\"Brightness (b)\",\n",
    "        zaxis_title=\"Grain Frequency (f)\",\n",
    "        xaxis=dict(range=[0, 1]),\n",
    "        yaxis=dict(range=[0, 10]),\n",
    "        zaxis=dict(range=[0, 1]),\n",
    "    ),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    font=dict(size=12),\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"üîÑ INTERACTIVE: Rotate the 3D plot to explore the feature space!\")\n",
    "print(\"\\nNotice the separating PLANE at gp ‚âà 0.4:\")\n",
    "print(\"   ‚Ä¢ The plane is perpendicular to the gp axis (only gp matters)\")\n",
    "print(\"   ‚Ä¢ It extends across ALL brightness and frequency values\")\n",
    "print(\"   ‚Ä¢ This proves: we need a PLANE to separate classes in 3D!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3472b43",
   "metadata": {},
   "source": [
    "### 4.1 Why We Need a Plane in 3D\n",
    "\n",
    "Look at the 3D plot above. The key observations:\n",
    "\n",
    "1. **A plane separates the classes** ‚Äî in 3D, we need a 2D surface (plane) as our decision boundary\n",
    "2. **The plane is perpendicular to the gp axis** ‚Äî because only grain prominence distinguishes the classes\n",
    "3. **Brightness and frequency don't help** ‚Äî they just add extra dimensions without improving separation\n",
    "\n",
    "**This is why feature selection matters:** Adding irrelevant features increases complexity without improving performance!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18cf7f4",
   "metadata": {},
   "source": [
    "## 5. When Linear Separation Fails\n",
    "\n",
    "What if we tried to classify using **brightness and grain frequency** (ignoring grain prominence)?\n",
    "\n",
    "Since NEITHER brightness nor frequency distinguishes the classes, this feature space is **NOT linearly separable**!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e310731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the b vs f feature space - NOT linearly separable!\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add Pine samples\n",
    "pine_data = wood_data[wood_data[\"wood_type\"] == \"Pine\"]\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=pine_data[\"grain_frequency\"],\n",
    "        y=pine_data[\"brightness\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=15, color=\"#2E86AB\", line=dict(width=2, color=\"black\")),\n",
    "        name=\"Pine\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add Birch samples\n",
    "birch_data = wood_data[wood_data[\"wood_type\"] == \"Birch\"]\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=birch_data[\"grain_frequency\"],\n",
    "        y=birch_data[\"brightness\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=15, color=\"#A23B72\", line=dict(width=2, color=\"black\")),\n",
    "        name=\"Birch\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Feature Space: Brightness vs Grain Frequency ‚Äî NOT Linearly Separable!\",\n",
    "    xaxis_title=\"Grain Frequency (f)\",\n",
    "    yaxis_title=\"Brightness (b)\",\n",
    "    xaxis=dict(range=[0, 1]),\n",
    "    yaxis=dict(range=[0, 10]),\n",
    "    width=700,\n",
    "    height=550,\n",
    "    font=dict(size=14),\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"‚ùå This data is NOT linearly separable!\")\n",
    "print(\"   Pine and Birch have the SAME brightness and frequency distributions.\")\n",
    "print(\"   No matter how you draw a straight line, you can't separate them!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a57113",
   "metadata": {},
   "source": [
    "### 5.1 Try It Yourself: Can You Find a Line?\n",
    "\n",
    "Use the sliders below to try to find a line that separates Pine from Birch using brightness and grain frequency.\n",
    "\n",
    "**Spoiler:** You won't be able to achieve 100% accuracy with a straight line!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81afac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive perceptron for the non-separable case (b vs f)\n",
    "\n",
    "\n",
    "def plot_perceptron_b_f(w1, w2, beta):\n",
    "    \"\"\"Plot the decision boundary for b vs f feature space.\"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Calculate decision boundary line\n",
    "    # w1 * f + w2 * b + beta = 0\n",
    "    # b = (-w1 * f - beta) / w2\n",
    "    f_range = np.linspace(0, 1, 100)\n",
    "\n",
    "    if abs(w2) > 0.01:\n",
    "        b_line = (-w1 * f_range - beta) / w2\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=f_range,\n",
    "                y=b_line,\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"red\", width=3),\n",
    "                name=\"Boundary\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Get data\n",
    "    pine_data = wood_data[wood_data[\"wood_type\"] == \"Pine\"]\n",
    "    birch_data = wood_data[wood_data[\"wood_type\"] == \"Birch\"]\n",
    "\n",
    "    # Calculate predictions\n",
    "    pine_pred = w1 * pine_data[\"grain_frequency\"] + w2 * pine_data[\"brightness\"] + beta\n",
    "    birch_pred = (\n",
    "        w1 * birch_data[\"grain_frequency\"] + w2 * birch_data[\"brightness\"] + beta\n",
    "    )\n",
    "\n",
    "    pine_correct = (pine_pred > 0).sum()\n",
    "    birch_correct = (birch_pred < 0).sum()\n",
    "    total_correct = pine_correct + birch_correct\n",
    "\n",
    "    # Add samples\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=pine_data[\"grain_frequency\"],\n",
    "            y=pine_data[\"brightness\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=15, color=\"#2E86AB\", line=dict(width=2, color=\"black\")),\n",
    "            name=\"Pine\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=birch_data[\"grain_frequency\"],\n",
    "            y=birch_data[\"brightness\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=15, color=\"#A23B72\", line=dict(width=2, color=\"black\")),\n",
    "            name=\"Birch\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    accuracy = total_correct / len(wood_data) * 100\n",
    "    title_text = f\"Brightness vs Grain Frequency ‚Äî Accuracy: {total_correct}/{len(wood_data)} ({accuracy:.0f}%)\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title_text,\n",
    "        xaxis_title=\"Grain Frequency (f)\",\n",
    "        yaxis_title=\"Brightness (b)\",\n",
    "        xaxis=dict(range=[0, 1]),\n",
    "        yaxis=dict(range=[0, 10]),\n",
    "        width=700,\n",
    "        height=500,\n",
    "        font=dict(size=14),\n",
    "    )\n",
    "\n",
    "    display(fig)\n",
    "\n",
    "    if total_correct == len(wood_data):\n",
    "        print(\"‚úÖ Perfect separation! (Lucky arrangement of points!)\")\n",
    "    else:\n",
    "        print(f\"‚ùå Best you can do: {total_correct}/{len(wood_data)} correct\")\n",
    "        print(\"   A straight line CANNOT perfectly separate this data!\")\n",
    "\n",
    "\n",
    "# Create widgets\n",
    "w1_slider2 = widgets.FloatSlider(\n",
    "    value=1.0, min=-10, max=10, step=0.5, description=\"w‚ÇÅ (f):\"\n",
    ")\n",
    "w2_slider2 = widgets.FloatSlider(\n",
    "    value=1.0, min=-10, max=10, step=0.5, description=\"w‚ÇÇ (b):\"\n",
    ")\n",
    "beta_slider2 = widgets.FloatSlider(\n",
    "    value=0.0, min=-10, max=10, step=0.5, description=\"Œ≤ (bias):\"\n",
    ")\n",
    "\n",
    "out2 = widgets.interactive_output(\n",
    "    plot_perceptron_b_f, {\"w1\": w1_slider2, \"w2\": w2_slider2, \"beta\": beta_slider2}\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"üéÆ TRY IT: Can you find a line that separates Pine from Birch? (Hint: You can't!)\"\n",
    ")\n",
    "print(\"=\" * 80)\n",
    "display(widgets.VBox([widgets.HBox([w1_slider2, w2_slider2, beta_slider2]), out2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43dd865",
   "metadata": {},
   "source": [
    "### 5.2 The Limits of Linear Models\n",
    "\n",
    "You've just experienced what researchers discovered in the 1960s: **simple perceptrons can't solve all problems.**\n",
    "\n",
    "In 1969, Marvin Minsky and Seymour Papert published \"Perceptrons,\" proving mathematically that single-layer perceptrons cannot solve problems where the classes aren't linearly separable.\n",
    "\n",
    "This caused the first \"AI Winter\" ‚Äî a period where funding and interest in neural networks collapsed.\n",
    "\n",
    "**The solution?** More powerful models that can learn non-linear decision boundaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41398255",
   "metadata": {},
   "source": [
    "## 6. Non-Linear Decision Boundaries\n",
    "\n",
    "What if we could use a **curved** boundary instead of a straight line?\n",
    "\n",
    "Let's create a dataset that's separable with a curve but not with a line, then show how a more powerful model (Support Vector Machine with a non-linear kernel) can handle it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad58805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset that requires a non-linear boundary\n",
    "# Pine forms a cluster in the center, Birch forms a ring around it\n",
    "np.random.seed(42)\n",
    "n_samples_nl = 12\n",
    "\n",
    "# Pine: cluster in center\n",
    "pine_gp_nl = np.random.normal(0.5, 0.1, n_samples_nl)\n",
    "pine_f_nl = np.random.normal(0.5, 0.1, n_samples_nl)\n",
    "\n",
    "# Birch: ring around the outside\n",
    "angles = np.random.uniform(0, 2 * np.pi, n_samples_nl)\n",
    "radii = np.random.uniform(0.3, 0.4, n_samples_nl)\n",
    "birch_gp_nl = 0.5 + radii * np.cos(angles)\n",
    "birch_f_nl = 0.5 + radii * np.sin(angles)\n",
    "\n",
    "# Clip to valid range\n",
    "pine_gp_nl = np.clip(pine_gp_nl, 0.05, 0.95)\n",
    "pine_f_nl = np.clip(pine_f_nl, 0.05, 0.95)\n",
    "birch_gp_nl = np.clip(birch_gp_nl, 0.05, 0.95)\n",
    "birch_f_nl = np.clip(birch_f_nl, 0.05, 0.95)\n",
    "\n",
    "# Create dataset\n",
    "nonlinear_data = pd.DataFrame(\n",
    "    {\n",
    "        \"gp\": np.concatenate([pine_gp_nl, birch_gp_nl]),\n",
    "        \"f\": np.concatenate([pine_f_nl, birch_f_nl]),\n",
    "        \"wood_type\": [\"Pine\"] * n_samples_nl + [\"Birch\"] * n_samples_nl,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "fig = go.Figure()\n",
    "\n",
    "pine_nl = nonlinear_data[nonlinear_data[\"wood_type\"] == \"Pine\"]\n",
    "birch_nl = nonlinear_data[nonlinear_data[\"wood_type\"] == \"Birch\"]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=pine_nl[\"gp\"],\n",
    "        y=pine_nl[\"f\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=15, color=\"#2E86AB\", line=dict(width=2, color=\"black\")),\n",
    "        name=\"Pine\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=birch_nl[\"gp\"],\n",
    "        y=birch_nl[\"f\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=15, color=\"#A23B72\", line=dict(width=2, color=\"black\")),\n",
    "        name=\"Birch\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Non-Linear Problem: Pine in Center, Birch Around the Edge\",\n",
    "    xaxis_title=\"Feature 1\",\n",
    "    yaxis_title=\"Feature 2\",\n",
    "    xaxis=dict(range=[0, 1]),\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    width=600,\n",
    "    height=600,\n",
    "    font=dict(size=14),\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"ü§î Can a straight line separate these classes?\")\n",
    "print(\"   No! But a CIRCLE could...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48031c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that a non-linear (RBF kernel) SVM can separate this data\n",
    "\n",
    "# Prepare data\n",
    "X_nl = nonlinear_data[[\"gp\", \"f\"]].values\n",
    "y_nl = (nonlinear_data[\"wood_type\"] == \"Pine\").astype(int)\n",
    "\n",
    "# Fit a non-linear SVM\n",
    "svm_rbf = SVC(kernel=\"rbf\", gamma=50, C=1.0)\n",
    "svm_rbf.fit(X_nl, y_nl)\n",
    "\n",
    "# Create a mesh to plot decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(0, 1, 200), np.linspace(0, 1, 200))\n",
    "Z = svm_rbf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot with decision boundary\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add decision boundary as contour\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=np.linspace(0, 1, 200),\n",
    "        y=np.linspace(0, 1, 200),\n",
    "        z=Z,\n",
    "        contours=dict(start=0, end=0, size=1, coloring=\"none\", showlines=True),\n",
    "        line=dict(color=\"red\", width=3),\n",
    "        showscale=False,\n",
    "        name=\"Non-linear Boundary\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add shaded regions\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=np.linspace(0, 1, 200),\n",
    "        y=np.linspace(0, 1, 200),\n",
    "        z=Z,\n",
    "        contours=dict(start=-10, end=0, coloring=\"fill\"),\n",
    "        colorscale=[[0, \"rgba(162, 59, 114, 0.2)\"], [1, \"rgba(162, 59, 114, 0.2)\"]],\n",
    "        showscale=False,\n",
    "        name=\"Birch Region\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=np.linspace(0, 1, 200),\n",
    "        y=np.linspace(0, 1, 200),\n",
    "        z=Z,\n",
    "        contours=dict(start=0, end=10, coloring=\"fill\"),\n",
    "        colorscale=[[0, \"rgba(46, 134, 171, 0.2)\"], [1, \"rgba(46, 134, 171, 0.2)\"]],\n",
    "        showscale=False,\n",
    "        name=\"Pine Region\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add data points\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=pine_nl[\"gp\"],\n",
    "        y=pine_nl[\"f\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=15, color=\"#2E86AB\", line=dict(width=2, color=\"black\")),\n",
    "        name=\"Pine\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=birch_nl[\"gp\"],\n",
    "        y=birch_nl[\"f\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=15, color=\"#A23B72\", line=dict(width=2, color=\"black\")),\n",
    "        name=\"Birch\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Non-Linear Boundary: SVM with RBF Kernel Can Separate This!\",\n",
    "    xaxis_title=\"Feature 1\",\n",
    "    yaxis_title=\"Feature 2\",\n",
    "    xaxis=dict(range=[0, 1]),\n",
    "    yaxis=dict(range=[0, 1]),\n",
    "    width=650,\n",
    "    height=600,\n",
    "    font=dict(size=14),\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Calculate accuracy\n",
    "y_pred = svm_rbf.predict(X_nl)\n",
    "accuracy = (y_pred == y_nl).mean() * 100\n",
    "print(f\"‚úÖ SVM with RBF kernel accuracy: {accuracy:.0f}%\")\n",
    "print(\"\\nThe curved (non-linear) boundary can separate the classes!\")\n",
    "print(\"This is the power of kernel methods ‚Äî they can learn complex patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fc0651",
   "metadata": {},
   "source": [
    "### 6.1 The Kernel Trick (Preview)\n",
    "\n",
    "How does the SVM find this curved boundary? The key idea is the **kernel trick**:\n",
    "\n",
    "1. **Transform** the data into a higher-dimensional space where it IS linearly separable\n",
    "2. **Find** a linear boundary in that high-dimensional space\n",
    "3. **Project** back to the original space ‚Äî the boundary appears curved!\n",
    "\n",
    "This is a powerful technique you'll learn more about in future courses. The key insight:\n",
    "\n",
    "> **Non-linear problems can often be solved by transforming them into linear problems in a higher-dimensional space.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88c2d1",
   "metadata": {},
   "source": [
    "## 7. Feature Space Comparison: Which Features Work?\n",
    "\n",
    "Let's compare all possible 2D feature combinations to see which ones are useful for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b306aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise scatter plot matrix\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    subplot_titles=[\n",
    "        \"Grain Prominence vs Brightness<br>(Linearly Separable ‚úÖ)\",\n",
    "        \"Brightness vs Grain Frequency<br>(NOT Separable ‚ùå)\",\n",
    "        \"Grain Prominence vs Grain Frequency<br>(Linearly Separable ‚úÖ)\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Plot 1: gp vs b (good!)\n",
    "pine_data = wood_data[wood_data[\"wood_type\"] == \"Pine\"]\n",
    "birch_data = wood_data[wood_data[\"wood_type\"] == \"Birch\"]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=pine_data[\"grain_prominence\"],\n",
    "        y=pine_data[\"brightness\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=12, color=\"#2E86AB\", line=dict(width=1, color=\"black\")),\n",
    "        name=\"Pine\",\n",
    "        legendgroup=\"pine\",\n",
    "        showlegend=True,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=birch_data[\"grain_prominence\"],\n",
    "        y=birch_data[\"brightness\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=12, color=\"#A23B72\", line=dict(width=1, color=\"black\")),\n",
    "        name=\"Birch\",\n",
    "        legendgroup=\"birch\",\n",
    "        showlegend=True,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Plot 2: b vs f (bad - NOT separable)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=pine_data[\"grain_frequency\"],\n",
    "        y=pine_data[\"brightness\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=12, color=\"#2E86AB\", line=dict(width=1, color=\"black\")),\n",
    "        name=\"Pine\",\n",
    "        legendgroup=\"pine\",\n",
    "        showlegend=False,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=birch_data[\"grain_frequency\"],\n",
    "        y=birch_data[\"brightness\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=12, color=\"#A23B72\", line=dict(width=1, color=\"black\")),\n",
    "        name=\"Birch\",\n",
    "        legendgroup=\"birch\",\n",
    "        showlegend=False,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "# Plot 3: gp vs f (bad)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=pine_data[\"grain_prominence\"],\n",
    "        y=pine_data[\"grain_frequency\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=12, color=\"#2E86AB\", line=dict(width=1, color=\"black\")),\n",
    "        name=\"Pine\",\n",
    "        legendgroup=\"pine\",\n",
    "        showlegend=False,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=3,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=birch_data[\"grain_prominence\"],\n",
    "        y=birch_data[\"grain_frequency\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=12, color=\"#A23B72\", line=dict(width=1, color=\"black\")),\n",
    "        name=\"Birch\",\n",
    "        legendgroup=\"birch\",\n",
    "        showlegend=False,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=3,\n",
    ")\n",
    "\n",
    "# Update axes\n",
    "fig.update_xaxes(title_text=\"Grain Prominence\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Brightness\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Grain Frequency\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Brightness\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Grain Prominence\", row=1, col=3)\n",
    "fig.update_yaxes(title_text=\"Grain Frequency\", row=1, col=3)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Feature Space Comparison: Which Features Separate the Classes?\",\n",
    "    width=1100,\n",
    "    height=400,\n",
    "    font=dict(size=12),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c1e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"Feature Combination Summary\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Feature Pair':<35} {'Linearly Separable?':<20} {'Why?':<25}\")\n",
    "print(\"-\" * 70)\n",
    "print(\n",
    "    f\"{'Grain Prominence + Brightness':<35} {'YES ‚úÖ':<20} {'gp separates classes':<25}\"\n",
    ")\n",
    "print(\n",
    "    f\"{'Grain Prominence + Frequency':<35} {'YES ‚úÖ':<20} {'gp separates classes':<25}\"\n",
    ")\n",
    "print(f\"{'Brightness + Frequency':<35} {'NO ‚ùå':<20} {'Neither feature helps!':<25}\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\\nüí° Key Insight: Only Grain Prominence distinguishes Pine from Birch!\")\n",
    "print(\"   Brightness and Frequency are useless ‚Äî they're the same for both classes.\")\n",
    "print(\"   But in 3D, we still need a PLANE to separate the classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9887d981",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Feature spaces are the geometric world where ML algorithms operate**\n",
    "   - Each data point becomes a location in multi-dimensional space\n",
    "   - Classification = finding boundaries (lines, planes, curves) that separate classes\n",
    "   - You literally watched clusters form as training data accumulated!\n",
    "\n",
    "2. **Feature selection is EVERYTHING**\n",
    "   - Grain prominence separates Pine from Birch ‚Üí linearly separable ‚úÖ\n",
    "   - Brightness and frequency don't help ‚Üí NOT linearly separable ‚ùå\n",
    "   - The right feature makes a simple model work; wrong features doom even sophisticated models\n",
    "\n",
    "3. **Decision boundaries scale with dimensions**\n",
    "   - In 2D: we need a **LINE** to separate classes\n",
    "   - In 3D: we need a **PLANE** to separate classes\n",
    "   - In n-dimensions: we need a **HYPERPLANE**\n",
    "   - But if the key feature is only grain prominence, even the 3D plane is perpendicular to that axis!\n",
    "\n",
    "4. **You just became a perceptron (manually!)**\n",
    "   - You adjusted sliders for $w_1$, $w_2$, and $\\beta$ to find a decision boundary\n",
    "   - That's exactly what gradient descent does automatically\n",
    "   - $f(\\mathbf{w}, \\beta) = \\mathbf{w}^T \\mathbf{a} + \\beta$ ‚Äî weights control direction, bias controls position\n",
    "   - \"Learning\" = tuning these parameters to minimize misclassification\n",
    "\n",
    "5. **Linear models hit a wall with non-separable data**\n",
    "   - You tried (and failed) to separate brightness vs frequency with a straight line\n",
    "   - This is what Minsky & Papert proved in 1969, triggering the first AI Winter\n",
    "   - Solution: non-linear models like SVMs with RBF kernels can learn curved boundaries\n",
    "\n",
    "6. **Irrelevant features are worse than useless**\n",
    "   - They increase dimensionality (computational cost)\n",
    "\n",
    "   - They add noise without signal\n",
    "\n",
    "   - They make data sparse in high dimensions (curse of dimensionality) - Always prefer fewer, meaningful features over many noisy ones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44255095",
   "metadata": {},
   "source": [
    "## 9. Discussion Questions\n",
    "\n",
    "1. **Feature Engineering in Practice:** You saw that grain prominence was the \"golden feature\" that separated Pine from Birch. If you were a data scientist at the lumber mill, what other wood properties might you measure? (Density? Weight? Color channels? Knot patterns? Surface roughness?)\n",
    "\n",
    "2. **The Perceptron's Historical Tragedy:** In 1969, Minsky & Papert's book \"Perceptrons\" proved that single-layer perceptrons couldn't solve non-linearly separable problems (like XOR). This killed neural network research for nearly 20 years. But today, deep neural networks dominate AI. What breakthrough made the difference? (Hint: think about stacking multiple layers...)\n",
    "\n",
    "3. **When Linear Isn't Enough:** In Section 6, you saw how an SVM with an RBF kernel could create a circular decision boundary. Can you think of real-world classification problems where you'd NEVER expect a straight-line boundary to work? (Medical diagnosis? Image recognition? Fraud detection?)\n",
    "\n",
    "4. **The Curse of Dimensionality:** Brightness and frequency were useless features. But what if you measured 100 useless features? Would \"more data\" help? (Consider: in 100-D space, all points become equally far apart ‚Äî nearest neighbors stop working!)\n",
    "\n",
    "5. **Hands-On Insight:** When you adjusted the sliders to tune the perceptron, did you develop an intuition for what $w_1$, $w_2$, and $\\beta$ actually DO? Could you explain to a non-technical person how a perceptron \"learns\"?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44ed88f",
   "metadata": {},
   "source": [
    "## The Bottom Line\n",
    "\n",
    "> **\"The question isn't 'which algorithm should I use?' ‚Äî it's 'which features should I measure?'\"**\n",
    "\n",
    "You just proved this yourself:\n",
    "\n",
    "- With grain prominence ‚Üí a simple perceptron works perfectly\n",
    "- Without it ‚Üí even manual tuning can't find a solution\n",
    "\n",
    "> **A perfect algorithm with the wrong features will fail. A simple algorithm with the right features will succeed.**\n",
    "\n",
    "When you rotated that 3D plot, you were seeing the world through an algorithm's eyes. Every ML model ‚Äî from perceptrons to GPT ‚Äî fundamentally operates by finding boundaries in high-dimensional feature spaces. Master this intuition, and you've mastered the core of machine learning.\n",
    "\n",
    "This is Lab 04's lesson (\"bad data beats good models\") from a geometric perspective. Feature spaces make it visual: if your classes overlap completely, no amount of algorithmic sophistication can separate them.\n",
    "\n",
    "> **Machine learning algorithms don't \"see\" raw data ‚Äî they see points in feature space. Understanding this geometry is understanding how ML works.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ing3513",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
