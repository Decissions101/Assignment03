{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca2a8bbe",
   "metadata": {},
   "source": [
    "# Lab 10: Multi-Layer Perceptrons (MLP) & Backpropagation\n",
    "\n",
    "In this lab, you'll build intuition for how neural networks learn by exploring MLPs on non-linearly separable data. You'll visualize decision boundaries, experiment with activation functions, and tune hyperparameters.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Understand the architecture of multi-layer perceptrons\n",
    "- Visualize how hidden layers enable non-linear decision boundaries\n",
    "- Compare activation functions (sigmoid, tanh, ReLU)\n",
    "- Explore hyperparameter effects (learning rate, hidden units, epochs)\n",
    "- Use train/validation/test splits for model selection\n",
    "\n",
    "### Overview\n",
    "\n",
    "| Part | Topic                                |\n",
    "| ---- | ------------------------------------ |\n",
    "| 1    | MLP Basics & Non-Linear Data         |\n",
    "| 2    | Activation Functions                 |\n",
    "| 3    | Network Architecture (Hidden Layers) |\n",
    "| 4    | Hyperparameter Tuning                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156cc448",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b952ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=UserWarning\n",
    ")  # Suppress convergence warnings for cleaner output\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3066ac2f",
   "metadata": {},
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352bb338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, X, y, ax=None, title=\"Decision Boundary\", cmap=\"RdBu\"):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of a classifier.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    clf : fitted classifier\n",
    "    X : feature array (n_samples, 2)\n",
    "    y : target array\n",
    "    ax : matplotlib axis (optional)\n",
    "    title : plot title\n",
    "    cmap : colormap for decision regions\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "\n",
    "    # Predict on mesh\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    else:\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot decision regions\n",
    "    ax.contourf(xx, yy, Z, levels=50, cmap=cmap, alpha=0.4)\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], colors=\"k\", linewidths=2)\n",
    "\n",
    "    # Plot data points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap, edgecolors=\"k\", s=50)\n",
    "\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_loss_curve(clf, ax=None, title=\"Loss Curve\"):\n",
    "    \"\"\"\n",
    "    Plot the training loss curve of an MLP classifier.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    ax.plot(clf.loss_curve_, linewidth=2)\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea41bdc8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: MLP Basics & Non-Linear Data\n",
    "\n",
    "### The Multi-Layer Perceptron\n",
    "\n",
    "An MLP consists of:\n",
    "\n",
    "- **Input layer:** Receives the features\n",
    "- **Hidden layer(s):** Transform the input through learned weights and non-linear activation functions\n",
    "- **Output layer:** Produces the prediction\n",
    "\n",
    "The key insight is that hidden layers with non-linear activations allow the network to learn **non-linear decision boundaries** — something a single perceptron (or linear classifier) cannot do.\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "The network learns by:\n",
    "\n",
    "1. **Forward pass:** Compute predictions\n",
    "2. **Compute loss:** Measure error between predictions and true labels\n",
    "3. **Backward pass:** Propagate gradients back through the network\n",
    "4. **Update weights:** Adjust weights to reduce loss\n",
    "\n",
    "This is repeated for many **epochs** (passes through the training data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9752c7",
   "metadata": {},
   "source": [
    "### 1.1 Non-Linearly Separable Data\n",
    "\n",
    "Let's create datasets that **cannot** be separated by a straight line. This is where MLPs shine compared to linear models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ead6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-linearly separable datasets\n",
    "X_moons, y_moons = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "X_circles, y_circles = make_circles(\n",
    "    n_samples=500, noise=0.2, factor=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Visualize the manifold structure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(\n",
    "    X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=\"RdBu\", edgecolors=\"k\", s=40\n",
    ")\n",
    "axes[0].set_title(\"Moons Dataset\\n(Two interleaving crescents)\", fontsize=12)\n",
    "axes[0].set_xlabel(\"Feature 1\")\n",
    "axes[0].set_ylabel(\"Feature 2\")\n",
    "\n",
    "axes[1].scatter(\n",
    "    X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap=\"RdBu\", edgecolors=\"k\", s=40\n",
    ")\n",
    "axes[1].set_title(\"Circles Dataset\\n(Concentric rings)\", fontsize=12)\n",
    "axes[1].set_xlabel(\"Feature 1\")\n",
    "axes[1].set_ylabel(\"Feature 2\")\n",
    "\n",
    "plt.suptitle(\"Non-Linearly Separable Data: The Manifold Structure\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how no straight line can separate the two classes in either dataset.\")\n",
    "print(\n",
    "    \"These datasets have complex 'manifold structure' that requires non-linear boundaries.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6889c5b",
   "metadata": {},
   "source": [
    "### 1.2 Data Preparation: Train / Validation / Test Split\n",
    "\n",
    "For proper model development, we split our data into **three** partitions:\n",
    "\n",
    "- **Training set (60%):** Used to train the model\n",
    "- **Validation set (20%):** Used to tune hyperparameters and select the best model\n",
    "- **Test set (20%):** Used only once at the end to estimate real-world performance\n",
    "\n",
    "This prevents us from overfitting to the test set during hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3447007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the moons dataset for most of this lab\n",
    "X = X_moons\n",
    "y = y_moons\n",
    "\n",
    "# First split: separate out test set (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: separate train and validation from the remaining 80%\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=y_temp,  # 0.25 of 80% = 20% overall\n",
    ")\n",
    "\n",
    "print(f\"Training set:   {len(X_train)} samples ({len(X_train) / len(X) * 100:.0f}%)\")\n",
    "print(f\"Validation set: {len(X_val)} samples ({len(X_val) / len(X) * 100:.0f}%)\")\n",
    "print(f\"Test set:       {len(X_test)} samples ({len(X_test) / len(X) * 100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963a2e4b",
   "metadata": {},
   "source": [
    "### 1.3 Scale the Features\n",
    "\n",
    "Neural networks are sensitive to feature scales. We standardize so that each feature has mean ≈ 0 and std ≈ 1.\n",
    "\n",
    "**Important:** We fit the scaler on the training data only, then transform all sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04059dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit on training data only\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\n",
    "    f\"Training data - mean: {X_train_scaled.mean(axis=0).round(3)}, std: {X_train_scaled.std(axis=0).round(3)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b99338",
   "metadata": {},
   "source": [
    "### 1.4 Train Your First MLP\n",
    "\n",
    "Let's train a simple MLP with one hidden layer of 20 neurons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10361cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train an MLP\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(20,),  # One hidden layer with 20 neurons\n",
    "    activation=\"relu\",  # ReLU activation function\n",
    "    solver=\"adam\",  # Adam optimizer\n",
    "    max_iter=1000,  # Maximum epochs\n",
    "    random_state=0,  # Different seed for better demo\n",
    ")\n",
    "\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = mlp.score(X_train_scaled, y_train)\n",
    "val_acc = mlp.score(X_val_scaled, y_val)\n",
    "\n",
    "print(f\"Training accuracy:   {train_acc:.3f}\")\n",
    "print(f\"Validation accuracy: {val_acc:.3f}\")\n",
    "print(f\"\\nNumber of iterations (epochs): {mlp.n_iter_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c44a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision boundary on both training and validation data\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Decision boundary - Training data (what the model learned from)\n",
    "plot_decision_boundary(\n",
    "    mlp,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    ax=axes[0],\n",
    "    title=f\"Training Data\\nAccuracy: {train_acc:.3f}\",\n",
    ")\n",
    "\n",
    "# Decision boundary - Validation data (unseen during training)\n",
    "plot_decision_boundary(\n",
    "    mlp,\n",
    "    X_val_scaled,\n",
    "    y_val,\n",
    "    ax=axes[1],\n",
    "    title=f\"Validation Data\\nAccuracy: {val_acc:.3f}\",\n",
    ")\n",
    "\n",
    "# Loss curve\n",
    "plot_loss_curve(mlp, ax=axes[2], title=\"Training Loss Over Epochs\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observation: The MLP has learned a NON-LINEAR decision boundary!\")\n",
    "print(\n",
    "    \"Compare: Training data (left) vs Validation data (middle) - same boundary, different points.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d0445",
   "metadata": {},
   "source": [
    "### 1.5 Discussion\n",
    "\n",
    "**Observe:**\n",
    "\n",
    "1. The MLP has learned a **non-linear decision boundary** that curves around the data\n",
    "2. The loss decreases over training iterations (epochs)\n",
    "3. Compare train vs. validation accuracy — if train >> validation, the model may be overfitting\n",
    "\n",
    "**Questions to consider:**\n",
    "\n",
    "- What would happen with more hidden neurons?\n",
    "- What if we trained for more epochs?\n",
    "- What if we used a different activation function?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96103a82",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Activation Functions\n",
    "\n",
    "The **activation function** introduces non-linearity into the network. Without it, stacking layers would be equivalent to a single linear transformation.\n",
    "\n",
    "### Common Activation Functions\n",
    "\n",
    "| Function    | Formula                                        | Range   | Notes                                               |\n",
    "| ----------- | ---------------------------------------------- | ------- | --------------------------------------------------- |\n",
    "| **Sigmoid** | $\\sigma(x) = \\frac{1}{1 + e^{-x}}$             | (0, 1)  | Historical; suffers from vanishing gradients        |\n",
    "| **Tanh**    | $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | (-1, 1) | Zero-centered; still has vanishing gradient issue   |\n",
    "| **ReLU**    | $\\text{ReLU}(x) = \\max(0, x)$                  | [0, ∞)  | Most common today; fast, avoids vanishing gradients |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1489d2a5",
   "metadata": {},
   "source": [
    "### 2.1 Visualize Activation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5fdea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "tanh = np.tanh(x)\n",
    "relu = np.maximum(0, x)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(x, sigmoid, \"b-\", linewidth=2)\n",
    "axes[0].axhline(y=0, color=\"k\", linewidth=0.5)\n",
    "axes[0].axvline(x=0, color=\"k\", linewidth=0.5)\n",
    "axes[0].set_title(\"Sigmoid\\n$\\\\sigma(x) = 1/(1+e^{-x})$\", fontsize=12)\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"σ(x)\")\n",
    "axes[0].set_ylim(-0.2, 1.2)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(x, tanh, \"g-\", linewidth=2)\n",
    "axes[1].axhline(y=0, color=\"k\", linewidth=0.5)\n",
    "axes[1].axvline(x=0, color=\"k\", linewidth=0.5)\n",
    "axes[1].set_title(\"Tanh\\n$\\\\tanh(x) = (e^x - e^{-x})/(e^x + e^{-x})$\", fontsize=12)\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[1].set_ylabel(\"tanh(x)\")\n",
    "axes[1].set_ylim(-1.2, 1.2)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(x, relu, \"r-\", linewidth=2)\n",
    "axes[2].axhline(y=0, color=\"k\", linewidth=0.5)\n",
    "axes[2].axvline(x=0, color=\"k\", linewidth=0.5)\n",
    "axes[2].set_title(\"ReLU\\n$\\\\text{ReLU}(x) = \\\\max(0, x)$\", fontsize=12)\n",
    "axes[2].set_xlabel(\"x\")\n",
    "axes[2].set_ylabel(\"ReLU(x)\")\n",
    "axes[2].set_ylim(-0.5, 5.5)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Activation Functions\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9815f868",
   "metadata": {},
   "source": [
    "### 2.2 Gradients and the Vanishing Gradient Problem\n",
    "\n",
    "During backpropagation, gradients are multiplied through each layer. The **derivative** of the activation function determines how gradients flow backward.\n",
    "\n",
    "Look at the derivatives:\n",
    "\n",
    "- **Sigmoid:** $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$ → max value is 0.25 at x=0\n",
    "- **Tanh:** $\\tanh'(x) = 1 - \\tanh^2(x)$ → max value is 1 at x=0\n",
    "- **ReLU:** $\\text{ReLU}'(x) = 1$ if $x > 0$, else $0$ → gradient is 1 or 0\n",
    "\n",
    "**The problem:** When inputs are far from zero, sigmoid and tanh derivatives become very small (near 0). Multiplying many small numbers through layers causes gradients to **vanish**, making early layers learn very slowly.\n",
    "\n",
    "**Why ReLU wins:** The gradient is either 0 or 1 — no shrinking. This allows gradients to flow through deep networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61670bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute derivatives of activation functions\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "# Activation functions\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "tanh_vals = np.tanh(x)\n",
    "\n",
    "# Derivatives\n",
    "sigmoid_deriv = sigmoid * (1 - sigmoid)\n",
    "tanh_deriv = 1 - tanh_vals**2\n",
    "relu_deriv = (x > 0).astype(float)\n",
    "\n",
    "# Plot derivatives\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(x, sigmoid_deriv, \"b-\", linewidth=2)\n",
    "axes[0].axhline(y=0.25, color=\"r\", linestyle=\"--\", alpha=0.7, label=\"max = 0.25\")\n",
    "axes[0].axhline(y=0, color=\"k\", linewidth=0.5)\n",
    "axes[0].axvline(x=0, color=\"k\", linewidth=0.5)\n",
    "axes[0].fill_between(x, sigmoid_deriv, alpha=0.3)\n",
    "axes[0].set_title(\n",
    "    \"Sigmoid Derivative\\n$\\\\sigma'(x) = \\\\sigma(x)(1-\\\\sigma(x))$\", fontsize=12\n",
    ")\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"σ'(x)\")\n",
    "axes[0].set_ylim(-0.05, 0.35)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(x, tanh_deriv, \"g-\", linewidth=2)\n",
    "axes[1].axhline(y=1, color=\"r\", linestyle=\"--\", alpha=0.7, label=\"max = 1\")\n",
    "axes[1].axhline(y=0, color=\"k\", linewidth=0.5)\n",
    "axes[1].axvline(x=0, color=\"k\", linewidth=0.5)\n",
    "axes[1].fill_between(x, tanh_deriv, alpha=0.3, color=\"green\")\n",
    "axes[1].set_title(\"Tanh Derivative\\n$\\\\tanh'(x) = 1 - \\\\tanh^2(x)$\", fontsize=12)\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[1].set_ylabel(\"tanh'(x)\")\n",
    "axes[1].set_ylim(-0.1, 1.2)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(x, relu_deriv, \"r-\", linewidth=2)\n",
    "axes[2].axhline(y=1, color=\"r\", linestyle=\"--\", alpha=0.7, label=\"gradient = 1\")\n",
    "axes[2].axhline(y=0, color=\"k\", linewidth=0.5)\n",
    "axes[2].axvline(x=0, color=\"k\", linewidth=0.5)\n",
    "axes[2].fill_between(x, relu_deriv, alpha=0.3, color=\"red\")\n",
    "axes[2].set_title(\"ReLU Derivative\\n$\\\\text{ReLU}'(x) = 1$ if $x>0$\", fontsize=12)\n",
    "axes[2].set_xlabel(\"x\")\n",
    "axes[2].set_ylabel(\"ReLU'(x)\")\n",
    "axes[2].set_ylim(-0.1, 1.3)\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Activation Function Derivatives (Gradients)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: For |x| > 2, sigmoid and tanh derivatives are nearly 0.\")\n",
    "print(\"In deep networks, multiplying many small gradients → vanishing gradients.\")\n",
    "print(\"ReLU maintains gradient = 1 for positive inputs, enabling deeper networks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53af328",
   "metadata": {},
   "source": [
    "### 2.3 Compare Activation Functions on the Moons Dataset\n",
    "\n",
    "Let's train MLPs with different activation functions and compare their decision boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d36dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [\"logistic\", \"tanh\", \"relu\"]  # 'logistic' is sklearn's name for sigmoid\n",
    "activation_names = [\"Sigmoid\", \"Tanh\", \"ReLU\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for ax, activation, name in zip(axes, activations, activation_names):\n",
    "    # Train MLP with this activation\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(12,),\n",
    "        activation=activation,\n",
    "        solver=\"adam\",\n",
    "        max_iter=500,\n",
    "        random_state=42,\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get accuracies\n",
    "    train_acc = mlp.score(X_train_scaled, y_train)\n",
    "    val_acc = mlp.score(X_val_scaled, y_val)\n",
    "\n",
    "    # Plot decision boundary\n",
    "    plot_decision_boundary(\n",
    "        mlp,\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        ax=ax,\n",
    "        title=f\"{name}\\nTrain: {train_acc:.3f}, Val: {val_acc:.3f}\",\n",
    "    )\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Decision Boundaries with Different Activation Functions\", fontsize=14, y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6d5b11",
   "metadata": {},
   "source": [
    "### 2.4 Compare Loss Curves\n",
    "\n",
    "Different activations can lead to different training dynamics. Let's compare how quickly each converges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c01a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = [\"blue\", \"green\", \"red\"]\n",
    "\n",
    "for activation, name, color in zip(activations, activation_names, colors):\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(10,),\n",
    "        activation=activation,\n",
    "        solver=\"adam\",\n",
    "        max_iter=1000,\n",
    "        n_iter_no_change=1000,  # Disable early stopping to train full 2000 epochs\n",
    "        random_state=42,\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "    ax.plot(mlp.loss_curve_, label=name, color=color, linewidth=2)\n",
    "\n",
    "ax.set_xlabel(\"Iteration (Epoch)\", fontsize=12)\n",
    "ax.set_ylabel(\"Loss\", fontsize=12)\n",
    "ax.set_title(\"Training Loss Curves by Activation Function\", fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\n",
    "    \"• ReLU (red) achieves the lowest final loss — gradients flow well, enabling continued learning\"\n",
    ")\n",
    "print(\n",
    "    \"• Sigmoid (blue) plateaus at a higher loss — this is the vanishing gradient problem!\"\n",
    ")\n",
    "print(\n",
    "    \"• Tanh (green) is in between — better than sigmoid but still saturates for large inputs\"\n",
    ")\n",
    "print(\n",
    "    \"\\nThis demonstrates WHY ReLU became the default activation for modern neural networks.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce89c8e",
   "metadata": {},
   "source": [
    "### 2.5 Your Turn: Test on Circles Dataset\n",
    "\n",
    "**Task:** Train MLPs with different activation functions on the circles dataset. Which activation works best?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86201de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the circles dataset (same way we split moons)\n",
    "X_temp_c, X_test_c, y_temp_c, y_test_c = train_test_split(\n",
    "    X_circles, y_circles, test_size=0.2, random_state=42, stratify=y_circles\n",
    ")\n",
    "X_train_c, X_val_c, y_train_c, y_val_c = train_test_split(\n",
    "    X_temp_c, y_temp_c, test_size=0.25, random_state=42, stratify=y_temp_c\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler_c = StandardScaler()\n",
    "X_train_c_scaled = scaler_c.fit_transform(X_train_c)\n",
    "X_val_c_scaled = scaler_c.transform(X_val_c)\n",
    "\n",
    "print(\n",
    "    f\"Training: {len(X_train_c)} | Validation: {len(X_val_c)} | Test: {len(X_test_c)}\\n\"\n",
    ")\n",
    "\n",
    "# TODO: Train MLPs with each activation function and compare\n",
    "# Use the lists below to loop through activations\n",
    "activations = [\"logistic\", \"tanh\", \"relu\"]\n",
    "activation_names = [\"Sigmoid\", \"Tanh\", \"ReLU\"]\n",
    "\n",
    "# TODO: Create a figure with 3 subplots (like section 2.3)\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# TODO: Loop through activations, train an MLP, and plot decision boundaries\n",
    "# Hint: Use plot_decision_boundary(mlp, X_train_c_scaled, y_train_c, ax=ax, title=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f08c1fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Network Architecture (Hidden Layers)\n",
    "\n",
    "The **hidden layer sizes** determine the network's capacity to learn complex patterns:\n",
    "\n",
    "- **More neurons (width):** Can learn more complex boundaries, but risk overfitting\n",
    "- **More layers (depth):** Can learn hierarchical representations, but harder to train\n",
    "\n",
    "In sklearn's `MLPClassifier`, `hidden_layer_sizes=(50, 30)` means:\n",
    "\n",
    "- First hidden layer: 50 neurons\n",
    "- Second hidden layer: 30 neurons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37ff609",
   "metadata": {},
   "source": [
    "### 3.1 Effect of Number of Hidden Neurons\n",
    "\n",
    "Let's see how the decision boundary changes with different numbers of neurons in a single hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367574a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [1, 2, 5, 10, 50, 100]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, n_hidden in zip(axes, hidden_sizes):\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(n_hidden,),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        max_iter=500,\n",
    "        random_state=42,\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "    train_acc = mlp.score(X_train_scaled, y_train)\n",
    "    val_acc = mlp.score(X_val_scaled, y_val)\n",
    "\n",
    "    plot_decision_boundary(\n",
    "        mlp,\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        ax=ax,\n",
    "        title=f\"{n_hidden} Hidden Neurons\\nTrain: {train_acc:.3f}, Val: {val_acc:.3f}\",\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"Effect of Number of Hidden Neurons (Single Layer)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9097b5f3",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "1. With very few neurons (1-2), the network cannot capture the non-linear structure\n",
    "2. With more neurons (5-10), the boundary becomes flexible enough\n",
    "3. With many neurons (50-100), the boundary may become too flexible (overfitting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e681729",
   "metadata": {},
   "source": [
    "### 3.2 Effect of Number of Hidden Layers (Depth)\n",
    "\n",
    "A network with **2 or more hidden layers** is often called a **deep neural network** — this is where \"deep learning\" gets its name!\n",
    "\n",
    "**Why does depth matter?**\n",
    "\n",
    "- Each layer can learn increasingly **abstract representations** of the input\n",
    "- Deeper networks can compose simpler features into complex patterns\n",
    "- In practice, deep networks excel at hierarchical data (images, text, audio)\n",
    "\n",
    "However, for simple 2D datasets like ours, depth may not provide much benefit. Let's experiment by comparing architectures with **similar parameter counts** (~150 parameters each) but different depths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9350f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different architectures with SIMILAR parameter counts (~150 params each)\n",
    "# Formula: params = sum of (inputs × outputs + biases) for each layer connection\n",
    "architectures = [\n",
    "    (37,),  # 1 layer: (2×37+37) + (37×1+1) = 149 params\n",
    "    (10, 10),  # 2 layers: (2×10+10) + (10×10+10) + (10×1+1) = 151 params\n",
    "    (7, 7, 7),  # 3 layers: (2×7+7) + 2×(7×7+7) + (7×1+1) = 141 params\n",
    "    (6, 6, 6, 6),  # 4 layers: (2×6+6) + 3×(6×6+6) + (6×1+1) = 151 params\n",
    "]\n",
    "\n",
    "\n",
    "def count_params(input_dim, hidden_sizes, output_dim=1):\n",
    "    \"\"\"Count total parameters in an MLP.\"\"\"\n",
    "    layers = [input_dim] + list(hidden_sizes) + [output_dim]\n",
    "    total = 0\n",
    "    for i in range(len(layers) - 1):\n",
    "        total += layers[i] * layers[i + 1] + layers[i + 1]  # weights + biases\n",
    "    return total\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "for ax, arch in zip(axes, architectures):\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=arch,\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        max_iter=500,\n",
    "        random_state=42,\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "    train_acc = mlp.score(X_train_scaled, y_train)\n",
    "    val_acc = mlp.score(X_val_scaled, y_val)\n",
    "\n",
    "    # Count parameters\n",
    "    n_params = count_params(2, arch, 1)\n",
    "    arch_str = \" → \".join(map(str, arch))\n",
    "    n_layers = len(arch)\n",
    "\n",
    "    plot_decision_boundary(\n",
    "        mlp,\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        ax=ax,\n",
    "        title=f\"{n_layers} Layer(s): {arch_str}\\n{n_params} params | Train: {train_acc:.3f}, Val: {val_acc:.3f}\",\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"Effect of Network Depth (Similar Parameter Count)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    \"All architectures have ~150 parameters, so differences are due to DEPTH, not capacity.\"\n",
    ")\n",
    "print(\"Observation: For this simple 2D problem, shallow networks work just as well.\")\n",
    "print(\"Deeper networks shine on more complex, hierarchical data (e.g., images).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c22301",
   "metadata": {},
   "source": [
    "### 3.3 Visualize Network Architecture\n",
    "\n",
    "Let's visualize what a simple MLP architecture looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_neural_network(ax, layer_sizes, title=\"Neural Network Architecture\"):\n",
    "    \"\"\"\n",
    "    Draw a simple neural network diagram.\n",
    "    \"\"\"\n",
    "    n_layers = len(layer_sizes)\n",
    "    max_neurons = max(layer_sizes)\n",
    "\n",
    "    # Positions\n",
    "    layer_positions = np.linspace(0, 1, n_layers)\n",
    "\n",
    "    for i, (n_neurons, x_pos) in enumerate(zip(layer_sizes, layer_positions)):\n",
    "        # Vertical positions for neurons in this layer\n",
    "        y_positions = np.linspace(0.2, 0.8, n_neurons) if n_neurons > 1 else [0.5]\n",
    "\n",
    "        for y_pos in y_positions:\n",
    "            # Draw neuron\n",
    "            circle = plt.Circle(\n",
    "                (x_pos, y_pos), 0.03, color=\"steelblue\", ec=\"black\", zorder=4\n",
    "            )\n",
    "            ax.add_patch(circle)\n",
    "\n",
    "            # Draw connections to next layer\n",
    "            if i < n_layers - 1:\n",
    "                next_n_neurons = layer_sizes[i + 1]\n",
    "                next_y_positions = (\n",
    "                    np.linspace(0.2, 0.8, next_n_neurons)\n",
    "                    if next_n_neurons > 1\n",
    "                    else [0.5]\n",
    "                )\n",
    "                next_x = layer_positions[i + 1]\n",
    "\n",
    "                for next_y in next_y_positions:\n",
    "                    ax.plot(\n",
    "                        [x_pos + 0.03, next_x - 0.03],\n",
    "                        [y_pos, next_y],\n",
    "                        \"gray\",\n",
    "                        alpha=0.3,\n",
    "                        linewidth=0.5,\n",
    "                        zorder=1,\n",
    "                    )\n",
    "\n",
    "    # Labels\n",
    "    labels = [\"Input\"] + [f\"Hidden {i + 1}\" for i in range(n_layers - 2)] + [\"Output\"]\n",
    "    for x_pos, label, n in zip(layer_positions, labels, layer_sizes):\n",
    "        ax.text(x_pos, 0.05, f\"{label}\\n({n})\", ha=\"center\", fontsize=10)\n",
    "\n",
    "    ax.set_xlim(-0.1, 1.1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(title, fontsize=12)\n",
    "\n",
    "\n",
    "# Draw example architectures\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "draw_neural_network(axes[0], [2, 4, 1], \"Simple: 2 → 4 → 1\")\n",
    "draw_neural_network(axes[1], [2, 6, 4, 1], \"Medium: 2 → 6 → 4 → 1\")\n",
    "draw_neural_network(axes[2], [2, 8, 6, 4, 1], \"Deep: 2 → 8 → 6 → 4 → 1\")\n",
    "\n",
    "plt.suptitle(\"MLP Architectures\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa3f9bb",
   "metadata": {},
   "source": [
    "### 3.4 Your Turn: Find a Good Architecture\n",
    "\n",
    "**Task:** Experiment with different architectures. Try to find one that achieves high validation accuracy without overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59410e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try different hidden_layer_sizes\n",
    "# Examples: (5,), (10, 5), (20, 10, 5), etc.\n",
    "\n",
    "# TODO: Compare training and validation accuracy\n",
    "# A big gap suggests overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e082d6b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Hyperparameter Tuning\n",
    "\n",
    "Key hyperparameters for MLPs:\n",
    "\n",
    "- **Learning rate (`learning_rate_init`):** Step size for weight updates\n",
    "- **Hidden layer sizes:** Network architecture\n",
    "- **Number of epochs (`max_iter`):** How long to train\n",
    "- **Regularization (`alpha`):** L2 penalty to prevent overfitting\n",
    "\n",
    "### The Learning Rate\n",
    "\n",
    "- **Too high:** Training is unstable, loss oscillates or diverges\n",
    "- **Too low:** Training is slow, may get stuck in local minima\n",
    "- **Just right:** Smooth convergence to a good solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139cf03c",
   "metadata": {},
   "source": [
    "### 4.1 Effect of Learning Rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca7602",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot loss curves\n",
    "for lr in learning_rates:\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(10,),\n",
    "        activation=\"relu\",\n",
    "        solver=\"sgd\",  # Use SGD to see learning rate effects more clearly\n",
    "        learning_rate_init=lr,\n",
    "        max_iter=400,\n",
    "        n_iter_no_change=400,  # Disable early stopping to train full 2000 epochs\n",
    "        random_state=42,\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "    axes[0].plot(mlp.loss_curve_, label=f\"lr={lr}\", linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Loss Curves for Different Learning Rates\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot validation accuracy vs learning rate\n",
    "lr_range = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 1]\n",
    "val_accs = []\n",
    "\n",
    "for lr in lr_range:\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(10,),\n",
    "        activation=\"relu\",\n",
    "        solver=\"sgd\",\n",
    "        learning_rate_init=lr,\n",
    "        max_iter=400,\n",
    "        random_state=42,\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "    val_accs.append(mlp.score(X_val_scaled, y_val))\n",
    "\n",
    "axes[1].semilogx(lr_range, val_accs, \"bo-\", linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel(\"Learning Rate (log scale)\")\n",
    "axes[1].set_ylabel(\"Validation Accuracy\")\n",
    "axes[1].set_title(\"Validation Accuracy vs Learning Rate\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"• Too LOW (0.0001): Loss barely decreases — learning is too slow\")\n",
    "print(\"• Too HIGH (1.0): Loss oscillates wildly — updates overshoot the minimum\")\n",
    "print(\"• JUST RIGHT (0.01-0.1): Smooth, fast convergence to low loss\")\n",
    "print(\n",
    "    \"\\nThe right plot shows accuracy drops at very high learning rates — the model becomes unstable.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081930b6",
   "metadata": {},
   "source": [
    "### 4.2 Effect of Number of Epochs (Overfitting)\n",
    "\n",
    "Training for too many epochs can lead to **overfitting** — the model memorizes the training data instead of learning general patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More granular view of training vs validation accuracy\n",
    "max_iters = list(range(10, 1000, 20))  # Every 20 epochs from 10 to 1000\n",
    "\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "for max_iter in max_iters:\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(1000,),  # More capacity to overfit\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        max_iter=max_iter,\n",
    "        random_state=0,\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "    train_accs.append(mlp.score(X_train_scaled, y_train))\n",
    "    val_accs.append(mlp.score(X_val_scaled, y_val))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(max_iters, train_accs, \"b-\", label=\"Training\", linewidth=2)\n",
    "plt.plot(max_iters, val_accs, \"r-\", label=\"Validation\", linewidth=2)\n",
    "plt.xlabel(\"Number of Epochs (max_iter)\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "plt.title(\"Training vs Validation Accuracy by Number of Epochs\", fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observe: Training accuracy keeps improving with more epochs.\")\n",
    "print(\"Validation accuracy may plateau or decrease — a sign of OVERFITTING.\")\n",
    "print(\n",
    "    \"The gap between train and val accuracy indicates how much the model has overfit.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e210714a",
   "metadata": {},
   "source": [
    "### 4.3 Grid Search for Best Hyperparameters\n",
    "\n",
    "Manually tuning hyperparameters is tedious. **Grid search** automates this by trying all combinations and using cross-validation to find the best one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db708f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    \"hidden_layer_sizes\": [(5,), (10,), (20,), (50,), (10, 5), (20, 10)],\n",
    "    \"learning_rate_init\": [0.001, 0.01, 0.1],  # Include 0.1 based on section 4.1\n",
    "    \"alpha\": [0.0001, 0.001, 0.01],  # L2 regularization\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "mlp_base = MLPClassifier(\n",
    "    activation=\"relu\", solver=\"adam\", max_iter=500, random_state=42\n",
    ")\n",
    "\n",
    "# Grid search with cross-validation\n",
    "print(\"Running grid search (this may take a moment)...\")\n",
    "grid_search = GridSearchCV(\n",
    "    mlp_base,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Fit on combined train+val for CV (we kept test separate)\n",
    "X_trainval_scaled = np.vstack([X_train_scaled, X_val_scaled])\n",
    "y_trainval = np.hstack([y_train, y_val])\n",
    "\n",
    "grid_search.fit(X_trainval_scaled, y_trainval)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV accuracy: {grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a09ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 10 parameter combinations\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "results_df = results_df.sort_values(\"mean_test_score\", ascending=False)\n",
    "\n",
    "print(\"Top 10 Hyperparameter Combinations:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "top_10 = results_df[[\"params\", \"mean_test_score\", \"std_test_score\"]].head(10)\n",
    "for i, row in top_10.iterrows():\n",
    "    print(\n",
    "        f\"CV Acc: {row['mean_test_score']:.3f} (±{row['std_test_score']:.3f})  |  {row['params']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aca84e",
   "metadata": {},
   "source": [
    "### 4.4 Final Evaluation on Test Set\n",
    "\n",
    "Now we use the test set (which we've kept completely separate) to estimate real-world performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af8fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model and evaluate on test set\n",
    "best_mlp = grid_search.best_estimator_\n",
    "\n",
    "y_pred_test = best_mlp.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=[\"Class 0\", \"Class 1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final model\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Decision boundary on test data\n",
    "plot_decision_boundary(\n",
    "    best_mlp,\n",
    "    X_test_scaled,\n",
    "    y_test,\n",
    "    ax=axes[0],\n",
    "    title=f\"Best MLP on Test Data\\nAccuracy: {test_accuracy:.3f}\",\n",
    ")\n",
    "\n",
    "# Confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_mlp,\n",
    "    X_test_scaled,\n",
    "    y_test,\n",
    "    display_labels=[\"Class 0\", \"Class 1\"],\n",
    "    cmap=\"Blues\",\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set_title(\"Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f1b90b",
   "metadata": {},
   "source": [
    "### 4.5 Your Turn: Full Pipeline on Circles Dataset\n",
    "\n",
    "**Task:** Apply everything you've learned to the circles dataset:\n",
    "\n",
    "1. Split into train/val/test\n",
    "2. Scale the features\n",
    "3. Use grid search to find the best hyperparameters\n",
    "4. Evaluate on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f85c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete pipeline for circles dataset\n",
    "\n",
    "# 1. Split data\n",
    "\n",
    "# 2. Scale features\n",
    "\n",
    "# 3. Grid search\n",
    "\n",
    "# 4. Evaluate on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ff54a",
   "metadata": {},
   "source": [
    "### 4.6 Reflection Questions\n",
    "\n",
    "1. How does the MLP's performance compare to the SVM from Lab 09 on similar datasets?\n",
    "2. What was the effect of the learning rate on training stability?\n",
    "3. Did you observe overfitting? How could you tell?\n",
    "4. Which hyperparameter had the biggest impact on performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ab362",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **MLPs with hidden layers** can learn non-linear decision boundaries that linear classifiers cannot.\n",
    "\n",
    "2. **Activation functions** (sigmoid, tanh, ReLU) introduce non-linearity. ReLU is most common today because it avoids the vanishing gradient problem.\n",
    "\n",
    "3. **Network architecture** (width and depth) controls model capacity — more neurons/layers can model more complex patterns but risk overfitting.\n",
    "\n",
    "4. **Hyperparameters** like learning rate, number of epochs, and regularization (alpha) significantly affect training dynamics and final performance.\n",
    "\n",
    "5. **Train/validation/test splits** are essential for proper model selection and unbiased performance estimation.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Always evaluate final performance on a held-out test set\n",
    "\n",
    "- Start simple: A single hidden layer with 10-50 neurons often works well- Grid search with cross-validation automates hyperparameter tuning\n",
    "\n",
    "- ReLU activation is a safe default choice- Use validation accuracy to detect overfitting (train >> val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ing3513",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
