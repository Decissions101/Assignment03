{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc5b83a",
   "metadata": {},
   "source": [
    "# Lab 04: Bad Data Beats Good Models\n",
    "\n",
    "**ING3513 - Introduction to Artificial Intelligence and Machine Learning**\n",
    "\n",
    "In Lab 03, we learned about overfitting â€” when models are too complex for the data. But there's an even more fundamental problem: **what if the data itself is flawed?**\n",
    "\n",
    "**What you'll learn:**\n",
    "\n",
    "- How sampling bias can completely mislead your analysis\n",
    "- Why \"best fit\" doesn't mean \"correct model\"\n",
    "- The critical importance of data collection strategy\n",
    "- How to design better data collection plans\n",
    "\n",
    "**The harsh truth:** No algorithm can save you from bad data. Garbage in, garbage out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d7bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Import helper functions\n",
    "from lab04_utils import true_function\n",
    "\n",
    "# Configure plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4edc31",
   "metadata": {},
   "source": [
    "## 1. The Scenario\n",
    "\n",
    "You've been hired as a data scientist at a research lab. Your colleague collected some measurements and needs your help finding the underlying relationship.\n",
    "\n",
    "**Your task:** Analyze the data, find the best-fit model, and report your findings.\n",
    "\n",
    "Let's start with **Dataset A**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286bb8ad",
   "metadata": {},
   "source": [
    "## 2. Dataset A â€” Your First Analysis\n",
    "\n",
    "Here's the data your colleague collected. Analyze it and find the best polynomial fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cbec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset A: Collected from a \"linear-looking\" region\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample from x = -6.5 to -5.5 (narrow band on the steep decline - looks linear)\n",
    "X_A = np.linspace(-6.5, -5.5, 15).reshape(-1, 1)\n",
    "noise_A = np.random.normal(0, 10, size=X_A.shape[0])\n",
    "y_A = true_function(X_A.flatten()) + noise_A\n",
    "\n",
    "# Visualize Dataset A\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_A.flatten(), y=y_A, s=80, alpha=0.7)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"f(X)\")\n",
    "plt.title(\"Dataset A: Your Colleague's Measurements\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset A: {len(X_A)} data points\")\n",
    "print(f\"X range: [{X_A.min():.1f}, {X_A.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed61c850",
   "metadata": {},
   "source": [
    "### Task 2.1: Find the Best Fit for Dataset A\n",
    "\n",
    "Try fitting polynomials of degree 1, 2, and 3. Which one fits best? Use MSE and RÂ² to evaluate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21715999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit different polynomial degrees to Dataset A\n",
    "degrees = [1, 2, 3]\n",
    "results_A = {}\n",
    "\n",
    "print(\"Dataset A - Model Comparison\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"{'Degree':<10} {'MSE':<15} {'RÂ²':<10}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for deg in degrees:\n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(degree=deg, include_bias=False), LinearRegression()\n",
    "    )\n",
    "    model.fit(X_A, y_A)\n",
    "    y_pred = model.predict(X_A)\n",
    "\n",
    "    mse = mean_squared_error(y_A, y_pred)\n",
    "    r2 = r2_score(y_A, y_pred)\n",
    "\n",
    "    results_A[deg] = {\"model\": model, \"mse\": mse, \"r2\": r2}\n",
    "    print(f\"{deg:<10} {mse:<15.2f} {r2:<10.4f}\")\n",
    "\n",
    "# Find best model\n",
    "best_deg_A = min(results_A, key=lambda d: results_A[d][\"mse\"])\n",
    "print(f\"\\nâœ“ Best fit for Dataset A: Degree {best_deg_A} (lowest MSE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the fits for Dataset A\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "X_smooth = np.linspace(X_A.min() - 0.5, X_A.max() + 0.5, 100).reshape(-1, 1)\n",
    "\n",
    "for ax, deg in zip(axes, degrees):\n",
    "    model = results_A[deg][\"model\"]\n",
    "    y_smooth = model.predict(X_smooth)\n",
    "\n",
    "    sns.scatterplot(x=X_A.flatten(), y=y_A, s=60, alpha=0.7, ax=ax)\n",
    "    ax.plot(X_smooth.flatten(), y_smooth, \"r-\", linewidth=2, label=f\"Degree {deg}\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"f(X)\")\n",
    "    ax.set_title(\n",
    "        f\"Degree {deg}\\nMSE: {results_A[deg]['mse']:.1f}, RÂ²: {results_A[deg]['r2']:.3f}\"\n",
    "    )\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle(\"Dataset A: Polynomial Fits\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    \"\\nðŸ“ Based on this analysis, what would you conclude about the underlying relationship?\"\n",
    ")\n",
    "print(\"   The data appears to follow a __________ pattern.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba4a2ed",
   "metadata": {},
   "source": [
    "### Your Conclusion for Dataset A\n",
    "\n",
    "**Question:** Based on your analysis, what type of relationship does the data show?\n",
    "\n",
    "_Write your answer here before continuing..._\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61b8c8",
   "metadata": {},
   "source": [
    "## 3. Dataset B â€” A Second Project\n",
    "\n",
    "A different colleague collected measurements from another experiment. Let's analyze this one too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b0962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset B: Collected from the \"quadratic-looking\" region\n",
    "np.random.seed(123)\n",
    "\n",
    "# Sample symmetrically around x=0\n",
    "X_B = np.linspace(-2, 2, 20).reshape(-1, 1)\n",
    "noise_B = np.random.normal(0, 1, size=X_B.shape[0])\n",
    "\n",
    "y_B = true_function(X_B.flatten()) + noise_B\n",
    "\n",
    "# Visualize Dataset B\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_B.flatten(), y=y_B, s=80, alpha=0.7, color=\"orange\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"f(X)\")\n",
    "plt.title(\"Dataset B: Another Colleague's Measurements\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset B: {len(X_B)} data points\")\n",
    "print(f\"X range: [{X_B.min():.1f}, {X_B.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ee25c9",
   "metadata": {},
   "source": [
    "### Task 3.1: Find the Best Fit for Dataset B\n",
    "\n",
    "Same task â€” fit polynomials of degree 1, 2, and 3. Which fits best?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105cfb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit different polynomial degrees to Dataset B\n",
    "results_B = {}\n",
    "\n",
    "print(\"Dataset B - Model Comparison\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"{'Degree':<10} {'MSE':<15} {'RÂ²':<10}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for deg in degrees:\n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(degree=deg, include_bias=False), LinearRegression()\n",
    "    )\n",
    "    model.fit(X_B, y_B)\n",
    "    y_pred = model.predict(X_B)\n",
    "\n",
    "    mse = mean_squared_error(y_B, y_pred)\n",
    "    r2 = r2_score(y_B, y_pred)\n",
    "\n",
    "    results_B[deg] = {\"model\": model, \"mse\": mse, \"r2\": r2}\n",
    "    print(f\"{deg:<10} {mse:<15.2f} {r2:<10.4f}\")\n",
    "\n",
    "# Find best model\n",
    "best_deg_B = min(results_B, key=lambda d: results_B[d][\"mse\"])\n",
    "print(f\"\\nâœ“ Best fit for Dataset B: Degree {best_deg_B} (lowest MSE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b58e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the fits for Dataset B\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "X_smooth_B = np.linspace(X_B.min() - 0.5, X_B.max() + 0.5, 100).reshape(-1, 1)\n",
    "\n",
    "for ax, deg in zip(axes, degrees):\n",
    "    model = results_B[deg][\"model\"]\n",
    "    y_smooth = model.predict(X_smooth_B)\n",
    "\n",
    "    sns.scatterplot(x=X_B.flatten(), y=y_B, s=60, alpha=0.7, ax=ax, color=\"orange\")\n",
    "    ax.plot(X_smooth_B.flatten(), y_smooth, \"r-\", linewidth=2, label=f\"Degree {deg}\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"f(X)\")\n",
    "    ax.set_title(\n",
    "        f\"Degree {deg}\\nMSE: {results_B[deg]['mse']:.1f}, RÂ²: {results_B[deg]['r2']:.3f}\"\n",
    "    )\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle(\"Dataset B: Polynomial Fits\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    \"\\nðŸ“ Based on this analysis, what would you conclude about the underlying relationship?\"\n",
    ")\n",
    "print(\"   The data appears to follow a __________ pattern.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a459a37",
   "metadata": {},
   "source": [
    "### Your Conclusion for Dataset B\n",
    "\n",
    "**Question:** Based on your analysis, what type of relationship does this data show?\n",
    "\n",
    "_Write your answer here before continuing..._\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd9a47",
   "metadata": {},
   "source": [
    "## 4. Wait... Something's Wrong\n",
    "\n",
    "Let's compare our conclusions:\n",
    "\n",
    "- **Dataset A** â†’ Best fit was degree \\_\\_\\_\n",
    "- **Dataset B** â†’ Best fit was degree \\_\\_\\_\n",
    "\n",
    "Two different experiments, two different conclusions. But what if I told you...\n",
    "\n",
    "**Both datasets came from the SAME underlying function.**\n",
    "\n",
    "Let's reveal the truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c84875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE BIG REVEAL: Both datasets came from the same 4th-order polynomial!\n",
    "\n",
    "# Plot the true function\n",
    "X_full = np.linspace(-7, 7, 500)\n",
    "y_full = true_function(X_full)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot the true curve\n",
    "plt.plot(\n",
    "    X_full,\n",
    "    y_full,\n",
    "    \"b-\",\n",
    "    linewidth=2,\n",
    "    label=\"True function: $f(x) = 0.2x^4 - 4x^2 + 20$\",\n",
    "    zorder=1,\n",
    ")\n",
    "\n",
    "# Plot both datasets\n",
    "plt.scatter(\n",
    "    X_A.flatten(),\n",
    "    y_A,\n",
    "    s=100,\n",
    "    alpha=0.8,\n",
    "    label='Dataset A (\"linear\" region)',\n",
    "    color=\"steelblue\",\n",
    "    edgecolor=\"black\",\n",
    "    zorder=3,\n",
    ")\n",
    "plt.scatter(\n",
    "    X_B.flatten(),\n",
    "    y_B,\n",
    "    s=100,\n",
    "    alpha=0.8,\n",
    "    label='Dataset B (\"quadratic\" region)',\n",
    "    color=\"orange\",\n",
    "    edgecolor=\"black\",\n",
    "    zorder=3,\n",
    ")\n",
    "\n",
    "# Highlight the sampling regions\n",
    "plt.axvspan(-6.5, -5.5, alpha=0.2, color=\"steelblue\", label=\"Region A sampled\")\n",
    "plt.axvspan(-2, 2, alpha=0.2, color=\"orange\", label=\"Region B sampled\")\n",
    "\n",
    "plt.xlabel(\"X\", fontsize=12)\n",
    "plt.ylabel(\"f(X)\", fontsize=12)\n",
    "plt.title(\"THE TRUTH: Both Datasets Came From the Same Function!\", fontsize=14)\n",
    "plt.legend(loc=\"upper center\", fontsize=10)\n",
    "plt.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ¤¯ SURPRISE!\")\n",
    "print(\"\\nBoth datasets were sampled from: f(x) = 0.2xâ´ - 4xÂ² + 20\")\n",
    "print(\"\\nBut because each dataset only captured a SMALL REGION of the function:\")\n",
    "print(\"  â€¢ Dataset A looked linear (steep declining section)\")\n",
    "print(\"  â€¢ Dataset B looked quadratic (around the local maximum at x=0)\")\n",
    "print(\"\\nNeither analysis discovered the true 4th-order polynomial!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e63f9",
   "metadata": {},
   "source": [
    "## 5. What Went Wrong?\n",
    "\n",
    "This is **sampling bias** â€” when your data collection systematically misses important parts of the phenomenon you're trying to study.\n",
    "\n",
    "### The Problem Isn't the Algorithm\n",
    "\n",
    "Both analyses were done correctly:\n",
    "\n",
    "- We used proper polynomial regression\n",
    "- We evaluated with MSE and RÂ²\n",
    "- We compared multiple models\n",
    "\n",
    "**The problem was the DATA:**\n",
    "\n",
    "- Dataset A only sampled from x âˆˆ [-6.5, -5.5] â€” a narrow slice of the steep decline\n",
    "- Dataset B only sampled from x âˆˆ [-2, 2] â€” symmetrically around the local maximum\n",
    "- Neither captured the full structure of the underlying function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df63b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate: What if we extrapolate our \"best\" models?\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "X_extrap = np.linspace(-7, 7, 200).reshape(-1, 1)\n",
    "\n",
    "# Left: Dataset A's \"best\" model extrapolated\n",
    "ax = axes[0]\n",
    "ax.plot(X_full, y_full, \"b-\", linewidth=2, alpha=0.5, label=\"True function\")\n",
    "ax.scatter(X_A.flatten(), y_A, s=60, alpha=0.7, color=\"steelblue\", label=\"Dataset A\")\n",
    "\n",
    "# Extrapolate the linear fit\n",
    "model_A = results_A[1][\"model\"]  # Linear fit\n",
    "y_extrap_A = model_A.predict(X_extrap)\n",
    "ax.plot(\n",
    "    X_extrap.flatten(),\n",
    "    y_extrap_A,\n",
    "    \"r--\",\n",
    "    linewidth=2,\n",
    "    label=\"Linear fit (extrapolated)\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"f(X)\")\n",
    "ax.set_title(\n",
    "    \"Dataset A: Linear Model Extrapolated\\n(Completely wrong outside the sample region!)\"\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_ylim(-100, 700)\n",
    "\n",
    "# Right: Dataset B's \"best\" model extrapolated\n",
    "ax = axes[1]\n",
    "ax.plot(X_full, y_full, \"b-\", linewidth=2, alpha=0.5, label=\"True function\")\n",
    "ax.scatter(X_B.flatten(), y_B, s=60, alpha=0.7, color=\"orange\", label=\"Dataset B\")\n",
    "\n",
    "# Extrapolate the quadratic fit\n",
    "model_B = results_B[2][\"model\"]  # Quadratic fit\n",
    "y_extrap_B = model_B.predict(X_extrap)\n",
    "ax.plot(\n",
    "    X_extrap.flatten(),\n",
    "    y_extrap_B,\n",
    "    \"r--\",\n",
    "    linewidth=2,\n",
    "    label=\"Quadratic fit (extrapolated)\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"f(X)\")\n",
    "ax.set_title(\n",
    "    \"Dataset B: Quadratic Model Extrapolated\\n(Also wrong outside the sample region!)\"\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_ylim(-100, 700)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    \"ðŸ’¡ Key insight: Models that fit the data perfectly can still be COMPLETELY WRONG\"\n",
    ")\n",
    "print(\"   when extrapolated beyond the sampled region.\")\n",
    "print(\"\\n   This is why data collection strategy is CRITICAL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3affb9c0",
   "metadata": {},
   "source": [
    "## 6. Real-World Examples of Sampling Bias\n",
    "\n",
    "This isn't just an academic exercise. Sampling bias causes real problems:\n",
    "\n",
    "| Domain                | Sampling Bias Example                     | Consequence                 |\n",
    "| --------------------- | ----------------------------------------- | --------------------------- |\n",
    "| **Medicine**          | Drug tested only on young adults          | Unknown effects on elderly  |\n",
    "| **Finance**           | Model trained on bull market data         | Fails during crashes        |\n",
    "| **Self-driving cars** | Training data from sunny California       | Poor performance in snow    |\n",
    "| **Hiring AI**         | Trained on past (biased) hiring decisions | Perpetuates discrimination  |\n",
    "| **Climate models**    | Temperature data only from cities         | Misses rural/ocean patterns |\n",
    "\n",
    "**The lesson:** Your model is only as good as your data's coverage of the problem space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec937cf",
   "metadata": {},
   "source": [
    "## 7. Combining the Datasets\n",
    "\n",
    "What if we had collected data from BOTH regions? Would we discover the true pattern?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde8a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both datasets\n",
    "X_combined = np.vstack([X_A, X_B])\n",
    "y_combined = np.concatenate([y_A, y_B])\n",
    "\n",
    "print(f\"Combined dataset: {len(X_combined)} points\")\n",
    "print(f\"X range: [{X_combined.min():.1f}, {X_combined.max():.1f}]\")\n",
    "\n",
    "# Fit multiple polynomial degrees\n",
    "degrees_extended = [1, 2, 3, 4, 5]\n",
    "results_combined = {}\n",
    "\n",
    "print(\"\\nCombined Dataset - Model Comparison\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"{'Degree':<10} {'MSE':<15} {'RÂ²':<10}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for deg in degrees_extended:\n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(degree=deg, include_bias=False), LinearRegression()\n",
    "    )\n",
    "    model.fit(X_combined, y_combined)\n",
    "    y_pred = model.predict(X_combined)\n",
    "\n",
    "    mse = mean_squared_error(y_combined, y_pred)\n",
    "    r2 = r2_score(y_combined, y_pred)\n",
    "\n",
    "    results_combined[deg] = {\"model\": model, \"mse\": mse, \"r2\": r2}\n",
    "    print(f\"{deg:<10} {mse:<15.2f} {r2:<10.4f}\")\n",
    "\n",
    "best_deg_combined = min(results_combined, key=lambda d: results_combined[d][\"mse\"])\n",
    "print(f\"\\nâœ“ Best fit for combined data: Degree {best_deg_combined}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c875cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the combined fit\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# True function\n",
    "plt.plot(X_full, y_full, \"b-\", linewidth=2, alpha=0.5, label=\"True function (degree 4)\")\n",
    "\n",
    "# Combined data\n",
    "plt.scatter(\n",
    "    X_A.flatten(),\n",
    "    y_A,\n",
    "    s=80,\n",
    "    alpha=0.7,\n",
    "    color=\"steelblue\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"Dataset A\",\n",
    ")\n",
    "plt.scatter(\n",
    "    X_B.flatten(),\n",
    "    y_B,\n",
    "    s=80,\n",
    "    alpha=0.7,\n",
    "    color=\"orange\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"Dataset B\",\n",
    ")\n",
    "\n",
    "# Best fit from combined data\n",
    "best_model = results_combined[best_deg_combined][\"model\"]\n",
    "X_plot = np.linspace(-7, 7, 200).reshape(-1, 1)\n",
    "y_plot = best_model.predict(X_plot)\n",
    "plt.plot(\n",
    "    X_plot.flatten(),\n",
    "    y_plot,\n",
    "    \"r--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Best fit (degree {best_deg_combined})\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"X\", fontsize=12)\n",
    "plt.ylabel(\"f(X)\", fontsize=12)\n",
    "plt.title(\"Combined Datasets: Better Coverage â†’ Better Model?\", fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ¤” Better! But there's still a gap in the data (x âˆˆ [-4, -1]).\")\n",
    "print(\"   The model might still be uncertain in that region.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1cced",
   "metadata": {},
   "source": [
    "## 8. The Data Collection Plan\n",
    "\n",
    "Now that you understand the problem, let's design a better data collection strategy.\n",
    "\n",
    "### Task 8.1: Design Your Sampling Strategy\n",
    "\n",
    "If you could collect 30 new data points, where would you sample them from?\n",
    "\n",
    "Consider:\n",
    "\n",
    "- What regions are currently under-sampled?\n",
    "- Should points be evenly spaced or concentrated in certain areas?\n",
    "- What would help you distinguish between degree 3, 4, and 5 polynomials?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85db500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize current coverage and gaps\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Show the true function\n",
    "plt.plot(X_full, y_full, \"b-\", linewidth=2, alpha=0.3, label=\"True function\")\n",
    "\n",
    "# Show existing data\n",
    "plt.scatter(X_A.flatten(), y_A, s=60, alpha=0.7, color=\"steelblue\", label=\"Dataset A\")\n",
    "plt.scatter(X_B.flatten(), y_B, s=60, alpha=0.7, color=\"orange\", label=\"Dataset B\")\n",
    "\n",
    "# Highlight gaps\n",
    "plt.axvspan(-5.5, -2, alpha=0.3, color=\"red\", label=\"GAP: No data here!\")\n",
    "plt.axvspan(2, 7, alpha=0.3, color=\"red\")\n",
    "plt.axvspan(-7, -6.5, alpha=0.3, color=\"red\")\n",
    "\n",
    "plt.xlabel(\"X\", fontsize=12)\n",
    "plt.ylabel(\"f(X)\", fontsize=12)\n",
    "plt.title(\"Where Should We Sample Next?\", fontsize=14)\n",
    "plt.legend(loc=\"upper center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“ Current gaps in coverage:\")\n",
    "print(\"   â€¢ x âˆˆ [-7, -6.5]: Left edge\")\n",
    "print(\"   â€¢ x âˆˆ [-5.5, -2]: Between datasets A and B\")\n",
    "print(\"   â€¢ x âˆˆ [2, 7]: Entire right side (includes local minimum at xâ‰ˆ3.16!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR DATA COLLECTION PLAN\n",
    "# Modify this to specify where you would collect new samples\n",
    "\n",
    "# YOUR PLAN: Fill in where you would sample\n",
    "X_new = np.concatenate(\n",
    "    [\n",
    "        np.linspace(-7, -6, 5),  # Left edge\n",
    "        # np.linspace(-4, -1, 15),  # Main gap\n",
    "        # np.linspace(4, 7, 10),  # Right edge\n",
    "    ]\n",
    ").reshape(-1, 1)\n",
    "\n",
    "# Simulate collecting this data\n",
    "np.random.seed(456)\n",
    "noise_new = np.random.normal(0, 12, size=X_new.shape[0])\n",
    "y_new = true_function(X_new.flatten()) + noise_new\n",
    "\n",
    "print(f\"New data points: {len(X_new)}\")\n",
    "print(f\"X range: [{X_new.min():.1f}, {X_new.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf1646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine ALL data and refit\n",
    "X_all = np.vstack([X_A, X_B, X_new])\n",
    "y_all = np.concatenate([y_A, y_B, y_new])\n",
    "\n",
    "print(f\"Total data points: {len(X_all)}\")\n",
    "\n",
    "# Fit with the new complete dataset\n",
    "print(\"\\nFull Dataset - Model Comparison\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"{'Degree':<10} {'MSE':<15} {'RÂ²':<10}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "results_all = {}\n",
    "for deg in degrees_extended:\n",
    "    model = make_pipeline(\n",
    "        PolynomialFeatures(degree=deg, include_bias=False), LinearRegression()\n",
    "    )\n",
    "    model.fit(X_all, y_all)\n",
    "    y_pred = model.predict(X_all)\n",
    "\n",
    "    mse = mean_squared_error(y_all, y_pred)\n",
    "    r2 = r2_score(y_all, y_pred)\n",
    "\n",
    "    results_all[deg] = {\"model\": model, \"mse\": mse, \"r2\": r2}\n",
    "    print(f\"{deg:<10} {mse:<15.2f} {r2:<10.4f}\")\n",
    "\n",
    "best_deg_all = min(results_all, key=lambda d: results_all[d][\"mse\"])\n",
    "print(f\"\\nâœ“ Best fit with complete data: Degree {best_deg_all}\")\n",
    "print(\"  (True function is degree 4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd40821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization with complete data\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# True function\n",
    "plt.plot(X_full, y_full, \"b-\", linewidth=3, alpha=0.5, label=\"True function (degree 4)\")\n",
    "\n",
    "# All data points\n",
    "plt.scatter(\n",
    "    X_A.flatten(),\n",
    "    y_A,\n",
    "    s=60,\n",
    "    alpha=0.6,\n",
    "    color=\"steelblue\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"Dataset A (original)\",\n",
    ")\n",
    "plt.scatter(\n",
    "    X_B.flatten(),\n",
    "    y_B,\n",
    "    s=60,\n",
    "    alpha=0.6,\n",
    "    color=\"orange\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"Dataset B (original)\",\n",
    ")\n",
    "plt.scatter(\n",
    "    X_new.flatten(),\n",
    "    y_new,\n",
    "    s=60,\n",
    "    alpha=0.6,\n",
    "    color=\"green\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"New samples (your plan)\",\n",
    ")\n",
    "\n",
    "# Best fit\n",
    "best_model_all = results_all[best_deg_all][\"model\"]\n",
    "y_plot_all = best_model_all.predict(X_plot)\n",
    "plt.plot(\n",
    "    X_plot.flatten(),\n",
    "    y_plot_all,\n",
    "    \"r--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Best fit (degree {best_deg_all})\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"X\", fontsize=12)\n",
    "plt.ylabel(\"f(X)\", fontsize=12)\n",
    "plt.title(\n",
    "    \"Complete Dataset: With Better Coverage Comes Better Understanding\", fontsize=14\n",
    ")\n",
    "plt.legend(fontsize=10)\n",
    "plt.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd7fa5",
   "metadata": {},
   "source": [
    "## ðŸŽ“ CRITICAL THINKING QUESTION\n",
    "\n",
    "Look at your results above. The \"best fit\" model has the lowest MSE, but **the true function is degree 4**.\n",
    "\n",
    "**Key questions to consider:**\n",
    "\n",
    "- **If your best fit is degree 5:** Why might a more complex model fit better? Should you use it?\n",
    "- **If your best fit is degree 4:** Great! But would you have known this was correct without being told the truth?\n",
    "- **If your best fit is degree 3 or lower:** What does this tell you about your data coverage?\n",
    "\n",
    "**The critical question:** In real scenarios where you don't know the true function, how could you detect whether your model is overfitting, underfitting, or just right?\n",
    "\n",
    "### The Deeper Lesson\n",
    "\n",
    "What you may have just witnessed is **overfitting** - one of the most important concepts in machine learning. If your best-fit model is degree 5, it likely didn't discover anything new about the underlying relationship; it just learned to fit the random noise in your specific sample.\n",
    "\n",
    "**In real-world scenarios:**\n",
    "\n",
    "- You never know the true function\n",
    "- \"Best fit on training data\" â‰  \"correct model\"\n",
    "- More complex models aren't always better, even when they achieve lower error\n",
    "- The difference between degree 4 and 5 might be small - are you overfitting or discovering real complexity?\n",
    "\n",
    "**This is exactly why we need:**\n",
    "\n",
    "- **Train/validation/test splits** - to detect overfitting\n",
    "- **Cross-validation** - to estimate true generalization performance\n",
    "- **Regularization** - to penalize unnecessary complexity\n",
    "- **Model selection criteria** (AIC/BIC) - to balance fit and complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f059f6",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Data quality beats algorithm quality**\n",
    "   - The best algorithm can't fix fundamentally flawed data\n",
    "   - \"Garbage in, garbage out\" is not just a saying\n",
    "\n",
    "2. **Sampling bias is invisible in the data itself**\n",
    "   - Dataset A looked perfectly linear â€” nothing suggested otherwise\n",
    "   - Dataset B looked perfectly quadratic â€” nothing suggested otherwise\n",
    "   - The bias was in what we DIDN'T collect\n",
    "\n",
    "3. **Good fits don't guarantee correct models**\n",
    "   - RÂ² = 0.99 doesn't mean you found the truth\n",
    "   - It means you fit the data you HAVE well\n",
    "\n",
    "4. **Data collection is a design problem**\n",
    "   - Where you sample determines what you can learn\n",
    "   - Coverage of the problem space is critical\n",
    "\n",
    "5. **Extrapolation is dangerous**\n",
    "   - Models trained on limited data can fail catastrophically outside that range\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ffe29",
   "metadata": {},
   "source": [
    "## 10. Deliverable: Your Memo\n",
    "\n",
    "Write a one-page memo (in markdown below or in a separate document) addressing:\n",
    "\n",
    "### Memo Template\n",
    "\n",
    "**To:** Research Lab Director  \n",
    "**From:** [Your Name]  \n",
    "**Subject:** Data Collection Issues and Recommendations\n",
    "\n",
    "**1. What went wrong?**\n",
    "\n",
    "- Describe the problem with the original data collection\n",
    "- Explain why the analyses gave misleading results\n",
    "\n",
    "**2. What did we learn?**\n",
    "\n",
    "- What is the true underlying relationship?\n",
    "- How did sampling bias hide this from us?\n",
    "\n",
    "**3. How do we fix it?**\n",
    "\n",
    "- What is your recommended data collection strategy?\n",
    "- Where should new samples be collected?\n",
    "- How many samples do we need?\n",
    "\n",
    "**4. Lessons for future projects**\n",
    "\n",
    "- What should we always consider before fitting models?\n",
    "- How can we detect sampling bias?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98cf488",
   "metadata": {},
   "source": [
    "### Your Memo (Write Here)\n",
    "\n",
    "_Double-click this cell to edit_\n",
    "\n",
    "---\n",
    "\n",
    "**To:** Research Lab Director  \n",
    "**From:** [Your Name]  \n",
    "**Subject:** Data Collection Issues and Recommendations\n",
    "\n",
    "**1. What went wrong?**\n",
    "\n",
    "[Your answer here]\n",
    "\n",
    "**2. What did we learn?**\n",
    "\n",
    "[Your answer here]\n",
    "\n",
    "**3. How do we fix it?**\n",
    "\n",
    "[Your answer here]\n",
    "\n",
    "**4. Lessons for future projects**\n",
    "\n",
    "[Your answer here]\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feefeb5",
   "metadata": {},
   "source": [
    "## The Bottom Line\n",
    "\n",
    "> **\"All models are wrong, but some are useful.\"** â€” George Box\n",
    "\n",
    "> **\"All models are wrong, but some are wrong because the data was collected wrong.\"** â€” This lab\n",
    "\n",
    "> **Before asking \"which algorithm should I use?\", ask \"is my data representative?\"**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ing3513",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
