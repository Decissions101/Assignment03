{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a88b8a6",
   "metadata": {},
   "source": [
    "# Lab 06: Error Surfaces & Gradient Descent\n",
    "\n",
    "**ING3513 - Introduction to Artificial Intelligence and Machine Learning**\n",
    "\n",
    "In Lab 05, you manually adjusted weights $w_1$, $w_2$ and bias $\\beta$ to find a **diagonal decision boundary** that separated Pine from Birch. You discovered that both grain prominence AND brightness matter for classification.\n",
    "\n",
    "But how did you know **which direction** to move the sliders? You probably used intuition and trial-and-error.\n",
    "\n",
    "This lab reveals the **systematic approach** that machine learning algorithms use.\n",
    "\n",
    "**What you'll learn:**\n",
    "\n",
    "- **Error surfaces** ‚Äî visualizing how loss changes as you adjust parameters\n",
    "- **Gradient descent** ‚Äî the algorithm that \"crawls\" downhill to find optimal weights\n",
    "- **Learning rate** ‚Äî why step size matters (too small = slow, too large = chaos)\n",
    "- **Local vs global minima** ‚Äî when algorithms get stuck in the wrong place\n",
    "- **Why this matters** ‚Äî every deep learning model you'll ever use trains this way\n",
    "\n",
    "**The intuition:** Imagine you're blindfolded on a hillside. You want to reach the valley (minimum error). Gradient descent says: \"Feel the slope under your feet, and take a step downhill.\" Repeat until you reach the bottom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335b594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Configure plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da57278",
   "metadata": {},
   "source": [
    "## Part 0: Why This Lab Exists\n",
    "\n",
    "In Lab 05, you saw that changing weights $w_1, w_2$ and bias $\\beta$ moves the decision boundary. You manually tuned these parameters to find the **diagonal line** that separated Pine from Birch ‚Äî using **both** grain prominence and brightness.\n",
    "\n",
    "But real machine learning models have **thousands or millions** of parameters. We can't tune them by hand!\n",
    "\n",
    "**The central question:** How does an algorithm automatically find the best parameters?\n",
    "\n",
    "**One powerful answer:** Follow the **negative gradient** ‚Äî the direction of steepest descent on the error surface. This is called **gradient descent**, and it's by far the most widely used optimization method in machine learning.\n",
    "\n",
    "> **Note:** There are other optimization algorithms (e.g., Newton's method, evolutionary algorithms, closed-form solutions for simple models), but gradient descent dominates modern ML because it scales well to millions of parameters.\n",
    "\n",
    "Today, you'll:\n",
    "\n",
    "1. **See** the error surface (a 3D landscape where height = error)\n",
    "2. **Watch** gradient descent crawl along this surface toward the minimum\n",
    "3. **Understand** why learning rate and initialization matter\n",
    "4. **Experience** what happens with non-convex (bumpy) surfaces\n",
    "\n",
    "Let's continue with the familiar wood dataset from Lab 05.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe2c06b",
   "metadata": {},
   "source": [
    "## Part 1: Generate the Wood Dataset (Review from Lab 05)\n",
    "\n",
    "We'll use the same wood classification problem from Lab 05: distinguishing **Pine** from **Birch** based on two features:\n",
    "\n",
    "| Feature              | Symbol | What it measures                                   | Think of it as...                                                  |\n",
    "| -------------------- | ------ | -------------------------------------------------- | ------------------------------------------------------------------ |\n",
    "| **Grain Prominence** | `gp`   | Contrast between light and dark bands in the grain | How much the grain pattern \"pops\" ‚Äî subtle (0) vs bold stripes (1) |\n",
    "| **Brightness**       | `b`    | Average lightness of the wood surface              | How light or dark the wood looks overall (0=black, 10=white)       |\n",
    "\n",
    "The key insight from Lab 05: **both features help separate the classes, but neither alone is enough!**\n",
    "\n",
    "- **Grain Prominence:** Pine ~0.3, Birch ~0.5 ‚Äî significant overlap!\n",
    "- **Brightness:** Pine ~6.0, Birch ~5.0 ‚Äî significant overlap!\n",
    "\n",
    "Neither feature alone separates the classes. But **together**, they create a **diagonal** decision boundary ‚Äî exactly what you discovered in Lab 05!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f1445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STANDARDIZATION HELPERS\n",
    "# =============================================================================\n",
    "# Note: In practice, you'd use sklearn.preprocessing.StandardScaler which provides\n",
    "# fit(), transform(), and inverse_transform() methods. Here we implement our own\n",
    "# simple version to understand what's happening under the hood.\n",
    "\n",
    "\n",
    "def standardize(X):\n",
    "    \"\"\"Standardize features to mean=0, std=1. Returns (X_scaled, params).\"\"\"\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    X_scaled = (X - mean) / std\n",
    "    return X_scaled, {\"mean\": mean, \"std\": std}\n",
    "\n",
    "\n",
    "def unstandardize(X_scaled, params):\n",
    "    \"\"\"Transform standardized features back to original scale.\"\"\"\n",
    "    return X_scaled * params[\"std\"] + params[\"mean\"]\n",
    "\n",
    "\n",
    "def unstandardize_weights(w, beta, params):\n",
    "    \"\"\"Transform weights from standardized space to original feature space.\"\"\"\n",
    "    w_orig = w / params[\"std\"]\n",
    "    beta_orig = beta - np.sum(w * params[\"mean\"] / params[\"std\"])\n",
    "    return w_orig, beta_orig\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# GENERATE DATA\n",
    "# =============================================================================\n",
    "np.random.seed(42)\n",
    "n_samples = (\n",
    "    40  # Per class (more than Lab 05's 8, for smoother error surface visualization)\n",
    ")\n",
    "\n",
    "# PINE: Lower grain prominence, HIGHER brightness (upper-left in feature space)\n",
    "pine_gp = np.random.normal(\n",
    "    0.3, 0.1, n_samples\n",
    ")  # Grain prominence: centered around 0.3 (overlaps with Birch!)\n",
    "pine_b = np.random.normal(6.0, 0.8, n_samples)  # Brightness: centered around 6.0\n",
    "\n",
    "# BIRCH: Higher grain prominence, LOWER brightness (lower-right in feature space)\n",
    "birch_gp = np.random.normal(\n",
    "    0.5, 0.1, n_samples\n",
    ")  # Grain prominence: centered around 0.5 (overlaps with Pine!)\n",
    "birch_b = np.random.normal(\n",
    "    5.0, 0.8, n_samples\n",
    ")  # Brightness: centered around 5.0 (overlaps with Pine!)\n",
    "\n",
    "# Clip to valid ranges\n",
    "pine_gp = np.clip(pine_gp, 0, 1)\n",
    "pine_b = np.clip(pine_b, 0, 10)\n",
    "birch_gp = np.clip(birch_gp, 0, 1)\n",
    "birch_b = np.clip(birch_b, 0, 10)\n",
    "\n",
    "# Combine into arrays: [grain_prominence, brightness]\n",
    "# Using gp as a‚ÇÅ and b as a‚ÇÇ to match Lab 05's feature ordering and notation\n",
    "X_pine = np.column_stack([pine_gp, pine_b])\n",
    "X_birch = np.column_stack([birch_gp, birch_b])\n",
    "X_raw = np.vstack([X_pine, X_birch])\n",
    "\n",
    "# Labels: -1 for Pine, +1 for Birch\n",
    "y = np.concatenate([np.ones(n_samples) * -1, np.ones(n_samples)])\n",
    "\n",
    "# STANDARDIZE features using our helper function\n",
    "# This is crucial for gradient descent! Without it, the error surface is very elongated.\n",
    "X, scaler_params = standardize(X_raw)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape} ({2 * n_samples} samples √ó 2 features)\")\n",
    "print(f\"Labels: {np.unique(y)} (-1 = Pine, +1 = Birch)\")\n",
    "print(f\"Class balance: {(y == -1).sum()} Pine, {(y == 1).sum()} Birch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b674a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data (standardized)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.scatter(\n",
    "    X[y == -1, 0],\n",
    "    X[y == -1, 1],\n",
    "    s=100,\n",
    "    alpha=0.7,\n",
    "    c=\"#2E86AB\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=1.5,\n",
    "    label=\"Pine\",\n",
    ")\n",
    "ax.scatter(\n",
    "    X[y == 1, 0],\n",
    "    X[y == 1, 1],\n",
    "    s=100,\n",
    "    alpha=0.7,\n",
    "    c=\"#A23B72\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=1.5,\n",
    "    label=\"Birch\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Grain Prominence (standardized)\", fontsize=12)\n",
    "ax.set_ylabel(\"Brightness (standardized)\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"Wood Classification Dataset (Standardized Features)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "ax.axvline(x=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dcf0a5",
   "metadata": {},
   "source": [
    "**Features are now standardized** (mean=0, std=1):\n",
    "\n",
    "- Pine (blue): lower grain prominence, higher brightness ‚Üí upper-left\n",
    "- Birch (pink): higher grain prominence, lower brightness ‚Üí lower-right\n",
    "\n",
    "**Why standardize?**\n",
    "\n",
    "- Original scales were very different (gp: 0-1, b: 0-10)\n",
    "- This made the error surface very elongated (oval)\n",
    "- Standardizing makes both features comparable ‚Üí rounder error surface\n",
    "- A horizontal line (using brightness only) misclassifies many points\n",
    "- But a **diagonal line** using **both features** can separate them!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f89c8ec",
   "metadata": {},
   "source": [
    "## Part 2: Define the Model and Loss Function\n",
    "\n",
    "To visualize the error surface in **3D**, we need exactly **2 parameters**. So we'll start with a simple linear model **without bias**:\n",
    "\n",
    "$$\\hat{y} = w_1 a_1 + w_2 a_2$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $a_1$ = grain prominence (gp)\n",
    "- $a_2$ = brightness (b)\n",
    "- $w_1, w_2$ = weights (parameters to learn)\n",
    "\n",
    "This is the same perceptron model from Lab 05: $f(\\mathbf{w}, \\beta) = w_1 \\cdot a_1 + w_2 \\cdot a_2 + \\beta$, but initially without the bias term.\n",
    "\n",
    "The **decision boundary** is where $\\hat{y} = 0$:\n",
    "\n",
    "$$w_1 a_1 + w_2 a_2 = 0 \\quad \\Rightarrow \\quad a_2 = -\\frac{w_1}{w_2} a_1$$\n",
    "\n",
    "This is a line through the origin with slope $-w_1/w_2$.\n",
    "\n",
    "**Connection to Lab 05:** In Lab 05, you manually tuned $w_1$, $w_2$, and $\\beta$ with sliders to find a diagonal boundary. Now you'll see how gradient descent finds these values automatically!\n",
    "\n",
    "### The Loss Function: Mean Squared Error (MSE)\n",
    "\n",
    "We measure how \"wrong\" our predictions are using **Mean Squared Error**:\n",
    "\n",
    "$$\\text{MSE}(w) = \\frac{1}{n}\\sum_{i=1}^n(\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "where $\\hat{y}_i = w_1 a_{i,1} + w_2 a_{i,2}$ is the model's prediction for sample $i$.\n",
    "\n",
    "**Key insight:** For any choice of $(w_1, w_2)$, we can compute the MSE. This creates a **surface** where:\n",
    "\n",
    "- **Horizontal axes:** $w_1$ and $w_2$ (the parameter space)\n",
    "\n",
    "- **Vertical axis:** MSE (the error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884cd469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MSE loss function for weights (w1, w2) without bias\n",
    "def mse_loss(w1, w2, X, y):\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error for linear model without bias.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    w1, w2 : float\n",
    "        Weights for the two features\n",
    "    X : ndarray of shape (n_samples, 2)\n",
    "        Feature matrix\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        True labels\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    loss : float\n",
    "        Mean squared error\n",
    "    \"\"\"\n",
    "    y_pred = w1 * X[:, 0] + w2 * X[:, 1]  # Linear predictions\n",
    "    loss = np.mean((y_pred - y) ** 2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Test it with random weights\n",
    "w1_test, w2_test = 0.5, 0.3\n",
    "test_loss = mse_loss(w1_test, w2_test, X, y)\n",
    "print(f\"MSE with w1={w1_test}, w2={w2_test}: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25903836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision boundary for a given set of weights (standardized space)\n",
    "def plot_decision_boundary(w1, w2, X, y, title=\"Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plot the data and decision boundary for given weights.\n",
    "\n",
    "    Boundary: w1*x1 + w2*x2 = 0  =>  x2 = -(w1/w2)*x1\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot data points\n",
    "    ax.scatter(\n",
    "        X[y == -1, 0],\n",
    "        X[y == -1, 1],\n",
    "        s=100,\n",
    "        alpha=0.7,\n",
    "        c=\"#2E86AB\",\n",
    "        edgecolors=\"black\",\n",
    "        linewidth=1.5,\n",
    "        label=\"Pine\",\n",
    "    )\n",
    "    ax.scatter(\n",
    "        X[y == 1, 0],\n",
    "        X[y == 1, 1],\n",
    "        s=100,\n",
    "        alpha=0.7,\n",
    "        c=\"#A23B72\",\n",
    "        edgecolors=\"black\",\n",
    "        linewidth=1.5,\n",
    "        label=\"Birch\",\n",
    "    )\n",
    "\n",
    "    # Plot decision boundary (if w2 != 0)\n",
    "    if abs(w2) > 1e-6:\n",
    "        x1_range = np.array([-3, 3])\n",
    "        x2_boundary = -(w1 / w2) * x1_range\n",
    "        ax.plot(x1_range, x2_boundary, \"k--\", linewidth=2, label=\"Decision boundary\")\n",
    "\n",
    "    ax.set_xlabel(\"Grain Prominence (standardized)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Brightness (standardized)\", fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlim([-3, 3])\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.3)\n",
    "    ax.axvline(x=0, color=\"gray\", linestyle=\"--\", alpha=0.3)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example: Random initialization\n",
    "w1_init, w2_init = 0.5, 0.5\n",
    "loss_init = mse_loss(w1_init, w2_init, X, y)\n",
    "plot_decision_boundary(\n",
    "    w1_init,\n",
    "    w2_init,\n",
    "    X,\n",
    "    y,\n",
    "    title=f\"Initial Weights: w‚ÇÅ={w1_init}, w‚ÇÇ={w2_init} | MSE={loss_init:.4f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba9da74",
   "metadata": {},
   "source": [
    "## Part 3: Compute the Error Surface\n",
    "\n",
    "Now for the exciting part: let's create a **grid of $(w_1, w_2)$ values** and compute the MSE at each point. This gives us the **error surface**.\n",
    "\n",
    "> ‚ö†Ô∏è **Important:** We're computing the entire surface here **only for visualization**! In practice, gradient descent **never sees the full surface** ‚Äî it only computes the gradient at the current position (like feeling the slope under your feet while blindfolded). This is what makes gradient descent scalable to millions of parameters.\n",
    "\n",
    "**What we'll see:**\n",
    "\n",
    "- A 3D surface plot (height = MSE)\n",
    "- A contour plot (top-down view, like a topographic map)\n",
    "\n",
    "The **minimum** of this surface is where gradient descent wants to go!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba9da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of (w1, w2) values\n",
    "# With standardized features, weights will be on similar scales\n",
    "# w1 (grain prominence): positive ‚Äî higher gp ‚Üí more likely Birch (+1)\n",
    "# w2 (brightness): negative ‚Äî higher brightness ‚Üí more likely Pine (-1)\n",
    "w1_range = np.linspace(-3, 3, 100)\n",
    "w2_range = np.linspace(-3, 3, 100)\n",
    "W1_grid, W2_grid = np.meshgrid(w1_range, w2_range)\n",
    "\n",
    "# Compute MSE at each grid point\n",
    "MSE_grid = np.zeros_like(W1_grid)\n",
    "for i in range(len(w1_range)):\n",
    "    for j in range(len(w2_range)):\n",
    "        MSE_grid[j, i] = mse_loss(W1_grid[j, i], W2_grid[j, i], X, y)\n",
    "\n",
    "# Find the minimum MSE location\n",
    "min_idx = np.unravel_index(np.argmin(MSE_grid), MSE_grid.shape)\n",
    "w1_optimal = W1_grid[min_idx]\n",
    "w2_optimal = W2_grid[min_idx]\n",
    "mse_optimal = MSE_grid[min_idx]\n",
    "\n",
    "print(f\"Error surface grid shape: {MSE_grid.shape}\")\n",
    "print(f\"MSE range: [{MSE_grid.min():.4f}, {MSE_grid.max():.4f}]\")\n",
    "print(f\"Optimal weights (from grid): w‚ÇÅ={w1_optimal:.4f}, w‚ÇÇ={w2_optimal:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8780b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive 3D error surface using Plotly\n",
    "# You can rotate, zoom, and pan the surface!\n",
    "\n",
    "fig_3d = go.Figure()\n",
    "\n",
    "# Add the error surface\n",
    "fig_3d.add_trace(\n",
    "    go.Surface(\n",
    "        x=W1_grid,\n",
    "        y=W2_grid,\n",
    "        z=MSE_grid,\n",
    "        colorscale=\"Viridis\",\n",
    "        opacity=0.9,\n",
    "        name=\"Error Surface\",\n",
    "        showscale=True,\n",
    "        colorbar=dict(title=\"MSE\", x=1.02),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Mark the minimum point\n",
    "fig_3d.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=[w1_optimal],\n",
    "        y=[w2_optimal],\n",
    "        z=[mse_optimal],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=10, color=\"red\", symbol=\"diamond\"),\n",
    "        name=f\"Minimum ({w1_optimal:.2f}, {w2_optimal:.2f})\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_3d.update_layout(\n",
    "    title=dict(\n",
    "        text=\"<b>Interactive 3D Error Surface</b><br><sup>Drag to rotate, scroll to zoom</sup>\",\n",
    "        x=0.5,\n",
    "    ),\n",
    "    scene=dict(\n",
    "        xaxis_title=\"w‚ÇÅ\",\n",
    "        yaxis_title=\"w‚ÇÇ\",\n",
    "        zaxis_title=\"MSE\",\n",
    "        camera=dict(eye=dict(x=1.5, y=1.5, z=1.2)),\n",
    "    ),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(l=0, r=0, t=60, b=0),\n",
    ")\n",
    "\n",
    "fig_3d.show()\n",
    "\n",
    "# Contour plot (static, for comparison)\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "contour = ax.contour(W1_grid, W2_grid, MSE_grid, levels=30, cmap=\"viridis\")\n",
    "ax.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Mark the minimum point\n",
    "ax.scatter(\n",
    "    w1_optimal,\n",
    "    w2_optimal,\n",
    "    color=\"red\",\n",
    "    s=200,\n",
    "    marker=\"*\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=2,\n",
    "    zorder=5,\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"w‚ÇÅ\", fontsize=12)\n",
    "ax.set_ylabel(\"w‚ÇÇ\", fontsize=12)\n",
    "ax.set_title(\"Contour Plot (Minimum Marked)\", fontsize=14, fontweight=\"bold\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfdc31f",
   "metadata": {},
   "source": [
    "### üîç What You're Seeing\n",
    "\n",
    "**3D Surface (left):**\n",
    "\n",
    "- Each point $(w_1, w_2)$ has a \"height\" = MSE\n",
    "- The surface is **bowl-shaped** ‚Äî this is called a **convex** function\n",
    "- There's a single **global minimum** (the red star)\n",
    "- Far from the minimum, the error is high (bad predictions)\n",
    "\n",
    "**Contour Plot (right):**\n",
    "\n",
    "- Like a topographic map showing \"elevation\" of error\n",
    "- Lines closer together = steeper slope\n",
    "- The center (red star) is the lowest point\n",
    "- This is where gradient descent will try to reach!\n",
    "\n",
    "**Connection to Lab 05:** The optimal weights create the **diagonal decision boundary** you discovered manually. Now you can see exactly where that solution lives in parameter space!\n",
    "\n",
    "**Mini-task:** Look at the contour plot. If you start at $(w_1=0.5, w_2=0.5)$, which direction should you move to reduce error? (Answer: toward the center, downhill!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22bcbb4",
   "metadata": {},
   "source": [
    "## Part 4: Gradient Descent ‚Äî The Crawling Cursor\n",
    "\n",
    "Now we'll implement the algorithm that **automatically** finds the minimum.\n",
    "\n",
    "### How Gradient Descent Works\n",
    "\n",
    "Remember in Lab 05 when you manually adjusted sliders for $w_1$, $w_2$, and $\\beta$ to find the best decision boundary? Gradient descent does this automatically!\n",
    "\n",
    "Think of yourself blindfolded on a hill. To reach the valley:\n",
    "\n",
    "1. **Feel the slope** under your feet (compute the gradient)\n",
    "2. **Take a step downhill** (update weights in the negative gradient direction)\n",
    "3. **Repeat** until you reach the bottom (or get tired!)\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\eta \\nabla_w \\text{MSE}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $w^{(t)}$ = weights at iteration $t$\n",
    "- $\\eta$ = **learning rate** (step size)\n",
    "- $\\nabla_w \\text{MSE}$ = **gradient** (direction of steepest ascent)\n",
    "\n",
    "### Computing the Gradient (Analytically)\n",
    "\n",
    "For our MSE loss with $\\hat{y} = w_1 a_1 + w_2 a_2$:\n",
    "\n",
    "$$\\nabla_w \\text{MSE} = \\frac{2}{n} A^T (Aw - y)$$\n",
    "\n",
    "where $A$ is the feature matrix and $w = [w_1, w_2]^T$.\n",
    "\n",
    "This tells us how MSE changes as we tweak $w_1$ and $w_2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e2b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(w, X, y):\n",
    "    \"\"\"\n",
    "    Compute the gradient of MSE with respect to weights w.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    w : ndarray of shape (2,)\n",
    "        Current weights [w1, w2]\n",
    "    X : ndarray of shape (n_samples, 2)\n",
    "        Feature matrix\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        True labels\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    gradient : ndarray of shape (2,)\n",
    "        Gradient vector [‚àÇMSE/‚àÇw1, ‚àÇMSE/‚àÇw2]\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    y_pred = X @ w  # Matrix multiplication: X¬∑w\n",
    "    gradient = (2 / n) * X.T @ (y_pred - y)\n",
    "    return gradient\n",
    "\n",
    "\n",
    "# Test the gradient function\n",
    "w_test = np.array([0.5, 0.3])\n",
    "grad_test = compute_gradient(w_test, X, y)\n",
    "print(f\"Gradient at w=[0.5, 0.3]: {grad_test}\")\n",
    "print(\"Interpretation: To reduce MSE, move in the OPPOSITE direction of the gradient.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce95570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_init, learning_rate=0.01, epochs=100):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to find optimal weights.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray of shape (n_samples, 2)\n",
    "        Feature matrix\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        True labels\n",
    "    w_init : ndarray of shape (2,)\n",
    "        Initial weights [w1, w2]\n",
    "    learning_rate : float\n",
    "        Step size for each update\n",
    "    epochs : int\n",
    "        Number of iterations\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    w_history : ndarray of shape (epochs+1, 2)\n",
    "        Weights at each iteration (including initial)\n",
    "    loss_history : ndarray of shape (epochs+1,)\n",
    "        MSE at each iteration (including initial)\n",
    "    \"\"\"\n",
    "    w = w_init.copy()\n",
    "    w_history = [w.copy()]\n",
    "    loss_history = [mse_loss(w[0], w[1], X, y)]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Compute gradient\n",
    "        grad = compute_gradient(w, X, y)\n",
    "\n",
    "        # Update weights (move in negative gradient direction)\n",
    "        w = w - learning_rate * grad\n",
    "\n",
    "        # Record history\n",
    "        w_history.append(w.copy())\n",
    "        loss_history.append(mse_loss(w[0], w[1], X, y))\n",
    "\n",
    "    return np.array(w_history), np.array(loss_history)\n",
    "\n",
    "\n",
    "# Run gradient descent from a random starting point\n",
    "w_init = np.array([0.5, 0.5])\n",
    "learning_rate = 0.1\n",
    "epochs = 50\n",
    "\n",
    "w_history, loss_history = gradient_descent(X, y, w_init, learning_rate, epochs)\n",
    "\n",
    "print(f\"Initial weights: {w_init}\")\n",
    "print(f\"Final weights: {w_history[-1]}\")\n",
    "print(f\"Initial MSE: {loss_history[0]:.4f}\")\n",
    "print(f\"Final MSE: {loss_history[-1]:.4f}\")\n",
    "print(f\"Improvement: {loss_history[0] - loss_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd40d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the optimization process\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Left: Loss vs Epoch\n",
    "axes[0].plot(loss_history, linewidth=2, color=\"#2E86AB\")\n",
    "axes[0].scatter(0, loss_history[0], s=100, color=\"red\", zorder=5, label=\"Start\")\n",
    "axes[0].scatter(\n",
    "    len(loss_history) - 1, loss_history[-1], s=100, color=\"green\", zorder=5, label=\"End\"\n",
    ")\n",
    "axes[0].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axes[0].set_ylabel(\"MSE\", fontsize=12)\n",
    "axes[0].set_title(\"Loss Decreases Over Time\", fontsize=14, fontweight=\"bold\")\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Middle: Path on contour plot\n",
    "contour = axes[1].contour(\n",
    "    W1_grid, W2_grid, MSE_grid, levels=20, cmap=\"viridis\", alpha=0.6\n",
    ")\n",
    "axes[1].clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Plot the gradient descent path\n",
    "axes[1].plot(\n",
    "    w_history[:, 0],\n",
    "    w_history[:, 1],\n",
    "    \"o-\",\n",
    "    color=\"red\",\n",
    "    linewidth=2,\n",
    "    markersize=6,\n",
    "    label=\"GD path\",\n",
    ")\n",
    "axes[1].scatter(\n",
    "    w_history[0, 0],\n",
    "    w_history[0, 1],\n",
    "    s=200,\n",
    "    color=\"red\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=2,\n",
    "    label=\"Start\",\n",
    "    zorder=5,\n",
    ")\n",
    "axes[1].scatter(\n",
    "    w_history[-1, 0],\n",
    "    w_history[-1, 1],\n",
    "    s=200,\n",
    "    color=\"green\",\n",
    "    marker=\"*\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=2,\n",
    "    label=\"End\",\n",
    "    zorder=5,\n",
    ")\n",
    "axes[1].scatter(\n",
    "    w1_optimal,\n",
    "    w2_optimal,\n",
    "    s=200,\n",
    "    color=\"gold\",\n",
    "    marker=\"X\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=2,\n",
    "    label=\"True minimum\",\n",
    "    zorder=5,\n",
    ")\n",
    "\n",
    "axes[1].set_xlabel(\"w‚ÇÅ\", fontsize=12)\n",
    "axes[1].set_ylabel(\"w‚ÇÇ\", fontsize=12)\n",
    "axes[1].set_title(\n",
    "    'Gradient Descent Path (The \"Crawling Cursor\")', fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Decision boundary with learned weights\n",
    "axes[2].scatter(\n",
    "    X[y == -1, 0],\n",
    "    X[y == -1, 1],\n",
    "    s=100,\n",
    "    alpha=0.7,\n",
    "    c=\"#2E86AB\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=1.5,\n",
    "    label=\"Pine\",\n",
    ")\n",
    "axes[2].scatter(\n",
    "    X[y == 1, 0],\n",
    "    X[y == 1, 1],\n",
    "    s=100,\n",
    "    alpha=0.7,\n",
    "    c=\"#A23B72\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=1.5,\n",
    "    label=\"Birch\",\n",
    ")\n",
    "\n",
    "# Plot decision boundary for final weights (no bias: w1*x1 + w2*x2 = 0)\n",
    "w1_final, w2_final = w_history[-1]\n",
    "if abs(w2_final) > 1e-6:\n",
    "    x1_range = np.array([-3, 3])\n",
    "    x2_boundary = -(w1_final / w2_final) * x1_range\n",
    "    axes[2].plot(x1_range, x2_boundary, \"k--\", linewidth=2.5, label=\"Learned boundary\")\n",
    "\n",
    "axes[2].set_xlabel(\"Grain Prominence (standardized)\", fontsize=12)\n",
    "axes[2].set_ylabel(\"Brightness (standardized)\", fontsize=12)\n",
    "axes[2].set_title(\n",
    "    f\"Decision Boundary: w‚ÇÅ={w1_final:.2f}, w‚ÇÇ={w2_final:.2f}\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[2].set_xlim([-3, 3])\n",
    "axes[2].set_ylim([-3, 3])\n",
    "axes[2].axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.3)\n",
    "axes[2].axvline(x=0, color=\"gray\", linestyle=\"--\", alpha=0.3)\n",
    "axes[2].legend(fontsize=10)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6731ce75",
   "metadata": {},
   "source": [
    "### üîç What You're Seeing\n",
    "\n",
    "**Left plot (Loss vs Epoch):** The MSE decreases over time ‚Äî the algorithm is learning!\n",
    "\n",
    "**Middle plot (Gradient Descent Path):** The red dots trace the path gradient descent takes through parameter space. Notice:\n",
    "\n",
    "- It starts at the red circle (initial guess)\n",
    "- It follows the contour lines downhill\n",
    "- It converges near the gold X (true minimum)\n",
    "\n",
    "**Right plot (Decision Boundary):** The learned weights create a diagonal decision boundary that separates Pine from Birch ‚Äî this is the best boundary gradient descent can find with these parameters!\n",
    "\n",
    "**Key insight:** Gradient descent is like a ball rolling downhill. It naturally finds low points on the error surface.\n",
    "\n",
    "**Connection to Lab 05:** Remember manually adjusting $w_1$, $w_2$, and $\\beta$ with the interactive sliders until the line separated Pine from Birch? Gradient descent automates exactly that process ‚Äî it computes which direction reduces error and takes a step that way!\n",
    "\n",
    "**Mini-task:** Try running the cell above with different starting points:\n",
    "\n",
    "- `w_init = np.array([-0.5, 2.0])`\n",
    "- `w_init = np.array([0.8, 0.2])`\n",
    "\n",
    "Does gradient descent still find the minimum? (Hint: Yes, because this surface is convex!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ba140",
   "metadata": {},
   "source": [
    "## Part 5: Learning Rate & Epochs ‚Äî The Goldilocks Problem\n",
    "\n",
    "Gradient descent has two critical hyperparameters:\n",
    "\n",
    "1. **Learning rate ($\\eta$):** How big a step to take\n",
    "2. **Epochs:** How many steps to take\n",
    "\n",
    "Let's see what happens when these are **too small**, **too large**, or **just right**.\n",
    "\n",
    "### The 4 Experiments:\n",
    "\n",
    "1. **Good learning rate + enough epochs** ‚Üí converges smoothly ‚úì\n",
    "2. **Too small learning rate** ‚Üí slow, looks stuck üêå\n",
    "3. **Too large learning rate** ‚Üí diverges or oscillates üí•\n",
    "4. **Good learning rate + too few epochs** ‚Üí stops early ‚è±Ô∏è\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad4b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 4 experiments with different settings\n",
    "experiments = [\n",
    "    {\"lr\": 0.1, \"epochs\": 50, \"label\": \"Good LR + Enough Epochs\"},\n",
    "    {\"lr\": 0.01, \"epochs\": 50, \"label\": \"Too Small LR (slow)\"},\n",
    "    {\"lr\": 0.7, \"epochs\": 10, \"label\": \"Too Large LR (unstable)\"},\n",
    "    {\"lr\": 0.1, \"epochs\": 10, \"label\": \"Good LR + Too Few Epochs\"},\n",
    "]\n",
    "\n",
    "# Initial point for all experiments\n",
    "w_init = np.array([0.6, 0.3])\n",
    "\n",
    "# Run all experiments\n",
    "results = []\n",
    "for exp in experiments:\n",
    "    w_hist, loss_hist = gradient_descent(X, y, w_init, exp[\"lr\"], exp[\"epochs\"])\n",
    "    results.append(\n",
    "        {\n",
    "            \"label\": exp[\"label\"],\n",
    "            \"w_history\": w_hist,\n",
    "            \"loss_history\": loss_hist,\n",
    "            \"final_loss\": loss_hist[-1],\n",
    "            \"converged\": loss_hist[-1] < 0.1,  # Threshold for \"converged\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Print summary table\n",
    "print(\"Experiment Results Summary\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Experiment':<35} {'Final Loss':>12} {'Converged?':>12}\")\n",
    "print(\"-\" * 70)\n",
    "for res in results:\n",
    "    status = \"‚úì\" if res[\"converged\"] else \"‚úó\"\n",
    "    print(f\"{res['label']:<35} {res['final_loss']:>12.4f} {status:>12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4908ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all 4 experiments in a 2√ó2 grid\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, res in enumerate(results):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Plot contour\n",
    "    contour = ax.contour(\n",
    "        W1_grid, W2_grid, MSE_grid, levels=20, cmap=\"viridis\", alpha=0.4\n",
    "    )\n",
    "\n",
    "    # Plot path\n",
    "    w_hist = res[\"w_history\"]\n",
    "    ax.plot(\n",
    "        w_hist[:, 0],\n",
    "        w_hist[:, 1],\n",
    "        \"o-\",\n",
    "        color=\"red\",\n",
    "        linewidth=2,\n",
    "        markersize=5,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        w_hist[0, 0],\n",
    "        w_hist[0, 1],\n",
    "        s=200,\n",
    "        color=\"red\",\n",
    "        marker=\"o\",\n",
    "        edgecolors=\"black\",\n",
    "        linewidth=2,\n",
    "        zorder=5,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        w_hist[-1, 0],\n",
    "        w_hist[-1, 1],\n",
    "        s=200,\n",
    "        color=\"green\",\n",
    "        marker=\"*\",\n",
    "        edgecolors=\"black\",\n",
    "        linewidth=2,\n",
    "        zorder=5,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        w1_optimal,\n",
    "        w2_optimal,\n",
    "        s=150,\n",
    "        color=\"gold\",\n",
    "        marker=\"X\",\n",
    "        edgecolors=\"black\",\n",
    "        linewidth=2,\n",
    "        zorder=4,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"w‚ÇÅ\", fontsize=11)\n",
    "    ax.set_ylabel(\"w‚ÇÇ\", fontsize=11)\n",
    "    ax.set_title(\n",
    "        f\"{res['label']}\\nFinal Loss: {res['final_loss']:.4f}\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([W1_grid.min(), W1_grid.max()])\n",
    "    ax.set_ylim([W2_grid.min(), W2_grid.max()])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a8cfc0",
   "metadata": {},
   "source": [
    "Notice how learning rate affects the **speed** and **stability** of convergence!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6787fb0",
   "metadata": {},
   "source": [
    "### üîç What You're Seeing\n",
    "\n",
    "**Top-left (Good LR + Enough Epochs):**\n",
    "\n",
    "- Smooth path toward the minimum\n",
    "- Converges close to the optimal point ‚úì\n",
    "\n",
    "**Top-right (Too Small LR):**\n",
    "\n",
    "- Takes tiny steps (slow progress)\n",
    "- May not reach the minimum within the given epochs üêå\n",
    "\n",
    "**Bottom-left (Too Large LR):**\n",
    "\n",
    "- Takes huge steps that overshoot\n",
    "- May oscillate or even diverge (bounce around) üí•\n",
    "- Sometimes still converges, but inefficiently\n",
    "\n",
    "**Bottom-right (Good LR + Too Few Epochs):**\n",
    "\n",
    "- Moving in the right direction but stops too early ‚è±Ô∏è\n",
    "- Needs more iterations to reach the minimum\n",
    "\n",
    "**The moral:** Choosing the right learning rate is crucial! Too small = slow, too large = chaos, just right = efficient convergence.\n",
    "\n",
    "**Mini-task:** Add a 5th experiment with an extremely large learning rate (e.g., `lr=1.0`). What happens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf88711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of loss curves for all experiments\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = [\"green\", \"blue\", \"red\", \"orange\"]\n",
    "for idx, res in enumerate(results):\n",
    "    ax.plot(\n",
    "        res[\"loss_history\"],\n",
    "        linewidth=2,\n",
    "        color=colors[idx],\n",
    "        label=res[\"label\"],\n",
    "        marker=\"o\",\n",
    "        markersize=4,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Epoch\", fontsize=12)\n",
    "ax.set_ylabel(\"MSE\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"Loss Curves for Different Hyperparameter Settings\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale(\"log\")  # Log scale to see details for small losses\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe4181b",
   "metadata": {},
   "source": [
    "## Part 6: Non-Convex Surfaces ‚Äî When Things Get Tricky\n",
    "\n",
    "So far, our error surface has been **convex** (bowl-shaped) with a single minimum. Gradient descent easily finds it from any starting point.\n",
    "\n",
    "But many real ML problems have **non-convex** loss surfaces with:\n",
    "\n",
    "- **Multiple local minima** ‚Äî valleys that aren't the lowest point\n",
    "- **Saddle points** ‚Äî flat regions where gradient ‚âà 0\n",
    "- **Plateaus** ‚Äî areas where learning gets stuck\n",
    "\n",
    "Let's explore this using a classic non-convex function: **Himmelblau's function**.\n",
    "\n",
    "$$f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$$\n",
    "\n",
    "This function has **4 local minima** of equal depth ‚Äî a perfect toy example!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fa643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Himmelblau's function (a classic non-convex test function)\n",
    "def himmelblau(x, y):\n",
    "    \"\"\"\n",
    "    Himmelblau's function: has 4 local minima of equal depth.\n",
    "\n",
    "    Minima at approximately:\n",
    "    - (3.0, 2.0)\n",
    "    - (-2.805, 3.131)\n",
    "    - (-3.779, -3.283)\n",
    "    - (3.584, -1.848)\n",
    "    \"\"\"\n",
    "    return (x**2 + y - 11) ** 2 + (x + y**2 - 7) ** 2\n",
    "\n",
    "\n",
    "# Gradient of Himmelblau's function (computed analytically)\n",
    "def himmelblau_gradient(x, y):\n",
    "    \"\"\"Compute gradient of Himmelblau's function.\"\"\"\n",
    "    df_dx = 4 * x * (x**2 + y - 11) + 2 * (x + y**2 - 7)\n",
    "    df_dy = 2 * (x**2 + y - 11) + 4 * y * (x + y**2 - 7)\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "\n",
    "# Create a grid for visualization\n",
    "x_range = np.linspace(-5, 5, 200)\n",
    "y_range = np.linspace(-5, 5, 200)\n",
    "X_grid, Y_grid = np.meshgrid(x_range, y_range)\n",
    "Z_grid = himmelblau(X_grid, Y_grid)\n",
    "\n",
    "print(\"Himmelblau's function computed on 200√ó200 grid\")\n",
    "print(f\"Function value range: [{Z_grid.min():.4f}, {Z_grid.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b3ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Himmelblau's function - Interactive 3D with Plotly\n",
    "\n",
    "# The 4 minima locations\n",
    "minima = [(3.0, 2.0), (-2.805, 3.131), (-3.779, -3.283), (3.584, -1.848)]\n",
    "\n",
    "# Interactive 3D surface using Plotly\n",
    "fig_3d_himmel = go.Figure()\n",
    "\n",
    "# Add the surface\n",
    "fig_3d_himmel.add_trace(\n",
    "    go.Surface(\n",
    "        x=X_grid,\n",
    "        y=Y_grid,\n",
    "        z=Z_grid,\n",
    "        colorscale=\"Viridis\",\n",
    "        opacity=0.9,\n",
    "        name=\"Himmelblau Surface\",\n",
    "        showscale=True,\n",
    "        colorbar=dict(title=\"f(x,y)\", x=1.02),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Mark the 4 minima\n",
    "for i, (xmin, ymin) in enumerate(minima):\n",
    "    zmin = himmelblau(xmin, ymin)\n",
    "    fig_3d_himmel.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=[xmin],\n",
    "            y=[ymin],\n",
    "            z=[zmin],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=8, color=\"red\", symbol=\"diamond\"),\n",
    "            name=f\"Min {i + 1}: ({xmin:.2f}, {ymin:.2f})\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig_3d_himmel.update_layout(\n",
    "    title=dict(\n",
    "        text=\"<b>Himmelblau's Function (Non-Convex)</b><br><sup>Drag to rotate, scroll to zoom ‚Äî notice the 4 valleys!</sup>\",\n",
    "        x=0.5,\n",
    "    ),\n",
    "    scene=dict(\n",
    "        xaxis_title=\"x\",\n",
    "        yaxis_title=\"y\",\n",
    "        zaxis_title=\"f(x, y)\",\n",
    "        camera=dict(eye=dict(x=1.5, y=1.5, z=1.2)),\n",
    "    ),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(l=0, r=0, t=60, b=0),\n",
    ")\n",
    "\n",
    "fig_3d_himmel.show()\n",
    "\n",
    "# Contour plot (static, for comparison)\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "contour = ax.contour(X_grid, Y_grid, Z_grid, levels=30, cmap=\"viridis\")\n",
    "ax.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Mark the 4 minima\n",
    "for xmin, ymin in minima:\n",
    "    ax.scatter(\n",
    "        xmin,\n",
    "        ymin,\n",
    "        color=\"red\",\n",
    "        s=200,\n",
    "        marker=\"*\",\n",
    "        edgecolors=\"black\",\n",
    "        linewidth=2,\n",
    "        zorder=5,\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"x\", fontsize=12)\n",
    "ax.set_ylabel(\"y\", fontsize=12)\n",
    "ax.set_title(\"Contour Plot (4 Minima Marked)\", fontsize=14, fontweight=\"bold\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed622d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent on Himmelblau's function\n",
    "def gradient_descent_himmelblau(x_init, y_init, learning_rate=0.01, epochs=100):\n",
    "    \"\"\"\n",
    "    Gradient descent on Himmelblau's function.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    history : ndarray of shape (epochs+1, 2)\n",
    "        [x, y] at each iteration\n",
    "    loss_history : ndarray of shape (epochs+1,)\n",
    "        Function value at each iteration\n",
    "    \"\"\"\n",
    "    pos = np.array([x_init, y_init])\n",
    "    history = [pos.copy()]\n",
    "    loss_history = [himmelblau(pos[0], pos[1])]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        grad = himmelblau_gradient(pos[0], pos[1])\n",
    "        pos = pos - learning_rate * grad\n",
    "\n",
    "        history.append(pos.copy())\n",
    "        loss_history.append(himmelblau(pos[0], pos[1]))\n",
    "\n",
    "    return np.array(history), np.array(loss_history)\n",
    "\n",
    "\n",
    "# Try gradient descent from 6 different random starting points\n",
    "np.random.seed(123)\n",
    "n_trials = 6\n",
    "starting_points = np.random.uniform(-4, 4, size=(n_trials, 2))\n",
    "\n",
    "# Run gradient descent from each starting point\n",
    "results_himmel = []\n",
    "for i, start in enumerate(starting_points):\n",
    "    history, loss_history = gradient_descent_himmelblau(\n",
    "        start[0], start[1], learning_rate=0.01, epochs=200\n",
    "    )\n",
    "    results_himmel.append(\n",
    "        {\n",
    "            \"start\": start,\n",
    "            \"history\": history,\n",
    "            \"loss_history\": loss_history,\n",
    "            \"final\": history[-1],\n",
    "            \"final_loss\": loss_history[-1],\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"Ran gradient descent from {n_trials} random starting points\")\n",
    "print(\"\\nResults:\")\n",
    "print(\"=\" * 70)\n",
    "for i, res in enumerate(results_himmel):\n",
    "    print(\n",
    "        f\"Trial {i + 1}: Start=({res['start'][0]:.2f}, {res['start'][1]:.2f}) \"\n",
    "        f\"‚Üí End=({res['final'][0]:.2f}, {res['final'][1]:.2f}), \"\n",
    "        f\"Loss={res['final_loss']:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d28a32",
   "metadata": {},
   "source": [
    "Notice the 4 \"valleys\" ‚Äî each is a **local minimum**! Depending on where you start, gradient descent will find different minima.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db56ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all gradient descent paths on the contour plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Plot contour\n",
    "contour = ax.contour(X_grid, Y_grid, Z_grid, levels=30, cmap=\"viridis\", alpha=0.5)\n",
    "ax.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Mark the 4 true minima\n",
    "for xmin, ymin in minima:\n",
    "    ax.scatter(\n",
    "        xmin,\n",
    "        ymin,\n",
    "        color=\"gold\",\n",
    "        s=300,\n",
    "        marker=\"*\",\n",
    "        edgecolors=\"black\",\n",
    "        linewidth=2,\n",
    "        zorder=5,\n",
    "        label=\"True minima\",\n",
    "    )\n",
    "\n",
    "# Plot each gradient descent path\n",
    "colors = [\"red\", \"blue\", \"green\", \"purple\", \"orange\", \"brown\"]\n",
    "for i, res in enumerate(results_himmel):\n",
    "    hist = res[\"history\"]\n",
    "    ax.plot(\n",
    "        hist[:, 0],\n",
    "        hist[:, 1],\n",
    "        \"o-\",\n",
    "        color=colors[i],\n",
    "        linewidth=2,\n",
    "        markersize=4,\n",
    "        alpha=0.8,\n",
    "        label=f\"Trial {i + 1}\",\n",
    "    )\n",
    "    ax.scatter(\n",
    "        hist[0, 0],\n",
    "        hist[0, 1],\n",
    "        s=150,\n",
    "        color=colors[i],\n",
    "        marker=\"o\",\n",
    "        edgecolors=\"black\",\n",
    "        linewidth=2,\n",
    "        zorder=4,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        hist[-1, 0],\n",
    "        hist[-1, 1],\n",
    "        s=150,\n",
    "        color=colors[i],\n",
    "        marker=\"X\",\n",
    "        edgecolors=\"black\",\n",
    "        linewidth=2,\n",
    "        zorder=4,\n",
    "    )\n",
    "\n",
    "# Remove duplicate labels for minima\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "unique_labels = {}\n",
    "for handle, label in zip(handles, labels):\n",
    "    if label not in unique_labels:\n",
    "        unique_labels[label] = handle\n",
    "ax.legend(unique_labels.values(), unique_labels.keys(), fontsize=10, loc=\"upper left\")\n",
    "\n",
    "ax.set_xlabel(\"x\", fontsize=12)\n",
    "ax.set_ylabel(\"y\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"Gradient Descent from Multiple Starting Points\\n(Different starts ‚Üí Different minima!)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf3e665",
   "metadata": {},
   "source": [
    "See how different starting points lead to different local minima? This is the challenge of **non-convex optimization**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f213b54b",
   "metadata": {},
   "source": [
    "### üîç The Local Minima Problem\n",
    "\n",
    "**What you're seeing:**\n",
    "\n",
    "- Gradient descent gets trapped in whichever minimum is **closest to the starting point**\n",
    "- All 4 minima are equally good (same function value)\n",
    "- But in real ML, some local minima are better than others!\n",
    "\n",
    "**Why this matters:**\n",
    "\n",
    "- In deep learning, loss surfaces are highly non-convex\n",
    "- Random initialization can lead to different (and sometimes bad) solutions\n",
    "- This is why we often train models multiple times with different seeds\n",
    "\n",
    "**Mitigation strategies:**\n",
    "\n",
    "1. **Random restarts** ‚Äî try many initializations, pick the best result\n",
    "2. **Better initialization** ‚Äî use smart starting points (e.g., Xavier, He initialization)\n",
    "3. **Momentum** ‚Äî help gradient descent \"roll through\" small bumps\n",
    "4. **Adaptive learning rates** ‚Äî algorithms like Adam that adjust step size automatically\n",
    "5. **Stochastic gradient descent** ‚Äî noise from mini-batches can help escape shallow local minima\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb1063d",
   "metadata": {},
   "source": [
    "### Mini-Demo: Random Restarts\n",
    "\n",
    "Since gradient descent gets trapped in the **nearest** minimum, one practical solution is to try **many random starting points** and keep the best result!\n",
    "\n",
    "This simple strategy is widely used in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed622d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random restarts: try many starting points, keep the best\n",
    "np.random.seed(42)\n",
    "n_restarts = 20\n",
    "\n",
    "restart_results = []\n",
    "for i in range(n_restarts):\n",
    "    # Random starting point\n",
    "    start = np.random.uniform(-4, 4, size=2)\n",
    "\n",
    "    # Run gradient descent\n",
    "    history, loss_history = gradient_descent_himmelblau(\n",
    "        start[0], start[1], learning_rate=0.01, epochs=200\n",
    "    )\n",
    "\n",
    "    restart_results.append(\n",
    "        {\n",
    "            \"start\": start,\n",
    "            \"history\": history,\n",
    "            \"final\": history[-1],\n",
    "            \"final_loss\": loss_history[-1],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Find the best result\n",
    "best_idx = np.argmin([r[\"final_loss\"] for r in restart_results])\n",
    "best_result = restart_results[best_idx]\n",
    "\n",
    "# Count how many times we found each minimum\n",
    "print(f\"Random Restarts: {n_restarts} attempts\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nFinal positions (which minimum each run found):\")\n",
    "for i, res in enumerate(restart_results):\n",
    "    print(\n",
    "        f\"  Run {i + 1:2d}: ({res['final'][0]:6.2f}, {res['final'][1]:6.2f}) ‚Üí loss = {res['final_loss']:.6f}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nBest result: Run {best_idx + 1}\")\n",
    "print(f\"  Final: ({best_result['final'][0]:.3f}, {best_result['final'][1]:.3f})\")\n",
    "print(f\"  Loss: {best_result['final_loss']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f2826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all random restart paths\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Plot contour\n",
    "contour = ax.contour(X_grid, Y_grid, Z_grid, levels=30, cmap=\"viridis\", alpha=0.4)\n",
    "\n",
    "# Mark the 4 true minima\n",
    "for xmin, ymin in minima:\n",
    "    ax.scatter(\n",
    "        xmin,\n",
    "        ymin,\n",
    "        color=\"gold\",\n",
    "        s=400,\n",
    "        marker=\"*\",\n",
    "        edgecolors=\"black\",\n",
    "        linewidth=2,\n",
    "        zorder=10,\n",
    "    )\n",
    "\n",
    "# Plot all paths (thin gray lines)\n",
    "for res in restart_results:\n",
    "    ax.plot(\n",
    "        res[\"history\"][:, 0], res[\"history\"][:, 1], \"gray\", linewidth=0.8, alpha=0.4\n",
    "    )\n",
    "    # Starting points (small circles)\n",
    "    ax.scatter(\n",
    "        res[\"start\"][0], res[\"start\"][1], s=30, color=\"blue\", alpha=0.5, zorder=4\n",
    "    )\n",
    "    # Final points (small X markers)\n",
    "    ax.scatter(\n",
    "        res[\"final\"][0],\n",
    "        res[\"final\"][1],\n",
    "        s=50,\n",
    "        color=\"red\",\n",
    "        marker=\"x\",\n",
    "        alpha=0.7,\n",
    "        zorder=5,\n",
    "    )\n",
    "\n",
    "# Highlight the best path\n",
    "ax.plot(\n",
    "    best_result[\"history\"][:, 0],\n",
    "    best_result[\"history\"][:, 1],\n",
    "    \"green\",\n",
    "    linewidth=3,\n",
    "    alpha=0.9,\n",
    "    label=\"Best run\",\n",
    ")\n",
    "ax.scatter(\n",
    "    best_result[\"start\"][0],\n",
    "    best_result[\"start\"][1],\n",
    "    s=200,\n",
    "    color=\"green\",\n",
    "    marker=\"o\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=2,\n",
    "    zorder=8,\n",
    "    label=\"Best start\",\n",
    ")\n",
    "ax.scatter(\n",
    "    best_result[\"final\"][0],\n",
    "    best_result[\"final\"][1],\n",
    "    s=200,\n",
    "    color=\"green\",\n",
    "    marker=\"X\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=2,\n",
    "    zorder=8,\n",
    "    label=\"Best final\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"x\", fontsize=12)\n",
    "ax.set_ylabel(\"y\", fontsize=12)\n",
    "ax.set_title(\n",
    "    f\"Random Restarts: {n_restarts} Attempts\\n(Different starts ‚Üí Different minima ‚Üí Pick the best!)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.legend(fontsize=11, loc=\"upper left\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328de3b3",
   "metadata": {},
   "source": [
    "**Key observations:**\n",
    "\n",
    "- Different starting points land in different local minima\n",
    "- By trying many starts, we explore the whole landscape\n",
    "- Pick the best result ‚Üí more likely to find a good solution!\n",
    "\n",
    "üí° **This is a real technique used in practice!** In deep learning, we often train multiple models with different random seeds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d73cc",
   "metadata": {},
   "source": [
    "## Part 7: Wrap-Up ‚Äî Connecting the Dots\n",
    "\n",
    "### What You Learned Today\n",
    "\n",
    "1. **Error surfaces** visualize how loss changes with parameters\n",
    "   - Convex surfaces (bowl-shaped) have a single minimum\n",
    "   - Non-convex surfaces have multiple local minima and saddle points\n",
    "\n",
    "2. **Gradient descent** is the fundamental optimization algorithm\n",
    "   - Follow the gradient (direction of steepest descent) to find minima\n",
    "   - Learning rate controls step size (too small = slow, too large = unstable)\n",
    "   - Requires enough epochs to converge\n",
    "\n",
    "3. **Hyperparameters matter**\n",
    "   - Learning rate and number of epochs critically affect performance\n",
    "   - Need to tune these for each problem\n",
    "\n",
    "4. **Non-convex problems are tricky**\n",
    "   - Different starting points ‚Üí different solutions\n",
    "   - Random restarts and smart initialization help\n",
    "   - This is a fundamental challenge in deep learning\n",
    "\n",
    "### The Red Thread from Lab 05\n",
    "\n",
    "In Lab 05, you **manually** adjusted weights $w_1, w_2, \\beta$ to find the **diagonal decision boundary** separating Pine from Birch. You learned:\n",
    "\n",
    "- $w_1$ (grain prominence) and $w_2$ (brightness) control the **slope** of the boundary\n",
    "- $\\beta$ (bias) controls the **position** of the boundary\n",
    "- Both features matter ‚Äî that's why the boundary is diagonal!\n",
    "\n",
    "Today, you saw how **gradient descent automatically** discovers these same optimal parameters by:\n",
    "\n",
    "- Computing the gradient (which direction reduces error)\n",
    "- Taking steps in that direction\n",
    "- Repeating until convergence\n",
    "\n",
    "**The key connection:** Those sliders you adjusted in Lab 05? Gradient descent moves them for you, following the steepest downhill path on the error surface.\n",
    "\n",
    "**This is how ALL modern machine learning models learn:** from linear regression to GPT!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
